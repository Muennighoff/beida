## 第九周笔记 tensorflow学习

### 张量

张量是不可变化的多维数组

- 0维Tensor的形式是标量，有0个轴
- 1维Tensor的形式是向量，有1个轴
- 2维Tensor的形式是矩阵，有2个轴
- 3维Tensor的形式是由矩阵组成的向量，有3个轴

#### 创建

```python
import tensorflow as tf
import numpy as np

#一维张量，就是一个数
rank_0_tensor = tf.constant(4)
print(rank_0_tensor)

rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
print(rank_1_tensor)

rank_2_tensor = tf.constant([[1, 2],
                             [3, 4],
                             [5, 6]], dtype=tf.float16)
print(rank_2_tensor)

#可以通过np中的函数将tf张量作为np中的数组输出
np.array(rank_2_tensor)
```

#### 运算操作

##### 数据类型转换

```python
a = tf.constant([[1,2],[3,4]], dtype=tf.float64)
b = tf.cast(a,dtype=tf.float16)
c = tf.cast(b,dtype=tf.uint8)
```

##### 基本运算

```python
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]]) # Could have also said `tf.ones([2,2])`

#一些基本运算，每个元素的运算
tf.add(a, b)
tf.multiply(a, b)
tf.matmul(a, b)#矩阵乘

tf.reduce_max(a)
#求最大值的index
tf.argmax(a)
#对元素进行归一化处理
tf.nn.softmax(a)
```

##### 变形

```python
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])
# 变成6x5
print(tf.reshape(rank_3_tensor,[3*2,5]),"\n")
# -1代表剩下的自己排
print(tf.reshape(rank_3_tensor,[3,-1]))
```

##### 索引（与数组类似）

```python
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())

print("Everything:", rank_1_tensor[:].numpy())
print("Before 4:", rank_1_tensor[:4].numpy())
print("From 4 to the end:", rank_1_tensor[4:].numpy())
print("From 2, before 7:", rank_1_tensor[2:7].numpy())
print("Every other item:", rank_1_tensor[::2].numpy())
print("Reversed:", rank_1_tensor[::-1].numpy())
```

##### 广播

广播是在一定条件下，对一组张量执行组合运算时，为了适应大张量，会对小张量进行“扩展”。

最简单和最常见的例子是尝试将张量与标量相乘或相加。在这种情况下会对标量进行广播，使其变成与其他参数相同的形状。

 **大大节省内存**

```
x = tf.constant([1, 2, 3])

y = tf.constant(2)
z = tf.constant([2, 2, 2])
# All of these are the same computation
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
```

##### 不规则张量与稀疏张量



### 变量

可以理解变量就是可以改变的张量

#### 初始化

```python
import tensorflow as tf
my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
my_variable = tf.Variable(my_tensor)#先建立张量，再生成变量

# Variables can be all kinds of types, just like tensors
bool_variable = tf.Variable([False, False, False, True])
complex_variable = tf.Variable([5 + 4j, 6 + 1j])#虚数单位是j
complex_variable
```

#### 修改

```python
a = tf.Variable([2.0, 3.0])
# This will keep the same dtype, float32
a.assign([1, 2]) 

#通过assign来修改
```



### 自动微分

- 自动微分（Automatic Differentiation，简称AD）也称自动求导，算法能够计算可导函数在某点处的导数值的计算，是反向传播算法的一般化。自动微分要解决的核心问题是计算复杂函数，通常是多层复合函数在某一点处的导数，梯度，以及Hessian矩阵值。

- 自动微分是介于符号微分和数值微分之间的一种方法：数值微分一开始就代入数值近似求解；符号微分直接对表达式进行推导，最后才代入自变量的值得到最终解。自动微分将符号微分应用于最基本的运算（或称原子操作），如常数，幂函数，指数函数，对数函数，三角函数等基本函数，代入自变量的值得到其导数值，作为中间结果进行保留。然后再根据

  ```
  
  ```

  这些基本运算单元的求导结果计算出整个函数的导数值。

#### 求一阶导

```python
x = tf.ones((2, 2))
print(x)
with tf.GradientTape() as t:
  t.watch(x)#watch是把x记录在梯度带上面
  y = tf.reduce_sum(x)#理解为y=x1+x2+x3+x4
  z = tf.multiply(y, y)#z=（x1+x2+x3+x4）*2
print(y)
# Derivative of z with respect to the original input tensor x
dz_dx = t.gradient(z, x)
for i in [0, 1]:
  for j in [0, 1]:
    assert dz_dx[i][j].numpy() == 8.0
print(dz_dx)
```

#### 求二阶导

```python
x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t:
  with tf.GradientTape() as t2:
    y = x * x * x
  # Compute the gradient inside the 't' context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t2.gradient(y, x)
d2y_dx2 = t.gradient(dy_dx, x)
print(dy_dx)
print(d2y_dx2)
```

