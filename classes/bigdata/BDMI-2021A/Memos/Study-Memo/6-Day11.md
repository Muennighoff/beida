### 第1 TF2


AutoDiff:
- Forward Model - 前向模式: 
  - Computes gradient just in regard to one input
- Backward Model:
  - We use this！
  - Advantage:
    - Computes gradients towards all inputs


Connection Types - 连接关系:
- Forward
- 反馈网络


- When not to use `Sequential`:
  - Residual Blocks
  - Complex In / Outputs


- TF uses `__call__()` , PyTorch `forward()`


```python
import tensorflow as tf

class Dense(tf.Module):
  def __init__(self, in_features=3, out_features=1, name=None):
    super().__init__(name=name)
    self.w = tf.Variable(
        initial_value=[[-3.14, -2.31, 2.16]], name='w')
    self.b = tf.Variable(tf.zeros([out_features]), name='b')

  def __call__(self, x):
    y = tf.matmul(x, tf.transpose(self.w)) + self.b
    return tf.nn.sigmoid(y)

data = tf.constant([[0.0288, -0.3256, 0.5925]])
model = Dense()
model(data)
```


```python
import tensorflow as tf

class Dense(tf.Module):
  def __init__(self, in_features=3, out_features=1, name=None):
    super().__init__(name=name)
    self.w = tf.Variable(tf.random.normal([in_features, in_features], name='w'))
    self.b = tf.Variable(tf.zeros([in_features]), name='b')

    self.w2 = tf.Variable(tf.random.normal([in_features, out_features], name='w'))
    self.b2 = tf.Variable(tf.zeros([out_features]), name='b')

  def __call__(self, x):
    x = tf.nn.relu(tf.matmul(x, self.w) + self.b)
    x = tf.nn.sigmoid(tf.matmul(x, self.w2) + self.b2)
    return x

data = tf.constant([[0.0288, -0.3256, 0.5925]])
model = Dense()
model(data)
```

```python
ckpt = tf.train.Checkpoint(model=model)
ckpt.write("out")
!cat out.data-00000-of-00001
```

```python
data_file = tf.keras.utils.get_file("data.csv", "https://gitee.com/zhenchen3419/BDMI-2021A/raw/master/Computing/logistic_regression/data.csv")
pd.read_csv(data_file)
```