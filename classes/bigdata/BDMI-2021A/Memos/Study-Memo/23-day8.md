# 第八节课 总结

## 1. pandas

### 一维数据结构

Series，带标记的一维数组，在参数中注明index可以实现标记，例如：

```python
import numpy as np
import pandas as pd
s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
###
s=
a    1.962417
b   -0.860986
c    0.304188
d    0.950094
e    1.409179
dtype: float64
```

该结构支持一些向量基本运算和切片运算。

### 二维数据结构

DataFrame，类似于Excel工具表或是SQL表，也可以视为储存了若干列的Series。例如：

```python
d = {'one' : pd.Series([1.,2.,3.],index=['a','b','c']),
     'two' : pd.Series([1.,2.,3.,4.],index=['a','b','c','d'])}
df = pd.DataFrame(d)
```

该结构可以视为一个结构体，可以直接使用'.'来调取行、列信息，其中index被置为行信息，而字典标签被置为列信息。使用DataFrame.T可以实现矩阵转置，交换行列信息。

可以与字典类似的，使用列标签提取列信息，并将该对象作为Series使用，同样支持向量运算、切片、逻辑运算等操作。

## 2.Tensorflow

### 机器学习入门

在[Playground](playground.tensorflow.org)中尝试可视化操作机器学习过程，更改输入的特征、激活函数、学习率、隐藏层数和神经元数等特征，实现不同图像的拟合。

### Softmax

$$
g(z_m)= \frac {e^{z_m}} {\Sigma_ke^{z_m}}
$$

实现输出概率的归一化，特征为输出向量和为一：

```python
import numpy as np
def softmax(z):
    sum=0
    for item in z:
        sum+=np.exp(item)
    return np.exp(z)/sum
```



### 损失函数

每次学习需要确定与预期结果的偏差即损失函数值，使用均方差或交叉熵等定义。

均方差(MSE)表示：

```python
import numpy as np
def MSE(x):
    y=x-np.mean(x)
    y=y*y
    return np.mean(y)
##参数表示（输出结果序列与预期序列的差值）
```

交叉熵(CE)表示:

```python
import numpy as np
def CE(label,result):
    return np.sum(-np.array(label)*np.log(result))
##参数表示预期标签值序列和结果序列
```

### 反向传播

对每个激活函数求逆函数，用于反向传播求输入值，例如求Sigmoid的逆函数logit：

```python
import numpy as np
def logit(x):
    return -np.log(1/x-1)
```

使用最大梯度下降法，需要对正向传播的每一步求导以获得反向传播后损失关于权重的梯度，进而进行梯度更新。

权重更新需要对学习率作调整，一般取0.01。

### 二元分类演示

详见代码与课件：

```python
import pandas as pd
import numpy as np
data = pd.read_excel('data.xlsx')
data = data.rename(columns={'Q1_性别': 'label', 
                            'Q2_身高（厘米）': 'height', 
                            'Q3_体重 （公斤）': 'weight', 
                            'Q4_头发长度（厘米）': 'hair'})
data['label'] = data['label'].apply(lambda x : {'男': 0, '女': 1}[x])
features = data[['height', 'weight', 'hair']].to_numpy()
mean = np.mean(features, axis=0)
std = np.std(features, axis=0)
features = (features - mean)/std
label = data['label'].to_numpy()
def sigmoid(scores):
    return 1 / (1 + np.exp(-scores))
def log_likelihood(features, target, weights):##对数似然
    scores = np.dot(features, weights)
    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )
    return ll
def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):##梯度下降递归权重更新
    if add_intercept:
        intercept = np.ones((features.shape[0], 1))
        features = np.hstack((intercept, features))
        
    weights = np.zeros(features.shape[1])
    
    for step in range(num_steps):
        scores = np.dot(features, weights)
        predictions = sigmoid(scores)

        # Update weights with log likelihood gradient
        output_error_signal = target - predictions
        
        gradient = np.dot(features.T, output_error_signal)
        weights += learning_rate * gradient

        # Print log-likelihood every so often
        if step % 10000 == 0:
            print(log_likelihood(features, target, weights))
        
    return weights
weights = logistic_regression(features, label,
                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)
def predict(features, weights):##训练完成后的预测函数
    global mean
    global std
    features = (features - mean)/std
    intercept = np.ones((features.shape[0], 1))
    features = np.hstack((intercept, features))
    scores = np.dot(features, weights)
    predictions = sigmoid(scores)
    
    return predictions
```

