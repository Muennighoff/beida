# 第九次课总结

## 1.张量表示神经网络

* 结合numpy库，构建张量：两个库的张量构建可以实现混用：

```python
import numpy as np
np.random.randn(3,3)
'''
array([[ 0.2048181 , -0.8483676 ,  0.25005575],
       [ 1.61002019,  2.29898272,  0.99110923],
       [-1.31844031,  0.8309438 , -0.66216197]])
'''
np.random.normal(2,3,4)
'''
array([1.26578821, 0.96227746, 4.65468837, 3.52510983])
'''
```

* 使用张量计算完成神经网络：输入[22,35,86]，经过两层随机的神经元输出两个值

``` python
x=np.array([22,35,86])
W1=np.random.randn(3,3)
W2=np.random.randn(2,3)
def softmax(z):
    sum=0
    for item in z:
        sum+=np.exp(item)
    return np.exp(z)/sum
Y=softmax(np.dot(W2,np.dot(W1,x)))
Y
'''
array([1.20593557e-49, 1.00000000e+00])
'''
```

## 2.TensorFlow2

### 张量

tensor，即tensorflow的基本运算单元，属于常量。可以使用tf.constant创建：

```python
import tensorflow as tf
import numpy as np
rank_0_tensor = tf.constant(4)
rank_1_tensor = tf.constant([2.0, 3.0, 4.0])
```

一个张量有若干属性，主要的是值value, 数据类型dtype, 名称name.

要创建一个张量，只有value值是必须的。

在创建张量时可以和numpy联用：

``` python
b=tf.constant(np.zeros((4,5,6)))
print(b)
'''
tf.Tensor(
[[[0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0.]]], shape=(4, 5, 6), dtype=float64)
  '''
ar1=tf.constant(np.random.randn(2,2))
ar2=tf.constant(np.random.randn(2,2))
print(ar1,ar2)
'''
tf.Tensor(
[[ 0.43151622  0.6191719 ]
 [-1.00997631  0.81287273]], shape=(2, 2), dtype=float64) tf.Tensor(
[[-1.25754975 -0.70361333]
 [ 0.72564097 -1.51702646]], shape=(2, 2), dtype=float64)
 '''
```

张量也适用一系列运算操作，与numpy类似。此外，tf内置softmax和最大值、最大值索引、最小值、最小值索引等：

``` python
print(tf.nn.softmax(ar1))#softmax
print(tf.reduce_max(ar1))#最大值
print(tf.argmax(ar1))#最大值索引
print(tf.reduce_min(ar1))#最小值
print(tf.argmin(ar1))#最小值索引
```

此外支持变量类型转换cast()、形状转换reshape()等操作。

张量可以广播、可以切片，均类似于matlab。

此外有不规则张量、稀疏张量、字符串张量等，不多做描述。

### 变量

使用tf.Variable(constant)来创建原始变量。

变量的参数、性质、用法与张量基本一致，只是其内容可变，使用a.assign(constant)更改，需要注意的是常量形状需要与变量初始化形状一致。

a.assign_add()和a.assign_sub()可以实现变量自身的加减运算，结果覆盖原变量：

``` python
a=tf.Variable([2.0 3.0])
b=tf.Variable(a)
a.assign([5,6])
print(a.numpy())
'''
[5. 6.]
'''
print(b.numpy())
'''
[2. 3.]
'''
print(a.assign_add([2,3]).numpy())
'''
[7. 9.]
'''
print(a.assign_sub([7,9]).numpy())
'''
[0. 0.]
'''
```

### 自动微分

实现对一个变量的梯度观测，使用梯度带tf.GradientTape()记录相关数据：

``` python
x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x)
  z = tf.multiply(y, y)

# Derivative of z with respect to the original input tensor x
dz_dx = t.gradient(z, x)
for i in [0, 1]:
  for j in [0, 1]:
    assert dz_dx[i][j].numpy() == 8.0
```

类似的，实现对一个变量x=1进行f(x)=$x^2+e^x$操作后记录梯度：

```python
x=tf.constant(1.0)
with tf.GradientTape() as tp:
    tp.watch(x)
    y=x*x+tf.exp(x)
dy_dx=tp.gradient(y,x)
dy_dx.numpy()
'''
4.7182817
'''
```

