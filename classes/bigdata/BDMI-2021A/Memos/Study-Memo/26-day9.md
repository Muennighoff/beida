# 课程总结9

## 1.张量

```
np.random.rand(2,3,4)
[[[ 0.95052148 -1.05799236  0.82612343 -0.23362751]
  [-0.85824765 -0.45275333 -2.52677003  0.56326931]
  [-0.47394829 -1.51459052 -0.17017117 -0.90459823]]

 [[ 0.34412792  0.05538345 -1.92158088 -0.1274031 ]
  [ 0.29309072 -1.38690722  1.08329462 -0.0558741 ]
  [ 0.58888643  1.38041541  0.43055181  0.90731318]]]
  
1维张量
tf.constant([2.0, 3.0, 4.0], shape=(3,), dtype=float32)

2维张量
tf.constant([[1, 2],
			[3, 4]
			[5, 6]], dtype=tf.float16)
			
3维张量
tf.constant([[[],[]],[[],[]],[[],[]]])

稀疏张量
sparse_tensor = tf.sparse.SparseTensor(indices = [[0,1],[1,2]], values = [1,2], dense_shape = [3,4])
print(sparse_tensor)
print(tf.sparse.to_dense(sparse_tensor))
```

## 2.张量运算

```
np.array(rank_2_tensor) #张量赋值给数组
rank_2_tensor.numpy() #用数组形式调用张量
#元素加法
tf.add(a,b)
a + b
#元素乘法
tf.multiply(a,b)
a * b
#矩阵乘法
tf.matmul(a,b)
a @ b
#最大值
tf.reduce_max()
#最大值的索引
tf.argmax()
#归一化
tf.nn.softmax()

aa = tf.contant([[1],[2],[3]])#是3*1的二维张量
bb = tf.reshape(aa, [1,3])#变成1*3的二维张量

 = tf.constant([1,2,3])
y = tf.constant(2)
z = tf.constant([2,2,2])

#下面三种运算结果相同
print(tf.multiply(x,2))
print(x * y)
print(x * z)

x = tf.constant([[1],[2],[3]])#3*1的张量
y = tf.constant([1,2,3,4])#1*4的张量
print(x * y)#结果得到3*4的张量
rank_1_tensor[2: 7]#从第2个元素，到第6个元素

```

## 3.变量

使用tf.Variable(constant)来创建原始变量。

a.assign(constant)更改.

a.assign_add() a.assign_sub()实现变量自身的加减运算，结果覆盖原变量

## 4.自动微分

```
梯度带
会把tf.GradientTape的所有操作记录在磁带上(tape)，基于每次操作的导数，用反向微分法来计算被 记录 的函数的导数

x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x) # = x[0][0] + ... + x[1][1] = 4
  z = tf.multiply(y, y) # = 4*4 = 16

# Derivative of z with respect to the original input tensor x
dz_dx = t.gradient(z, x)
for i in [0, 1]:
  for j in [0, 1]:
    assert dz_dx[i][j].numpy() == 8.0
    
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as t:
  t.watch(x)
  y = x * x
  z = y * y
dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = t.gradient(y, x)  # 6.0
print(dz_dx)
print(dy_dx)
del t  # Drop the reference to the tape
# 使用del t 来主动释放资源

def f(x, y):
  output = 1.0
  for i in range(y):
    if i > 1 and i < 5:
      output = tf.multiply(output, x)
  return output

def grad(x, y):
  with tf.GradientTape() as t:
    t.watch(x)
    out = f(x, y)
  return t.gradient(out, x)

x = tf.convert_to_tensor(2.0)
print(grad(x, 6))
print(grad(x, 5))
print(grad(x, 4))
assert grad(x, 6).numpy() == 12.0
assert grad(x, 5).numpy() == 12.0
assert grad(x, 4).numpy() == 4.0

如果再测试的时候将variable变成了 False trainable，最后会报错

自动微分
高阶导数
x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t:
  with tf.GradientTape() as t2:
    y = x * x * x
  # Compute the gradient inside the 't' context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t2.gradient(y, x)
d2y_dx2 = t.gradient(dy_dx, x)
print(dy_dx)
print(d2y_dx2)
assert dy_dx.numpy() == 3.0
assert d2y_dx2.numpy() == 6.0

求两次导，最后分别得到3 和 6
y = x ** 3
```



