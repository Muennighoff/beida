## 学习小结-1110-day9
---

#### 1. TensorFlow2基础

- 张量
     - 基础知识：

       张量是具有统一类型（称为`dtype`）的多维数组，可以在`tf.dtypes.DType`中查看所有支持的`dtypes`

       标量也称0秩张量：`rank_0_tensor = tf.constant(4)`

       向量也称1秩张量：`rank_1_tensor = tf.constant([2.0, 3.0, 4.0])`

       矩阵也称2秩张量：

       ```python
       rank_2_tensor = tf.constant([[1, 2],
                                    [3, 4],
                                    [5, 6]], dtype=tf.float16)
       ```

      - 3秩张量：

      ```python
      rank_3_tensor = tf.constant([
        [[0, 1, 2, 3, 4],
         [5, 6, 7, 8, 9]],
        [[10, 11, 12, 13, 14],
         [15, 16, 17, 18, 19]],
        [[20, 21, 22, 23, 24],
         [25, 26, 27, 28, 29]],])
      ```

      - 通过使用`np.array`或`tensor.numpy`方法，可以将张量转换为`NumPy`数组
      - 张量支持基本数学运算，包括加法、逐元素乘法和矩阵乘法

     ```python
     a = tf.constant([[1, 2],
                      [3, 4]])
     b = tf.constant([[1, 1],
                      [1, 1]])
     print(tf.add(a, b), "\n")
     print(tf.multiply(a, b), "\n")
     print(tf.matmul(a, b), "\n")
     print(a + b, "\n") # element-wise addition
     print(a * b, "\n") # element-wise multiplication
     print(a @ b, "\n") # matrix multiplication
     ```

     - 同时还支持其他更多的运算

     ```python
     c = tf.constant([[4.0, 5.0], [10.0, 1.0]])
     # Find the largest value
     print(tf.reduce_max(c))
     # Find the index of the largest value
     print(tf.argmax(c))
     # Compute the softmax
     print(tf.nn.softmax(c))
     ```

     - 形状：

     ```python
     rank_4_tensor = tf.zeros([3, 2, 4, 5])
     print("Type of every element:", rank_4_tensor.dtype)
     print("Number of dimensions:", rank_4_tensor.ndim)
     print("Shape of tensor:", rank_4_tensor.shape)
     print("Elements along axis 0 of tensor:", rank_4_tensor.shape[0])
     print("Elements along the last axis of tensor:", rank_4_tensor.shape[-1])
     print("Total number of elements (3*2*4*5): ", tf.size(rank_4_tensor).numpy())
     ```

     - 如果要操作形状，可以使用`tf.reshape()`

     ```python
     print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
     print(tf.reshape(rank_3_tensor, [3, -1]))
     ```

     - 可以使用索引和切片来访问张量

     ```python
     rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
     print("First:", rank_1_tensor[0].numpy())
     print("Second:", rank_1_tensor[1].numpy())
     print("Last:", rank_1_tensor[-1].numpy())
     print("Everything:", rank_1_tensor[:].numpy())
     print("Before 4:", rank_1_tensor[:4].numpy())
     print("From 4 to the end:", rank_1_tensor[4:].numpy())
     print("From 2, before 7:", rank_1_tensor[2:7].numpy())
     print("Every other item:", rank_1_tensor[::2].numpy())
     print("Reversed:", rank_1_tensor[::-1].numpy())
     ```

     - 不规则张量

     ```python
     ragged_list = [
         [0, 1, 2, 3],
         [4, 5],
         [6, 7, 8],
         [9]]
     ragged_tensor = tf.ragged.constant(ragged_list)
     ```

     - 稀疏张量

     ```python
     sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                            values=[1, 2],
                                            dense_shape=[3, 4])
     # We can convert sparse tensors to dense
     print(tf.sparse.to_dense(sparse_tensor))
     ```

- 变量

     - 变量通过`tf.Variable`类进行创建和跟踪
     - 创建变量，需要提供初始值

     ```python
     my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
     my_variable = tf.Variable(my_tensor)
     # Variables can be all kinds of types, just like tensors
     bool_variable = tf.Variable([False, False, False, True])
     complex_variable = tf.Variable([5 + 4j, 6 + 1j])
     ```

     ​	大部分张量运算在变量上也可以按预期运行，不过变量无法重构形状，会新建一个张量

     ```python
     print("A variable:",my_variable)
     print("\nViewed as a tensor:", tf.convert_to_tensor(my_variable))
     print("\nIndex of highest value:", tf.argmax(my_variable))
     # This creates a new tensor; it does not reshape the variable.
     print("\nCopying and reshaping: ", tf.reshape(my_variable, ([1,4])))
     ```

     - 变量支持的运算

     ```python
     a = tf.Variable([2.0, 3.0])
     # Create b based on the value of a
     b = tf.Variable(a)
     a.assign([5, 6])
     
     # a and b are different
     print(a.numpy())
     print(b.numpy())
     
     # There are other versions of assign
     print(a.assign_add([2,3]).numpy())  # [7. 9.]
     print(a.assign_sub([7,9]).numpy())  # [0. 0.]
     ```

     - 还可以为变量命名，两个变量可以使用相同的名称，但是值不一定相等

     ```python
     a = tf.Variable(my_tensor, name="Mark")
     b = tf.Variable(my_tensor + 1, name="Mark")
     print(a == b)
     ```

- 自动微分

     - 梯度带

          TensorFlow 为自动微分提供了 `tf.GradientTape` API；即计算某个计算相对于某些输入（通常是 `tf.Variable`）的梯度。TensorFlow 会将在 `tf.GradientTape` 上下文内执行的相关运算“记录”到“条带”上。TensorFlow 随后会该使用条带通过反向模式微分计算“记录的”计算的梯度。

     - 默认情况下，调用`GradientTape.gradient()`方法，`GradientTape`占用的资源会立即释放，可以通过设置`persistent=True`来创建一个持久的梯度带

     ```python
     x = tf.constant(3.0)
     with tf.GradientTape(persistent=True) as t:
       t.watch(x)
       y = x * x
       z = y * y
     dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
     dy_dx = t.gradient(y, x)  # 6.0
     print(dz_dx)
     print(dy_dx)
     del t  # Drop the reference to the tape
     ```

     - 一阶导数

     ```python
     def f(x):
       return x ** 2 + tf.exp(x)
     
     def grad(x):
       with tf.GradientTape() as t:
         t.watch(x)
         out = f(x)
       return t.gradient(out, x)
     
     x = tf.constant(1.0)
     print(grad(x).numpy())
     ```

     - 高阶导数

     ```python
     x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0
     with tf.GradientTape() as t:
       with tf.GradientTape() as t2:
         y = x * x * x
       # Compute the gradient inside the 't' context manager
       # which means the gradient computation is differentiable as well.
       dy_dx = t2.gradient(y, x)
     d2y_dx2 = t.gradient(dy_dx, x)
     print(dy_dx)
     print(d2y_dx2)
     ```

     

