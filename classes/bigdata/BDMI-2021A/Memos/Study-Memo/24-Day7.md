## 第7节课
### numpy学习
##### numpy的函数

- arrange

建立一个从4到7的向量

```python
import numpy as np
b=np.arange(4,8)
b
```

- mutual

  矩阵乘法

```python
a=np.arrange(4)
b=np.arrange(4,8)
np.matmul(a,b)
```

- array

```python
g=np.array([[1,2,3],[4,5,6]])
print(g)
```

- ones
构建一个全为1的向量或者矩阵

```python
a=np.ones(4)
a
```

- reshape

  可以把一个向量变成一个矩阵

```python
A=np.arange(0,9).reshape(3,3)
A
```

- ravel

  将矩阵铺平

- transpose

  将矩阵转置

- numpy debug

  | 方法名      | 作用                     |
  | ----------- | ------------------------ |
  | type(stuff) | 得到变量类型             |
  | numpy.dtyp  | 查看数组中元素的变量类型 |
  | numpy.shape | 查看数组的形状           |

  

### 人工神经元

##### 激活函数

三个函数：sigmoid函数，tanh函数，ReLU函数

```python
import numpy as np
def sigmoid(x):
    return 1/(1+np.exp(-x))
def tanh(x):
    return (np.exp(x)-np.exp(-x))/(np.exp(-x)+np.exp(x))
def ReLU(x):
	return np.max(x,0)
```

##### 画出函数图像

```python
import matplotlib.pyplot as plt
import numpy as np
x = np.linspace(-5,5,100)
sigmoid = lambda x: 1 / (1 + np.exp(-x))
plt.plot(x,sigmoid(x), color='red', lw=2)
x = np.linspace(-5,5,100)
th = lambda x: np.tanh(x)
plt.plot(x,th(x), color='blue', lw=2)
```

##### 人工神经元1——relu神经元

通过relu函数，计算出输出值

```python
import numpy as np
relu_ = lambda x1,x2,x3 : np.maximum(-0.21*x1+0.3*x2+0.7*x3,0)
print(relu_(1,0,1))
```

##### 人工神经元2——逻辑斯提神经元，通过sigmoid函数实现

```python
import numpy as np
x=np.array([1,0,1])
sigmoid_new = lambda x : 1/(1+np.exp(-(-0.21*x[0] + 0.3*x[1] + 0.7*x[2])))
print(sigmoid_new(x))
```

##### 单个神经元实现与或非

- 与运算，通过sigmoid神经元实现，**当前面两个来源传入的比重是20，附加比重为10**，（比重是可以自己定义）就能实现and运算。

  ```python
  import numpy as np
  x = np.array([0,1])
  weight=10
  and_function = lambda x : 1/(1+np.exp(-(20*x[0] +20*x[1] -weight)))
  and_function(x)
  ```

- 或运算，同理，把weight修正为30

- 非运算，前面两个来源传入的比重是-20，附加比重为10

##### 多个神经元实现异或运算

```
XOR 运算
	输入：X1,X2 {0,1}
	输出{0，1}
	相同的话给出0
	不同的时候给出1
	X1 XOR X2 = {X1 or x2} and {nand{x1,x2}}#nand在x1,x2同时为1时是0
	or
	x1 xor x2 = {x1 or x2} and { (not x1) or (not x2)}
	此时需要三个神经元
```

### 多层神经网络

名词术语 随机梯度下降法

属于监督学习
