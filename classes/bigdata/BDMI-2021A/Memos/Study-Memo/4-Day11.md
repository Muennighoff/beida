# Tensorflow Keras

[TOC]

## AutoDiff 自动微分

目标：
$$
L(w)=\sum_i l(y_i,\hat{y_i})+\lambda||w||^2
$$
更新策略：
$$
w\leftarrow w-\eta \nabla_{w} L(w)
$$

### 符号积分：Symbolic differentiation

1. 已知一些基本函数和复合函数的准确解析求导结果
2. 将最终函数看作是不同符号按照一定顺序组合的结果，并进行求导

缺点：需要大量中间变量，并且符号程指数增长

### 自动微分

一般使用***反向模式***法。

前向模式：

1. 从最初的x1，x2输入开始，每一步进行求导运算（基于符号积分原则），最后得到目标y对x1或x2的偏导数。
2. 最终获得Jacobian矩阵的其中一项

反向模式：

1. 从最后的输出y开始，每一步求对前一步求导的结果，最后得到y对**所有输入**的导数。
2. 最终获得Jacobian矩阵的转置的一列

### 反向传播

1. 先使用前向模式算出（并缓存）每个节点的输出值。
2. 计算出损失函数值
3. 反向模式，得到损失函数对每个节点的导数

## 卷积网络

### 多层人工神经网络

单个cell可以进行线性分类，而多个cell组合可以解决更多问题（比如异或XOR）

若有多层人工神经网络之间有向连接，则能实现复杂的智能功能

* 前馈网络：一个方向往前
* 反馈网络：节点的输出可以当作输入再放入
* 记忆网络：节点每一次更新的输出都存下来，当作输入
* ![截屏2021-11-24 下午2.10.17](4-Day11.assets/截屏2021-11-24 下午2.10.17.png)

**全连接**（密集连接）：输入特征与中间（每个）神经元进行边连接；隐藏层和下一个隐藏层的神经元全连接

### 卷积(convolution)网络的基本结构

卷积网络是**前馈网络**，是**局部连接**的。

<img src="4-Day11.assets/截屏2021-11-24 下午2.17.51.png" alt="截屏2021-11-24 下午2.17.51" style="zoom:33%;" />

卷积算法得到的结果是一个matrix

有填充：上图中的方块将进一步左、上、右、下移动，直到完全离开底层。在没有值的覆盖处填充内容来计算

下采样层：在卷积计算后，取每一个小单元（比如2x2）中的最大/最小/平均值，来减小最终结果的size。称作池化pooling。

​	分别为: max-pooling min-pooling avg-pooling

归一化/正则化层：比如LCN层，给目标减平均除以标准差，标准化。

## Keras 实现卷积网络

Keras本质上是对tensorflow的封装API

顺序模型：直接按顺序调用各种**层**来完成任务

层：各类layer，比如Dean、卷积等等

激活函数：relu，sigmoid等等

优化器：optimizer，最简单的是SGD，带有动量参数。各种优化器侧重点不同。

### 实现流程：以Minist为例

Define-Compile-Fit-Evaluate-Predict-Load/Save

顺序模型：Sequencial

使用add方法将Dean一层层叠加

可以使用迁移学习中的**冻结**来只训练某一层

Layer：

* 













