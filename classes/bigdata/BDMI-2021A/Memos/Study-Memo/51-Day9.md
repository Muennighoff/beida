# 第9课总结

## Tensorflow

#### 张量

从numpy看

`np.random.rand(3,3)`3x3二维数组

`np.random.rand(3,4,5)` 3x4x5三维张量

`np.dot/matmul`点乘积

tensorflow操作

```python
tf.constant(

	value,

	dtype=None,

	shape=None,

	name='Const',

	verify_shape=False

)
#操作方式
rank1=tf.constant([2,3,4])
c=np.random.rand(3,3)
rank2=tf.constant(c)#随机生成
#互相转换
np.array(rank_2_tensor)#转为numpy或者
rank_2_tensor.numpy()
```

`tf.add`对应元素加

`tf.multiply`对应元素乘

`tf.matmul`矩阵乘法

`tf.reduce_max`寻找最大值

`tf.argmax`索引最大值，找最大值位置（列寻找）

`tf.nn.softmax`元素归一化

`tf.reshape(tensor,shape,name=None)`将tensor转换为想要的形状

索引rank_1_tensor[0].numpy()第一个数字

rank_1_tensor[-1].numpy()最后一个数字

rank_1_tensor[2:7].numpy()2 3 4 5 6 共5个数字（0是首位）

rank_1_tensor[::-1].numpy()翻转//与上述直接取段无法直接复合

多轴切片rank2

rank_2_tensor[1,:].numpy()#取第一行（0行为首行）

rank_2_tensor[:,1].numpy()#取第一列（0列为首列）

#### 字符串张量

和普通张量类似，正常定义

#### Tensorflow变量

my_variable=tf.Variable(my_tensor)#储存

变量的值可用assign改变

eg  a=tf.Variable([2,3])

a.assign([1,2])

#### 自动微分

```python
x=tf.constant(1.)
with tf.GradientTape(persistent=True) as t:
  t.watch(x)
  y=x*x+tf.exp(x)
dy_dx=t.gradient(y,x).numpy()
print(dy_dx)
#2次微分则
x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0
with tf.GradientTape() as t:
  with tf.GradientTape() as t2:
    y = x * x * x
  # Compute the gradient inside the 't' context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t2.gradient(y, x)
d2y_dx2 = t.gradient(dy_dx, x)
print(dy_dx)
print(d2y_dx2)
```

