# Week 11 Summary

# 课程大纲
```
Keras学习
自动微分
卷积网络

理论部分
	自动微分
	深度神经网络的架构
	深度卷积网络结构

学习Keras
	总体介绍
	顺序模型
	层和模型
	模型的保存和加载
```

## 自动微分 AutoDiff
```
求导运算
	数值微分
	符号微分
	自动微分

数值微分：
	根据导数定义进行运算
		计算效率低（缺点
		有误差（缺点
		比较简单（优点
		
符号微分：
	从表达式出发，将表达式本身看作是符号的运算，直接得到求导结果的表达式
	d(f+g)/dx = df/dx + dg/dx
	d(fg)/dx = g * df/dx + f * dg/dx
		随着函数复杂化，复杂程度指数级增加
		需要保存大量中间变量，容易出错

自动微分
	前向模式
	反向模式
	
	
自动（auto）有点歧义，代表的是

方法出现于1950

前向模式：
	将函数中的表达式用v表示 x1 --> v-1, x2 --> v0
	ln(x1) + x1x2 - sin x2
	ln(x1) --> v1
	x1x2 --> v-1 * v0 --> v2
	...
	y --> v5
	计算某个输入的答案，根据给定要求（x1 = 1),计算dy/dx1

反向模式：
	计算输出对 所有 输入的微分值
	
	同样是前向模式的计算图
	输入 v-1 = 2(x1), v0 = 5(x2), 得到v5(y)
	但是之后的求导是从y开始， y = v5 = 1, 最终得到x1 & x2
	
	
反向传播：
	在神经网络进行梯度下降法的主要算法
	根据算出来的梯度值进行调整
	
tensorflow 1.0 和 2.0 有较大区别
2.0 是动态计算图
```

## 卷积网络
```
基本结构：
	卷积层
	池化层
	归一化层
应用：
	图像分类
	人脸识别
	对象检查
	马路驾驶
	
多层人工神经网络：
	不同层神经元之间的链接关系
	前馈网络（之前学的
	反馈网络（输出可以变成输入
	记忆网络（每个节点都保存了以前的状态，可以作为输入
	图网络结构（新的）（知道就行）
	
	全连接(Fully-Connected)
		每个神经元都和下一层的每一个链接

卷积网络（局部链接），和全链接不同

卷积核（Kernel）
卷积运算（张量运算）
	无填充式的卷积运算 （3x4)中选择所有(2x2)和kernel相乘
	
2d卷积核（有填充式）(P, padding), (zero-padding)
	卷积核小于输入
3d卷积核
	输入 [10,10,3]
	一个卷积核[4,4,3]生成[9,9]
	二个卷积核[4,4,3]生成[9,9]
	最终形成 张量 [9,9,2(代表卷积核个数)]
	
	卷积核数量越多，越深
	
通常卷积层后紧跟着下采样层(subsample/ down sampling)
	目的是缩小输出张量的大小
	[14,14](输入), kernel[5,5] --> [10,10] -下采样后-> [5,5]
	
归一化层（正则化层）（LCN）
	减去平均值，除以标准差
	
局部链接（卷积网络）好处：
	相对全连接来说：
		计算快，参数的个数少，可以并行化

分类问题可以使用全连接来做（dense net）
	缺点：1个神经元有120,000参数
	如果是3层，每层10个神经元：3,600,000参数
	计算量太大
	
所以推荐卷积网络：
	参数少
	并行化
	运算快
	
```

## keras
```
代码简洁
本质是高级API
	基于tensorflow2的快速搭建深度神经网络的程序库
方便易用
import tensorflow as tf
from tensorflow import keras

核心概念：
	模型(model)
	最简单的model类型是顺序模型

# Define Sequential model with 3 layers
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu", name="layer1"),
        layers.Dense(3, activation="relu", name="layer2"),
        layers.Dense(4, name="layer3"),
    ]
)
# Call model on a test input
x = tf.ones((3, 3))
y = model(x)

核心概念 层
	tf.keras.layers
	层包括 dense, flatten, dropout, ...
	卷积层，...

核心概念 激活函数
	tf.keras.activations
		包括 relu, elu, tanh, softmax, sigmoid, ...
		
核心概念 优化器
	SGD
	
	自适应调整学习率的算法
		RMSprop, Adagrad, Adam
	用的最多的是 nadam, RMSprop
```

## keras 使用
```
mnist = tf.keras.datasets.mnist #7万图片，一般5万训练，1万测试，1万实验

1 定义网络（定义大小）
2 编译网络（指定优化器、指定函数、损失函数）
3 训练网络（进行拟合的过程，输出损失及准确度）
4 评估网络（检测训练效果）
5 数据预测（进行预测）
6 保存/载入网络（不推荐hdf5格式保存）
```

## keras 顺序模型
```
适用范围：
	一个普通堆栈的图层，每个图层只有一个输入张量和一个输出张量
不适用范围：
	模型有多个输入/输出
	任何层有多个输入/输出
	需要做图层共享
	需要非线性拓扑（多分子模型、剩余链接residual）


创建模型方法：
model = keras.Sequential()
model.add(layers.Dense(2, activation='relu'))（添加
model.pop()（删除
特定删除：
	先命名 2,activation = '', name = 'layer2'

预先指定输入形状
layer = layers.Dense(3)
layer.weights  # Empty


使用/调用模型：
model.summary() #观察训练成果，会打印模型


基于顺序模型的迁移学习
	冻结最上层外所有层
	for layer in model.layers[-1]:
		layer.trainable = False
```

## keras 层和模型
```
layer类
	封装了状态，但是变量w和b要自己定义
	
添加权重：
	add_weight(shape=(units,), initializer='zeros', trainable=True)
	
可以添加不可训练权重
	trainable=False

将权重创建推迟到得知输入的形状之后
```

## keras的保存和加载 (实际应用中经常用到的函数)
```
三种形式：
1 保存和加载整个模型：
保存了优化器及状态（断线后可以重新恢复）
model.save()/save_model()
tf.keras.models.load_model()
保存形式：
	创建 my_model 的文件夹
		assets
		saved_model.pb （模型）
		variables （训练的权重值）
2 保存架构：
get_config() 返回包含模型配置的Python字典
from_config() 重建同一模型

3 仅保存和加载模型的权重值（共享模型的权重值）
get_weights() 返回numpy数组列表
set_weights()

model.save_weights() #保存到磁盘
load_weights() #从磁盘恢复到模型
```

## keras使用示例（鲜花分类）
```
下载数据集
创建数据集（训练集，验证集）
可视化数据
配置数据提高性能
归一化数据
创建模型
编译模型（使用adam作为优化器,
训练模型
查看训练结果
预测新数据

过度拟合：
	从不重要的细节学到不必要的东西（噪声）
	数据太少
	稍微变化数据就会得到不同结果
	解决方法：数据增强
		RandomFlip
		RandomRotation(0,1)
		RandomZoom(0,1)
		dropout（随机无效一些数据，使得每次都是不一样的）
	
```