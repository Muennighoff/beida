# BDMI Day8

`Student No.52`

## Pandas

**数据结构**

* 一维数据Series

  * 初始化

    ```python
    s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
    ```

    ```python
    a    0.136437
    b    1.014519
    c   -1.519393
    d   -0.144638
    e    2.165888
    dtype: float64
    ```

* 二维数据DataFrame

  * 初始化

    ```python
    d = {'one' : pd.Series([1.,2.,3.],index=['a','b','c']),
         'two' : pd.Series([1.,2.,3.,4.],index=['a','b','c','d'])}
    df = pd.DataFrame(d)
    ```

    ```python
    df_data = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))
    ```

  * 存储为csv文件

    ```python
    df.to_csv('BDMI.csv')
    df_2 = pd.read_csv('BDMI.csv')
    ```

  * 索引

    ```python
    df['one']
    df.loc['a']
    df.iloc[0]
    ```

  * 初始化时间片

    ```python
    dates = pd.date_range('20201013',periods=6)
    ```


**连接数据库**

```python
from sqlalchemy import create_engine

engine = create_engine('sqlite:///test.db')
#写入数据库
df_classmates.to_sql('tb_test',con=engine)
#读取数据库
with engine.connect() as conn, conn.begin():
    data = pd.read_sql_table('tb_test', conn)
```

## 多层神经网络

其实神经网络就像是**logistic regression**，只不过我们把**logistic regression**中的输入向量$\left[ x_1\sim {x_3} \right]$ 变成了中间层的$\left[ a_1^{(2)}\sim a_3^{(2)} \right]$, 即:  $h_\theta(x)=g\left( \Theta_0^{\left( 2 \right)}a_0^{\left( 2 \right)}+\Theta_1^{\left( 2 \right)}a_1^{\left( 2 \right)}+\Theta_{2}^{\left( 2 \right)}a_{2}^{\left( 2 \right)}+\Theta_{3}^{\left( 2 \right)}a_{3}^{\left( 2 \right)} \right)$ 
我们可以把$a_0, a_1, a_2, a_3$看成更为高级的特征值，也就是$x_0, x_1, x_2, x_3$的进化体，并且它们是由 $x$与$\theta$决定的，==因为是梯度下降的，所以$a$是变化的，并且变得越来越厉害，所以这些更高级的特征值远比仅仅将 $x$次方厉害，也能更好的预测新数据。==
==这就是神经网络相比于逻辑回归和线性回归的优势。==



- 网络层数越多
  - 一方面，神经网络的表达能力增强了
  - 另一方面，神经网络的权重数量增加了

一般的流程
1.确定损失函数(Loss):定义损失函数
2.权重初始化( Initialization):随机初始化？
3.反向传播( Back Propagation):计算损失函数对权重的梯度
4.权重修正( weights Adjusting):随机梯度下降法？

### 损失函数

* 均方误差(MSE)--常用于线性回归模型，**回归问题**

  * ```
    def MSE(X, real):
        return np.sum((X-real)**2)/len(X)
    ```

  * `tf.keras.losses.MSE`

<u>方差( variance)</u>是各个数据和**==平均数==**之差的平方和的平均数。
<u>标准差（均方差）( standard deviation)</u>是方差的平方根。标准差也称为均方差。
*均方误差( mean squared error,MSE)*是各个数据偏离**==真实值==**差值的平方和的平均数，也就是误差平方和的平均数
*均方根误差（ Root Mean Squared Error，RMSE）*是均方误差的开方

* 交叉熵(CE)--常用于逻辑回归模型，**分类任务**

  * **负对数似然损失函数**
  
  * $H_{y'}(y)=-\sum_{i}y_i'log(y_i)$
  
  * ```python
    def CE(y,label_y):
        return -np.sum(label_y*np.log(y))
    ```
  
  * `tf.keras.losses.categorical_crossentropy`
  

神经网络实际输出y，预期输出（训练样本对应的标签）y'

似然和概率是两个概念，似然是针对过去发生的事件，而概率是某一未来事件发生的可能性。

### 反向传播

> 损失函数对权重的梯度计算

**反向传播算法**-损失函数对权重的梯度计算

- **在神经网络上执行梯度下降法的主要算法**
- 根据损失函数的性质以及链式求导法则
- 反向逐层计算损失函数对权重的梯度

**随机梯度下降法(SGD)**--权重更新

* 权重更新与学习率
  $$
  \theta=\theta-\eta\cdot\nabla_\theta J(\theta)
  $$

  > * 学习率：$\eta$，也即步长
  > * $\nabla_\theta J(\theta)$是经过反向传播算法计算出来的关于权重$\theta$的梯度

* SGD步骤

  * 随机初始化每个神经元权重和偏差
  * 选取一个随机样本
  * 反向传播算法计算出每层权重的梯度
  * 逐层调整每层的权重，产生新的权重值
  * 返回步骤2，继续随机选取下一个样本

* 正则化--解决过拟合问题

###  神经网络的实际训练

- 整个训练集称为一个批次(batch)，切分为多个大小相同的子集
  - 每个子集称为mini-batch，又称为小批量
  - 每个子集的大小参数称为mini-batch-size
- 小批量学习是批量学习和随机梯度下降法的一个折中
  - 每个mini-batch的数据被一次送入网络中进行训练
  - 训练完一个mini-batch，称为一次迭代(iteration)
  - 训练集中所有训练样本都送入网络，完成一次训练的过程，称为一个时代(epoch)
- 当mini-batch大小为1时，就是普通的SGD

### 神经网络运用的一般流程

- 准备数据集
- 搭建网络模型
  - 多层神经网络结构
  - 相应的每层的权重数值
- 训练模型
- 测试模型
