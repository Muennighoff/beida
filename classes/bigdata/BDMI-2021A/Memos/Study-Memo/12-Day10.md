# Python类&继承

## 类 Class

```python
class MyClass:   #类 class的定义
    i = 123456   #属性 attribute, 此类的每个对象都有相同属性
    def __init__(self, a, b):   #构造函数
        self.a = a
        self.b = b
        
    def __call__(self):   
        print("hello"+self.a)
        
    def func(self):   #方法 method, self是指针(this)
        return 'hello world'

class1 = MyClass(5,6)   #创建对象 object
print(x.i)
print(x.func())
class1()   #call
```



## 类的继承 Inheritance

子类继承父类的所有属性和方法，同时可以增加新的属性/方法，也能覆盖父类的属性/方法

```python
class People:   #定义父类/基类
    def __init__(self,n,a):   #构造函数
        self.name = n
        self.age = a
    def speak(self):   #父类方法
        print("%s 说：我今年 %d 岁。"%(self.name,self.age))

class Student(people):   #定义子类/派生类
    def __init__(self,n,a,g):
        people.__init__(self,n,a)   #调用父类的构造函数
        self.grade = g
    def speak(self):   #覆写 overridden，方法名字和父类相同
        print("%s 说：我今年 %d 岁,在读 %d 年级"%(self.name,self.age,self.grade))

s = Student('ken',10,3)
s.speak()
```



#### 重载父类方法 super()

```python
s2 = Student('Billy',11,4)
s2.speak() #调用子类方法
super(Student,s2).speak() #调用父类方法
```



# TensorFlow 模块、层、模型

## 模块 Module

- 可以定义、保存、回复
  - 对张量进行计算的函数
  - 变量在训练中被更新

```python
import tensorflow as tf
class SimpleModule(tf.Module):   #继承tf.Module类
    def __init__(self, name=None):
        super().__init__(name=name)
        self.a = tf.Variable(5.0, name ="train_me")
        self.b = tf.Variable(2.0, trainable=False, name = "do_not_train")
    delf __call__(self,x):
        return self.a*x+self.b   #a*x+b

module_1 = SimpleModule(name = 'simple')
module_1(tf.constant(5.0))  #call
print(module_1.trainable_variables) #显示所有可训练变量
print(module_1.variables)   #显示所有变量(tf.module的method)
```



## 层 Layer(Dense)

```python
class Dense(tf.Module): #继承tf.Module类
    def __init__(self , in_features , out_features , name=None): #in_features输入维度，out_features输出维度
        super().__init__(name=name)
        self.w = tf.Variable(tf.random.normal([in_features, out_features]), name='w')
        self.b = tf.Variable(tf.zeros([out_features]), name='b')
    def __call__(self,x):
        y = tf.matmul(x,self.w)+self.b  #w*x+b
        return tf.nn.sigmoid(y)

x=tf.constant(tf.random.normal([1,in_features])) #in_features = 3
dense = Dense(3,1)
np.array(dense(x))
```

<img src="https://i.imgur.com/WDtcjGT.png" alt="image-20211120220306798" style="zoom:50%;" />

w*x+b, matmul(x,w)+b=[1,3]\*[3,1]+[1,1]

- 权重 w : [3,1]
- 输入层 x : [1,3]
- 偏置 b : [1,1]
- 输出层 y : [1,1]



## 模型 Model(Sequential module)

- 隐藏层$\ge1$                                             <img src="https://i.imgur.com/eFA7osA.png" alt="image-20211120221550202" style="zoom: 33%;" />

```python
class MySequentialModule(tf.Module):
    def __init__(self , name=None):
        super().__init__(name=name)
        
        self.dense1 = Dense(in_features=3, out_features=3)
        self.dense2 = Dense(in_features=3, out_features=2) #n层的 out_features = n+1层的 in_features
                            
    def __call__(self,x):
        x = self.dense1(x)
        return self.dense2(x)
```



### 模型的存储和恢复

```python
#存储模型
ckpt_path = "my_checkpoint" #文件名
checkpoint = tf.train.Checkpoint(model = my_model)
checkpoint.write(ckpt_path) #写入文件
tf.train.list_variables(ckpt_path) #遍历所有变量

#恢复模型
new_model = SequentialModule()
new_checkpoint = tf.train.Checkpoint(model = new_model)
new_checkpoint.restore("my_checkpoint")
new_model(tf.constant([2.0,2.0,2.0]))
```

#### Tensorboard可视化

```python
# Set up logging.
stamp = datetime.now().strftime("%Y%m%d-%H%M%S")
logdir = "logs/func/%s" % stamp
writer = tf.summary.create_file_writer(logdir)   #指定输出路径

# Create a new model to get a fresh trace
# Otherwise the summary will not see the graph.
new_model = MySequentialModule()

# Bracket the function call with
# tf.summary.trace_on() and tf.summary.trace_export().
tf.summary.trace_on(graph=True, profiler=True)   #开始记录计算图
# Call only one tf.function when tracing.
z = print(new_model(tf.constant([[2.0, 2.0, 2.0]])))
with writer.as_default():
  tf.summary.trace_export(   #停止记录，把之前的记录导出
      name="my_func_trace",
      step=0,
      profiler_outdir=logdir)

%tensorboard --logdir logs/func
```



# 计算图 Graph

Graph：记录一系列tensorflow操作&Tensor张量的数据结构

Graph的种类：静态图(tf.function)、动态图(eager)

优势：

- 灵活性、效率高、容易优化
- 用Grappler进行图的优化和加速

<img src="https://raw.githubusercontent.com/tensorflow/docs/master/site/en/guide/images/intro_to_graphs/two-layer-network.png" alt="img" style="zoom:33%;" />



## Graph的建立

tf.function建立图并追踪

```python
import tensorflow as tf
import numpy

@tf.function   #装饰器
def py_function(x, y, b): #python function
  x = tf.matmul(x, y)
  x = x + b
  return x

graph_function = tf.function(py_function)

# Make some tensors.
x1 = tf.constant([[1.0, 2.0]])
y1 = tf.constant([[2.0], [3.0]])
b1 = tf.constant(4.0)

orig_value = py_function(x1, y1, b1).numpy()
# Call a `Function` like a Python function.
tf_function_value = graph_function(x1, y1, b1).numpy()
assert(orig_value == tf_function_value)
```



## Graph的控制流

默认情况下会转换为TensorFlow

```python
def simple_relu(x):
  if tf.greater(x, 0):
    return x
  else:
    return 0

tf_simple_relu = tf.function(simple_relu)

print("First branch, with graph:", tf_simple_relu(tf.constant(1)).numpy())
print("Second branch, with graph:", tf_simple_relu(tf.constant(-1)).numpy())
```



### Graph的加速(优化)

- 可以减少通信开销，显著加快复杂的运算

### Graph的多态性

- 每次调用function可以使用不同的数据类型(dtype)和形状(shape)

```python
print("Int:",a_function(tf.constant(2)))
print("Float:",a_function(tf.constant(2.0)))
print("Tensor:",a_function(tf.constant([2.0,2.0,2.0])))
```

### Graph的动态模式(eager)

容易debug

恢复eager模式的方法：

- 直接调用模型和图层
- 使用Keras编译/拟合时， 在编译时使用 `model.compile(run_eagerly=True)`
- 通过设置全局执行模式 `tf.config.run_functions_eagerly(True)`











# 训练流程 Training loop

#### 1.获取训练数据

$f(x)=x*W+b$

#### 2.定义模型

- 用变量表示$W$和$b$ `tf.Variable`

- 给出初始值 

- 用模块封装变量和计算 `class MyModel(tf.Module)`

- 验证模型的有效性 `assert model(3.0).numpy()== n`

#### 3.定义损失函数

```python
def loss(target_y, predicted_y):
return tf.reduce_mean(tf.square(target_y - predicted_y))
#可视化损失值
plt.scatter(x,y,c='b')
plt.scatter(x,model(x),c='r')
plt.show()
print("Current loss: %1.6f"%loss(model(x),y).numpy())
```

#### 4.运行训练数据， 从目标值计算损失

训练循环：

​	1.发送一批输入到模型中生成输出

​	2.比较生成的输出和目标输出，计算损失

​	3.用GradientTape计算损失loss对权重w的梯度

​	4.用梯度优化变量w和b

#### 5.计算损失的梯度， 并使用优化器来调整变量以适应数据

#### 6.结果评估

- 复杂的模型用keras