# 课程总结10

## 1.类的继承

```
#基类/父类
class people():
	def __init__(self,n,a):
		self.name = n
		self.age = a
	def speak(self):
		print("%s说：我%d岁。"%(self.name,self.age))
	
	#定义了call()方法。调用实例化的类：
	def __call__(self):
		print('hello'+self.name)

#派生类/子类
class student(people):
	def __init__(self,n,a,w,g):
		people.__init__(self,n,a) #调用父类的构造函数，从而继承父类的属性
		self.grade = g
	def speak(self): #定义同名的speak函数，将父类的speak方法覆写
		print("%s说:我%d岁了,我在读%d年级"%(self.name,self.age,self.grade))

s = student('ken',10,60,3) #实例化student类
s.speak() #调用这个实例化的student类的方法

#super()就是超，父的意思，返回()里的类的父类，同时保留了属性。
super(student,s).speak() #用student这个子类对象，调用已被覆写的父类的speak方法
```

## 2.模块

对张量进行计算的函数

```
import tensorflow as tf
from datetime import datetime
%load_ext tensorboard

class SimpleModule(tf.Module):
	def __init__(self,name=None):
		super().__init__(name=name)
		self.a_variable = tf.Variable(5.0,name="train_me") #权重可以训练
		self.non_trainable_variable =tf.Variable(5.0,trainable=False,name="do_not_train_me") #偏置不训练				
	#定义call方法
	def __call__(self,x):
		return self.a_variable*x + self.non_trainable_variable

simple_module = SimpleModule(name='Simple') #实例化
simple_module(tf.constant(5.0)) #调用call方法

#输出各种变量
#输出可训练的变量
print("trainable variables:",simple_module.trainable_variables)
#输出所有的变量
print("all variables:",simple_module.variables)

#模型储存，检查点
chkp_path = "my_checkpoint"
checkpoint = tf.train.Checkpoint(model=my_model)
checkpoint.write(chkp_path)
checkpoint.write(chkp_path)
```

## 3.tensorflow 密集(线性)层

可重用的带参数的结构

```
class Dense(tf.Module):
  def __init__(self, in_features, out_features, name=None):
    super().__init__(name=name)
    self.w = tf.Variable(
                         tf.random.normal([in_features, out_features]), 								 name='w')
    self.b = tf.Variable(tf.zeros([out_features]), name='b')
  def __call__(self, x):
    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)
```

## 4.模型

```
class SequentialModule(tf.Module): 
    def __init__(self, name=None): 
        super().__init__(name=name) 
        self.dense_1 = Dense(in_features=3, out_features=3) 
        self.dense_2 = Dense(in_features=3, out_features=2) 
    def __call__(self, x): 
        x = self.dense_1(x) 
        return self.dense_2(x) 
# You have made a model! 
my_model = SequentialModule(name="the_model") 
# Call it, with random results 
print("Model results:", my_model(tf.constant([[2.0, 2.0, 2.0]]))) 

Model results: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)

# 存储和恢复
class MySequentialModule(tf.Module): 
    def __init__(self, name=None):  
        super().__init__(name=name) 
        self.dense_1 = Dense(in_features=3, out_features=3) 
        self.dense_2 = Dense(in_features=3, out_features=2) 
    @tf.function 
    def __call__(self, x): 
        x = self.dense_l(x) 
        return self.dense_2(x) 
# You have made a model with a graph! 
my_model = MySequentialModule(name="the_model") 
```

## 5.GRAPH

```
#Define a Python function 
def function_to_get_faster(x, y, b):
	x=tf.matmul(x. y) 
    x=x+b 
	return x 

#Create a 'Function' object that contains a graph 
a_function_that_uses_a_graph=tf.function(function_to_get_faster) 

#Make some tensors 
xl=tf.constant([[1 0. 2.0]]) 
yl=tf.constant([[2.0], [3.0]]) 
b1=tf.constant(4.0) 

#it just works! 
a_function_that_uses_a_graph(xl, yl. b1)numpy() 
```

## 6. tensorboard

```
# Set up logging. 
stamp = datetime.now().strftime("%Y%m%d-%H%M%S") 
logdir = "logs/func/%s" % stamp 
writer = tf.summary.create_file_writer(logdir) 

# Create a new model to get a fresh trace 
# Otherwise the summary will not see the graph. 
new_model = MySequentialModule() 

# Bracket the function call with 
# tf.summary.trace_on() and tf.summary.trace_export(). 
tf.summary.trace_on(graph=True) 
tf.profiler.experimental.start(logdir) 
# Call only one tf.function when tracing. 
z = print(new_model(tf.constant([[2.0, 2.0, 2.0]]))) 
with writer.as_default(): 
    tf.summary.trace_export( name="my_func_trace", step=0, profiler_outdir=logdir) 
```

## 7.数据处理

```
#tf.data.Dataset用于表示序列的元素,每个元素是一个基本的训练样本,用一对张量表示图像和对应的标签
#读取list
dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])
for elem in dataset:
  print(elem.numpy())
#读取numpy数据
dataset = tf.data.Dataset.from_tensor_slices((images, labels))
#读取TFRecord数据
dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])
#读取文本数据
dataset = tf.data.TextLineDataset(file_paths)
for line in dataset.take(5):
  print(line.numpy())
#读取图片
img_raw = tf.io.read_file(img_path)
#解码
img_tensor = tf.image.decode_image(img_raw)
#重构
img_final = tf.image.resize(img_tensor, [192, 192])
#归一化
img_final = img_final/255.0
#利用高级API读取图片
img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)
images, labels = next(img_gen.flow_from_directory(flowers))

import ssl
ssl._create_default_https_context = ssl._create_unverified_context

file = tf.keras.utils.get_file("train.csv", "https://gitee.com/zhenchen3419/BDMI-2021A/raw/master/Computing/logistic_regression/data.csv") # file path is returned
import pandas as pd
df = pd.read_csv(file, index_col=None)
df.head()


#读取表格数据
titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name="survived")
#数据混排
dataset.shuffle(buffer_size=100)
#预处理函数
dataset.map()
#类别不平衡数据的处理
balanced_ds = tf.data.experimental.sample_from_datasets(
[negative_ds, positive_ds],[0.5,0.5]).batch(10)
```

## 8.附在最后：课堂分类例子具体代码

#### 1．获取训练数据。

f(x)=x*W+b

W：权重

b：偏置

```
# The actual line 
TRUE_W = 3.0 
TRUE_B = 2.0
NUM EXAMPLES = 1000 

# A vector of random x 
values x = tf.random.normal(shape=[NUM_EXAMPLES]) 

# Generate some noise
noise = tf.random.normal(shape=[NUM_EXAMPLES]) 

# Calculate y 
y = x * TRUE_W + TRUE_B + noise 
```

#### 2.定义模型

．用变量表示权重和偏置 ．给出初始值 ・使用模块封装变量和计算 ・验证模型的有效性

```
class MyModel(tf.Module): 
    def __init__(self, **kwargs)： 
    	super().__init__(**kwargs) 
		# Initialize the weights to '5.0' and the bias to '0.0' 
		# In practice, these should be randomly initialized 
		self.w = tf.Variable(5.0) 
		self.b = tf.Variable(0.0) 
        
	def __call__(self, x): 
    	return self.w * x + self.b 

model = MyModel() 
# List the variables tf.modules's built-in variable aggregation. 
print("Variables:", model.variables) 

# Verify the model works assert 
model(3.0).numpy() == 15.0 
```

#### 3．定义损失函数

损失函数度量给定输入模型的输出与目标输出的匹配程度。

```
# This computes a single loss value for an entire batch 
def loss(target_y, predicted_y): 
    return tf.reduce_mean(tf.square(target_y - predicted_y)) 
```

·可视化损失值 红色：模型的预测值

蓝色：训练数据

```
plt.scatter(x, y, c="b") 
plt.scatter(x, model(x), c="r") 
plt.show() 
print("Current loss: %1.6f" % loss(model(x), y).numpy()) 
```

#### 4．运行训练数据，从目标值计算损失

・训练循环由重复执行的任务组成，依次为： 1．通过发送一批输入到模型中以生成输出

2．通过生成的输出与目标输出的比较来计算损失

3．使用GradientTap计算损失loss对权重w的梯度

4．用梯度优化变量w,b

使用梯度下降来训练这个模型。

```
# Given a callable model, inputs, outputs, and a learning rate... 
def train(model, x, y, learning_rate): 
	with tf.GradientTape() as t: 
        # Trainable variables are automatically tracked by GradientTape 
        current_loss = loss(y, model(x)) 
	
    # Use GradientTape to calculate the gradients with respect to W and b 
    dw, db = t.gradient(current_loss, [model.w, model.b]) 
	
    # Subtract the gradient scaled by the learning rate 
    model.w.assign_sub(learning_rate * dw) 
    model.b.assign_sub(learning_rate * db) 
```

#### 5．计算损失的梯度，并使用优化器来调整变量以适应数据。

```
model = MyModel() 

# Collect the history of W-values and b-values to plot later 
Ws, bs = [], [] 
epochs = range(10) 

# Define a training loop 
def training_loop(model, x, y): 
	for epoch in epochs: 
        # Update the model with the single giant batch 
        train(model, x, y, learning_rate=0.1) 
		
        # Track this before I update 
        Ws.append(model.w.numpy()) 
        bs.append(model.b.numpy()) 
        current_loss = loss(y, model(x)) 
		
        print("Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f" % (epoch, Ws[-1], bs[-1], current_loss)) 
print("Starting: W=%1.2f b=%1.2f, loss=%2.5f" % (model.w, model.b, loss(y, model(x)))) 
# Do the training 
training_loop(model, x, y) 
# Plot it 
plt.plot(epochs, Ws, "r", epochs, bs, "b") 
plt.plot([TRUE_W] * len(epochs), "r--", [TRUE...81 * len(epochs), "b--") 
plt.legend(["W", "b", "True W", "True V]) 
plt.show() 
```

#### 6.结果评估

```
# Visualize how the trained model performs 
plt.scatter(x, y, c="6") 
plt.scatter(x, model(x), c="r") 
plt.show() 

print("Current loss: %1.6f" % loss(model(x), y).numpy()) 
```

#### 使用keras模型

```
class MyModelKeras(tf.keras.Model): 
    def __init__(self, **kwargs): 
        super().__init__(**kwargs) 
        # Initialize the weights to '5.0' and the bias to '0.0' 
        # In practice, these should be randomly initialized 
        self.w = tf.Variable(5.0) 
        self.b = tf.Variable(0.0) 

    def __call__(self, x, **kwargs): 
        return self.w * x + self.b 

keras_model = MyModelKeras() 

# Reuse the training loop with a Keras model 
training_loop(keras_model, x, y) 

# You can also save a checkpoint using Keras's built-in sup' 
keras_model.save_weights("my_checkpoint") 
keras_model = MyModelKeras() 

# compile sets the training paramaeters 
keras_model.compile( 
    # By default, fit() uses tf.function(). You can 
    # turn that off for debugging, but it is on now. 	
    run_eagerly=False, 
	# Using a built-in optimizer, configuring as an object   
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1), 
	# Keras comes with built-in MSE error 
    # However, you could use the loss function 
    # defined above 
    loss=tf.keras.losses.mean_sguared_error, 
)
```

