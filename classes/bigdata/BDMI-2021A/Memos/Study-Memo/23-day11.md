# 第11次课总结

## 1.自动微分

* 前向微分：从某一个输入出发，前向微分依次获得各级输出关于该输入的微分梯度；缺点是只能获得输出对单一输入的梯度。
* 反向微分：从某一个输出出发，反向微分依次获得该输出关于各个输入的梯度；优点是可以获得多个多个输入对应的梯度。

## 2.网络

### 多层网络

多层网络分输入层、隐藏层、输出层。前一层每一个神经元均连接后一层每一层神经元的多层网络称为全连接（Dense）。多层网络也称深度神经网络，实际效果优异。

### 卷积网络

卷积网络基本结构分卷积层、归一化层、池化层、随机丢弃层。

#### 基本结构

* 卷积层由输入和卷积核组成，返回输入张量在卷积核内的线性组合，并通过非线性激活单元
* 归一化层对池化结果做处理，例如LCN层拟合标准正态分布。
* 池化层对卷积结果做处理，目的是缩小输出张量大小，例如取范围内最大值或平均值。
* 随机丢弃层会随机丢弃部分输出，以便学习时发生突变并保留有益突变。

### 应用

* Alex net，实现大赛最优图像识别。
* 手写数字识别、物品识别分类、人脸识别、自动驾驶识别。

## Keras

一款高级API，实现tensorflow2的神经网络极简化架构。

### 模型

例如顺序模型，按照一层接一层的顺序输出：

```python
import tensorflow as tf
from tensorflow import keras
model = keras.Sequential(
[
    layers.Dense(2,activation='relu',name='layer1'),
    layers.Dense(3,activation='relu',name='layer2'),
    layers.Dense(4,name='layer3'),
]
)

x=tf.ones((3,3))
y=model(x)
```

### 层

tf.keras.layers：

model中的各层，包括

* Dense，Activation，Dropout，Flatten，Reshape
* Permute，RepeatVector，Lambda，Masking

等

### 激活函数

每一层各神经元使用的激活函数，例如softmax，relu，elu，tanh，sigmoid等

### 优化器

可以优化网络收敛速度，例如RMSprop、Adagrad自适应调整学习率，Nadam加速梯度算法提升速度

### 使用流程

1. 定义网络：model=Sequential()
2. 编排网络：model.compile(optimizer,loss,metrics)，分别表示梯度下降方式、损失优化目标、观察准确度指标变化方式
3. 网络训练：model.fit()
4. 评估网络：model.evaluate()
5. 数据预测：model.predict()
6. 保存/载入：model.save(), tf.keras.models.save_model()/tf.keras.models.load_model()

