## 第八周小结

#### pandas

##### 数据结构

- 一维数据结构——series

  ```python
  import numpy as np
  import pandas as pd
  s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
  ```

  注意index一定要与data的长度一致，data可以是字典，ndarray或者是标量。index默认为0,1,2,3

  可以直接对series进行加乘运算

- 二维数据结构

  ```python
  d = {'one' : pd.Series([1.,2.,3.],index=['a','b','c']),
       'two' : pd.Series([1.,2.,3.,4.],index=['a','b','c','d'])}
  df = pd.DataFrame(d)
  pd.DataFrame(d,index=['d','b','a'],columns=['two','three'])#这时候three这一列就会全部是NULL
  ```

  ```python
  dates = pd.date_range('20201013',periods=6)
  df_data = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))
  #columns参数是表示列名
  ```

  | 操作           | 语法                            | 结果           |
  | -------------- | ------------------------------- | -------------- |
  | 选出列         | df[col]#列号                    | 一维数据series |
  | 根据行号选出行 | df.loc[label]#选行的时候要用loc | series         |
  | 根据数字选出行 | df.iloc[loc]                    | series         |
  | 选出多行       | df[5:10]                        | dataframe      |



#### tensorflow playground

[tensorflow playground](https://playground.tensorflow.org/)

```
学习率：太大会震荡，太小收敛速度过慢

激活函数，将最终层的结果进行激活

噪声：在机器学习中我们在独立随机抽样的时候会出现一些搞错的信息，这些错误的数据我们称之为杂讯（或者噪音  noise），一般可以归结为一下两种（以二分为例）：

输出错误：1.同样的一笔数据会出现两种不同的评判  2.在同样的评判下会有不同的后续处理。

输入错误：1.在收集数据的时由于数据源的随机性会出现错误（比如说，客户在填信息的时候出现的误填）
```



#### 深度学习

##### 基础知识

- 深度学习是机器学习中监督学习的一种方式
- 深度学习可用与回归或者分类问题
- 具体实现中要准备数据集（时尚mnist集，手写数组mnist集），测试集和验证集

##### 基本处理

- softmax函数，将一个向量转化为所有分量和为1的概率向量。（在分类中，最大的分量就是最有可能分到的一类）

  ```python
  def softmax(x):
  	x = np.exp(x)
  	return x/np.sum(x)
  ```

- 分对数logit

  把（0,1）的数变换到(−∞,+∞)，与softmax函数作用相反。

  ```python
  logit = lambda x : np.log(x/(1-x))
  ```

##### 自动化权重确定

- 自定义损失函数
- 随机初始化权重
- BP算法，反向传播，计算损失函数对于每一个权重的偏导数
- 权重修正（利用梯度下降法）

##### 损失函数的种类

- MSE均方误差函数，通常用于回归任务

- 交叉熵函数，分类任务

- 这两种常见误差函数可以通过tensorflow中keras中的函数来实现

  ```python
  loss_mse = tf.keras.losses.MeanSquaredError()
  
  loss_crossentropy = tf.keras.losses.categorical_crossentropy()
  ```

##### 反向传播算法

![反向传播](https://pic1.zhimg.com/v2-a8a5fb9c472869195cce853716915d30_r.jpg)

![求导过程](https://pic2.zhimg.com/v2-d38c136a7b018fb3ea7344a902d0e3d1_r.jpg)

本质上就是求导的链式法则，从结果出发求导直到求到开头

##### 更新权重

- 利用梯度下降法，损失函数 Loss记为 J(*θ*)，沿着梯度方向行动一个步长 Step，再更新*θ*值

- $$
  \theta-\eta\times\nabla_\theta J(\theta)
  $$

  

