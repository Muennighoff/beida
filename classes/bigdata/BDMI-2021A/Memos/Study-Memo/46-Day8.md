#	BDMI	Class8



## 1	Pandas



### 1.1	数据结构

* 一维数据Series

  * 初始化

    ```python
    s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
    ```

    ```python
    a    0.136437
    b    1.014519
    c   -1.519393
    d   -0.144638
    e    2.165888
    dtype: float64
    ```

    ```python
    pd.Series(np.random.randn(5)) #默认label为数字
    ```

    ```python
    0    0.369155
    1   -0.695860
    2    0.316489
    3   -0.241731
    4   -0.388681
    dtype: float64
    ```

  * 向量运算

    > 与numpy一致

  * 切片

* 二维数据DataFrame

  * 初始化

    ```python
    d = {'one' : pd.Series([1.,2.,3.],index=['a','b','c']),
         'two' : pd.Series([1.,2.,3.,4.],index=['a','b','c','d'])}
    df = pd.DataFrame(d)
    ```

    ```python
    df_data = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))
    ```

  * 存储为csv文件

    ```python
    df.to_csv('BDMI.csv')
    df_2 = pd.read_csv('BDMI.csv')
    ```

  * 索引

    ```python
    df['one']
    df.loc['a']
    df.iloc[0]
    ```

  * 初始化时间片

    ```python
    dates = pd.date_range('20201013',periods=6)
    ```

* 三维数据Panel



### 1.2	连接数据库

```python
from sqlalchemy import create_engine

engine = create_engine('sqlite:///test.db')
#写入数据库
df_classmates.to_sql('tb_test',con=engine)
#读取数据库
with engine.connect() as conn, conn.begin():
    data = pd.read_sql_table('tb_test', conn)
```



## 2	Tensorflow



### 2.1	Logit函数

* 与sigmoid函数互为反函数
  $$
  \sigma^{-1}(x)=log(\frac{x}{1-x})
  $$

* 

### 2.2	多层神经网络

* 网络层数越多
  * 一方面，神经网络的表达能力增强了
  * 另一方面，神经网络的权重数量增加了
* 自动化的权重确定(**神经网络的训练**)
  * 确定损失函数：定义损失函数
  * 权重初始化
  * 反向传播算法：计算损失函数对权重的梯度
  * 权重修正：随机梯度下降法

### 2.3	损失函数

1. 度量函数$D(y,y')$

   * 均方误差(MSE)--常用于线性回归模型，**回归问题**

     ```python
     def MSE(X, real):
         return np.sum((X-real)**2)/len(X)
     ```

     ```python
     print(MSE(np.array([72,94,79,83,65,81,73,67,85,82]),80))
     ```

     ```python
     74.3
     ```

     * `tf.keras.losses.MSE`

   * 交叉熵(CE)--常用于逻辑回归模型，**分类任务**

     **负对数似然损失函数**
     $$
     H_{y'}(y)=-\sum_{i}y_i'log(y_i)
     $$

     ```python
     def CE(y,label_y):
         return -np.sum(label_y*np.log(y))
     ```

     ```python
     print(CE(np.array([0.9,0.1]),np.array([1,0])))
     print(CE(np.array([0.2,0.8]),np.array([0,1])))
     ```

     ```python
     0.10536051565782628
     0.2231435513142097
     ```

     * `tf.keras.losses.categorical_crossentropy`

2. 最优化算法

   * **梯度下降法**

     * 原理：如果实值函数$F(x)$在点a处可微且有定义，那么函数$F(x)$在a点沿着**梯度相反**的方向的下降最快。

   * **反向传播算法**-损失函数对权重的梯度计算

     * **在神经网络上执行梯度下降法的主要算法**

     * 根据损失函数的性质以及链式求导法则

     * 反向逐层计算损失函数对权重的梯度

       <img src="D:\大三课程\笔记image\image-20211103153318468.png" alt="image-20211103153318468" style="zoom: 67%;" />

     * 单个算子的反向传播原理

       <img src="D:\大三课程\笔记image\image-20211103154254509.png" alt="image-20211103154254509" style="zoom: 50%;" />

       <img src="D:\大三课程\笔记image\image-20211103154327909.png" alt="image-20211103154327909" style="zoom: 50%;" />

   * **随机梯度下降法(SGD)**--权重更新
     * 权重更新与学习率
       $$
       \theta=\theta-\eta\cdot\nabla_\theta J(\theta)
       $$
     
       > * 学习率：$\eta$，也即步长
       > * $\nabla_\theta J(\theta)$是经过反向传播算法计算出来的关于权重$\theta$的梯度
     
     * SGD步骤
     
       * 随机初始化每个神经元权重和偏差
       * 选取一个随机样本
       * 反向传播算法计算出每层权重的梯度
       * 逐层调整每层的权重，产生新的权重值
       * 返回步骤2，继续随机选取下一个样本
     
     * 正则化--解决过拟合问题



### 2.4	神经网络的实际训练

* 整个训练集称为一个批次(batch)，切分为多个大小相同的子集
  * 每个子集称为mini-batch，又称为小批量
  * 每个子集的大小参数称为mini-batch-size
* 小批量学习是批量学习和随机梯度下降法的一个折中
  * 每个mini-batch的数据被一次送入网络中进行训练
  * 训练完一个mini-batch，称为一次迭代(iteration)
  * 训练集中所有训练样本都送入网络，完成一次训练的过程，称为一个时代(epoch)
* 当mini-batch大小为1时，就是普通的SGD



### 2.5	神经网络运用的一般流程

* 准备数据集
* 搭建网络模型
  * 多层神经网络结构
  * 相应的每层的权重数值
* 训练模型
* 测试模型
