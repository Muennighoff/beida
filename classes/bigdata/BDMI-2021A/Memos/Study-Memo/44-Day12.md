# 第十二周课程小结

## 循环网络
>循环网络的特点：每一个时间步处理时，采用相同的权重

循环网络的结构：
* 训练目标y
* 损失函数L
* 网络输出层o
* 网络隐藏层h
* 网络输入层x
反馈连接：隐藏层h的当前状态会通过权重w反馈到下一个时间步的状态。

### 循环网络的训练：BPTT
>时间反向传播算法，涉及一次前向传播、一次反向传播，运行时间为O（T）
循环网络前向传播的各个状态需要被保存，因此内存代价也是O（T），可见循环网络训练代价很大。

### 长短时记忆网络 LSTM
LSTM增加了记忆单元和输入门、遗忘门、输出门。
对RNN的改进：
* 解决了RNN网络收敛慢的特点
* 提升了RNN处理远距离依赖问题的能力。

### 门控循环单元GRU
> LSTM的简化版，也是RNN的一种变种。
> GRU参数较少，运算更简单也更快，需要的训练数据少
GRU合并了记忆单元和隐藏状态，合并了输入门和遗忘门为更新门。


### keras RNN类
keras 有内置的RNN层类，包括：
* keras.layers.RNN
* keras.layers.SimpleRNN
* keras.layers.LSTM
* keras.layers.GRU

### keras RNN-Cell类
keras内置的三个RNN cell：
* keras.layers.SimpleRNNCell
* keras.layers.LSTMCell
* keras.layers.GRUCell
RNN类处理整批输入序列，而RNN Cell 类仅处理单个时间步