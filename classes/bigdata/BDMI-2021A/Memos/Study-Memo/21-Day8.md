# DAY8课程小结Memo
## 回顾上节课，介绍本节课内容
### 上节课教学内容：
>>Python Numpy
>>Deep learning:人工神经元

### 本节课教学内容：
>>Python Pandas
>>TensorFlow2 Playground
>>Deep Learning II
>>二元分类（逻辑思提回归）
>>TensorFlow2基础

## Q&A
>>第11周，结队分组
>>第8-15周，AI邀请赛（分类）

## 课后作业
>>读：
>>>>《机器学习实战》（Hands-on Machine Learning with Scikit-Learn,Keras,TensorFlow2)第1-4章，第10章
>>>>Deep Learning，nature，2015
>>>>Machine Learning，scinence 2015
>>>>Automatic differentiation in machine learning a survey,2018
>>写：
>>>>学习小结

## Pandas学习
### pandas基础：
>>一维数据Series s=pd.Series(data,index=index) 如果data是个ndarray，那么index的长度必须跟data一致
>>

## TensorFlow2基础：张量、变量、自动微分、即刻执行
### 分对数logit
>>分对数（logit）模型
>>把区间（0，1）内的数值，变换到区间（-∞，+∞）
>>与sigmoid函数互为反函数


## TensorFlow2基础：计算量、模型、训练流程

## 多层神经网络
### 回归任务的损失度量函数-均方差
>>方差（variance)
>>标准差(standard deviation)
>>均方误差（mean squared error,MSE)
>>均方误差的开放叫均方根误差（Root Mean Squared Error,RMSE)
>>注：均方误差不同于标准差（均方差）
>>练习：用Python表述一下MSE，然后计算一下结果
>>>>一组预测值X=[72,94,79,83,65,81,73,67,85,82]
>>>>真实值（标签值）=80

### 分类问题的损失的度量函数-交叉熵
### 损失：输出值与标签的差异
## 反向传播算法
### 多层网络（深度网络）
### 损失函数的计算过程
### 回顾一下：导数与梯度
### 梯度计算-示例
### 最优化算法：梯度下降法
### 反向传播算法-损失函数对权重的梯度计算
## 反向传播的实际计算
### 单个人工神经元
### 单个算子（Operator）的反向传播的原理
### 反向传播的计算
## 权重更新-随机梯度下降法
### 梯度算法-权重更新与学习率
### 权重值更新-随机梯度下降法
## 模型训练过拟合解决方法
### 正则化
### 丢弃正则化
### 神经网络的实际训练-小批量训练（mini-batch）
### 神经网络的应用-推断（inference）
### 神经网络运用的一般流程
>>准备数据集
>>搭建网络模型
>>训练模型
>>测试模型

## 实际示例
>>用逻辑斯提回归单元进行二元分类案例
>>逻辑斯提回归实验（Logistic Regression)二元分类演示实验
>>>>步骤一：获取训练数据
>>>>步骤二：搭建模型
>>>>步骤三：定义损失函数
>>>>步骤四：运行训练数据，从目标值计算损失
>>>>步骤五：优化器来调整权重变量
>>>>步骤六：结果评估-预测

## 作业
>>作业一：有序数组的平方
>>作业二：使用Tkinter为第一题制作一个GUI



## 课程小结，展望下节课
