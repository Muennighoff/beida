# Pandas (python)

### Examples:

In[1]:

```python
import numpy as np
import pandas as pd
```

In[2]:

```python
s = pd.Series(np.random.randn(5),index=['a','b','c','d','e'])
```

In[3]:

```python
s
```

Out[3]:

```python
a    1.962417
b   -0.860986
c    0.304188
d    0.950094
e    1.409179
dtype: float64
```

In[4]:

```python
pd.Series(np.random.randn(5))
```

Out[4]:

```python
0    0.972564
1   -1.134109
2   -0.107277
3    0.459911
4    1.130684
dtype: float64
```

In[5]:

```python
s+s
```

Out[5]:

```python
a    3.924834
b   -1.721972
c    0.608377
d    1.900188
e    2.818359
dtype: float64
```

In[6]:

```python
s*2
```

Out[6]:

```python
a    0.624418
b   -2.249947
c   -2.143014
d   -1.197906
e    0.512872
dtype: float64
```

In[7]:

```python
s[1:]+s[:-1]
```

Out[7]:

```python
a         NaN
b   -1.721972
c    0.608377
d    1.900188
e         NaN
dtype: float64
```

In[8]:

```python
d = {'one' : pd.Series([1.,2.,3.],index=['a','b','c']),
     'two' : pd.Series([1.,2.,3.,4.],index=['a','b','c','d'])}
```

In[9]:

```python
d
```

Out[9]:

```python
{'one': a    1.0
 b    2.0
 c    3.0
 dtype: float64,
 'two': a    1.0
 b    2.0
 c    3.0
 d    4.0
 dtype: float64}
```

In[10]:

```python
df = pd.DataFrame(d)
```

In[11]:

```python
df
```

Out[11]:

```python
    one	two
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   #in table format at python
a	1.0	1.0
b	2.0	2.0
c	3.0	3.0
d	NaN	4.0
```

In[12]:

```python
df.index
```

Out[12]:

```python
Index(['a', 'b', 'c', 'd'], dtype='object')
```

In[13]:

```python
df.columns
```

Out[13]:

```python
Index(['one', 'two'], dtype='object')
```

In[14]:

```python
df.to_csv('BDMI.csv')
```

In[15]:

```python
df_2 = pd.read_csv('BDMI.csv')
```

In[16]:

```python
df_2
```

Out[16]:

```python
	Unnamed: 0	one	two
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
0	      a	    1.0	1.0   #in table format at python
1	      b	    2.0	2.0
2	      c	    3.0	3.0
3	      d	    NaN	4.0
```

In[17]:

```python
df['one']
```

Out[17]:

```python
a    1.0
b    2.0
c    3.0
d    NaN
Name: one, dtype: float64
```

In[18]:

```python
df.loc['a']
```

Out[18]:

```python
one    1.0
two    1.0
Name: a, dtype: float64
```

In[19]:

```python
df.iloc[0]
```

Out[19]:

```python
one    1.0
two    1.0
Name: a, dtype: float64
```

In[20]:

```python
dates = pd.date_range('20201013',periods=6)
```

In[21]:

```python
dates
```

Out[21]:

```python
DatetimeIndex(['2020-10-13', '2020-10-14', '2020-10-15', '2020-10-16',
               '2020-10-17', '2020-10-18'],
              dtype='datetime64[ns]', freq='D')
```

In[22]:

```python
df_data = pd.DataFrame(np.random.randn(6,4),index=dates,columns=list('ABCD'))
```

In[23]:

```python
df_data
```

Out[23]:

```python
                   A	       B	       C	       D
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”
2020-10-13	-1.306577	1.349374	0.645363	1.052033   #in table format at python
2020-10-14	-1.400359	0.225960	-0.216934	0.509910
2020-10-15	-0.803978	-1.225785	-0.014694	0.901230
2020-10-16	-0.723000	1.527116	-2.033375	-0.076240
2020-10-17	1.372222	0.263691	1.057796	0.120970
2020-10-18	-1.269953	-0.239283	-1.117896	0.642246
```

In[24]:

```python
classmates = {
    'wangxu': pd.Series(['201921****','male'],index = ['ID','gender']),
    'zhengyu':pd.Series(['201831****','male'],index = ['ID','gender'])
}
```

In[25]:

```python
classmates
```

Out[25]:

```python
{'wangxu': ID        201921****
 gender          male
 dtype: object, 'zhengyu': ID        201831****
 gender          male
 dtype: object}
```

In[26]:

```python
df_classmates = pd.DataFrame(classmates)
```

In[27]:

```python
df_classmates = df_classmates.T
```

In[28]:

```python
df_classmates
```

Out[28]:

```python
	            ID	gender
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”    #in table format at python
wangxu	201921****	male
zhengyu	201831****	male
```

In[29]:

```python
df_classmates.loc['wangxu']
```

Out[29]:

```python
ID        201921****
gender          male
Name: wangxu, dtype: object
```

In[30]:

```python
from sqlalchemy import create_engine

#å‚æ•°å­—æ®µ sqlite:///<database path>
engine = create_engine('sqlite:///test.db') 
```

å†™å…¥æ•°æ®åº“

In[31]:

```python
df_classmates.to_sql('tb_test',con=engine)
```

è¯»å–æ•°æ®åº“

In[32]:

```python
with engine.connect() as conn, conn.begin():
    data = pd.read_sql_table('tb_test', conn)
```

In[33]:

```python
data
```

Out[33]:

```python
     index	       ID	gender
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”     #in table format at python
0	wangxu	201921****	male
1	zhengyu	201831****	male
```



# Functions

### Logit 

In:

```python
import math
import numpy as np
logit = lambda x: np.math.log(x/(1-x))
print(logit(0.5))
```

Out:

```
0.0
```



### Mean Square Error

In:

```python
import numpy as np
def mse(A,B):
    return np.square(np.subtract(A,B)).mean()
B=[80,80,80,80,80,80,80,80,80,80]
A=[72,94,79,83,65,81,73,67,85,82]
print(mse(A,B))
```

Out:

```python
74.3
```



### Cross Entropy

In:

```python
import numpy as np
def CE(A,Y):
    return -np.sum(Y*np.log(A))

Y1=[1,0]
A1=[0.9,0.1]
Y2=[0,1]
A2=[0.2,0.8]
print(CE(np.array(A1),np.array(Y1)))
print(CE(np.array(A2),np.array(Y2)))
```

Out:

```python
0.10536051565782628
0.2231435513142097
```





# Logistic Regression Scratch

## Examples:

In[1]:

```python
import pandas as pd
import numpy as np
```

In[2]:

```python
data = pd.read_excel('data.xlsx')
```

In[3]:

```python
data.head()
```

Out[3]:

```python
    Q1_æ€§åˆ«	Q2_èº«é«˜ï¼ˆå˜ç±³ï¼‰  Q3_ä½“é‡ ï¼ˆå…¬æ–¤ï¼‰  Q4_å¤´å‘é•¿åº¦ï¼ˆå˜ç±³ï¼‰
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”    #in table format at python
0	    ç”·	    190	                70	         7
1	    å¥³	    160	                45	        20
2	    ç”·    	179	                61	         5
3	    å¥³	    173	                60	        50
4	    ç”·	    175	                70	        15
```

In[4]:

```python
data = data.rename(columns={'Q1_æ€§åˆ«': 'label', 
                            'Q2_èº«é«˜ï¼ˆå˜ç±³ï¼‰': 'height', 
                            'Q3_ä½“é‡ ï¼ˆå…¬æ–¤ï¼‰': 'weight', 
                            'Q4_å¤´å‘é•¿åº¦ï¼ˆå˜ç±³ï¼‰': 'hair'})
```

In[5]:

```python
data['label'] = data['label'].apply(lambda x : {'ç”·': 0, 'å¥³': 1}[x])
```

In[6]:

```python
data.head()
```

Out[6]:

```python
	label	height	weight	hair
â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”â€”   #in table format at python
0	   0	  190	   70	  7
1	   1	  160	   45	 20
2	   0	  179      61	  5
3	   1	  173	   60	 50
4	   0	  175	   70	 15
```

In[7]:

```python
features = data[['height', 'weight', 'hair']].to_numpy()
```

In[8]:

```python
mean = np.mean(features, axis=0)
std = np.std(features, axis=0)
```

In[9]:

```python
features = (features - mean)/std
```

In[10]:

```python
label = data['label'].to_numpy()
```

In[11]:

```python
features
```

Out[11]:

```python
array([[ 9.86969285e-01, -2.25614423e-03, -4.63916995e-01],
       [-7.03854590e-01, -8.10707828e-01, -1.44545566e-01],
       [ 3.67000531e-01, -2.93298750e-01, -5.13051061e-01],
       [ 2.88357560e-02, -3.25636818e-01,  5.92465423e-01],
       [ 1.41557348e-01, -2.25614423e-03, -2.67380731e-01],
       [-5.91132998e-01, -5.52003289e-01,  1.01124764e-01],
       [-1.40246631e-01,  3.21124529e-01, -5.13051061e-01],
       [-2.52968223e-01, -4.87327154e-01, -5.13051061e-01],
       [ 1.41557348e-01, -1.63946481e-01, -5.37618094e-01],
       [ 1.41557348e-01, -2.25614423e-03, -5.37618094e-01],
       [-8.16576182e-01, -7.46031693e-01,  5.92465423e-01],
       [ 4.23361327e-01, -6.69322789e-02, -5.62185127e-01],
       [ 4.23361327e-01,  1.59434192e-01, -5.13051061e-01],
       [-7.03854590e-01, -6.81355558e-01, -1.44545566e-01],
       [-3.09329019e-01, -3.25636818e-01, -3.41081830e-01],
       [-7.03854590e-01, -6.81355558e-01,  3.46795093e-01],
       [ 1.41557348e-01, -1.31608414e-01, -2.67380731e-01],
       [ 8.74247694e-01, -2.25614423e-03, -2.67380731e-01],
       [-8.38858357e-02, -2.28622616e-01, -3.90215896e-01],
       [ 1.41557348e-01,  1.59434192e-01, -3.90215896e-01],
       [-2.75250398e-02, -1.63946481e-01, -5.62185127e-01],
       [-7.03854590e-01, -5.84341356e-01,  5.92465423e-01],
       [ 4.79722123e-01,  1.91772260e-01, -3.41081830e-01],
       [ 1.41557348e-01, -3.25636818e-01,  1.82081707e+00],
       [-2.75250398e-02, -3.57974885e-01, -5.13051061e-01],
       [ 2.88357560e-02, -2.93298750e-01, -5.62185127e-01],
       [-1.40246631e-01, -3.25636818e-01, -4.63916995e-01],
       [ 4.23361327e-01, -3.90312952e-01, -5.13051061e-01],
       [-1.96607427e-01, -4.87327154e-01,  1.01124764e-01],
       [ 4.23361327e-01, -1.96284548e-01,  5.92465423e-01],
       [-7.03854590e-01, -6.49017491e-01,  3.46795093e-01],
       [-2.75250398e-02, -2.60960683e-01, -5.62185127e-01],
       [ 3.10639735e-01, -2.25614423e-03, -4.88484028e-01],
       [ 3.10639735e-01, -2.25614423e-03, -3.90215896e-01],
       [ 8.51965518e-02,  1.61464722e+00, -3.90215896e-01],
       [ 4.31225624e+00,  4.16935454e+00,  3.73704564e+00],
       [ 7.05165306e-01, -3.25636818e-01, -2.67380731e-01],
       [-4.08550234e+00,  4.16935454e+00,  4.22838630e+00],
       [-5.34772202e-01, -4.54989087e-01,  1.01124764e-01],
       [-8.38858357e-02,  1.59434192e-01, -5.62185127e-01],
       [ 2.88357560e-02, -1.63946481e-01, -5.13051061e-01],
       [-1.96607427e-01, -5.19665222e-01, -9.54115001e-02],
       [-2.75250398e-02,  6.24199904e-02, -5.62185127e-01]])
```

In[12]:

```python
label
```

Out[12]:

```python
array([0, 1, 0, 1, 0, 1, 0, 0, 0, 0, 1, 0, 0, 1, 0, 1, 0, 0, 0, 0, 0, 1,
       0, 0, 0, 0, 0, 0, 1, 1, 1, 0, 0, 0, 0, 0, 0, 1, 1, 0, 0, 1, 0],
      dtype=int64)
```



### Picking a Link Function

Generalized linear models usually tranform a linear model of the predictors by using a [link function](https://en.wikipedia.org/wiki/Generalized_linear_model#Link_function). In logistic regression, the link function is the [sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). We can implement this really easily.

In[13]:

```python
def sigmoid(scores):
    return 1 / (1 + np.exp(-scores))
```



### Maximizing the Likelihood

To maximize the likelihood, I need a way to compute the likelihood and the gradient of the likelihood. Fortunately, the likelihood (for binary classification) can be reduced to a fairly intuitive form by switching to the log-likelihood. We're able to do this without affecting the weights parameter estimation because log transformation are [monotonic](https://en.wikipedia.org/wiki/Monotonic_function).

For anyone interested in the derivations of the functions I'm using, check out Section 4.4.1 of Hastie, Tibsharani, and Friedman's [Elements of Statistical Learning](http://statweb.stanford.edu/~tibs/ElemStatLearn/). For those less mathematically inclined, Carlos Guestrin (Univesity of Washington) details one possible derivation of the log-likelihood in a series of short lectures on [Coursera](https://www.coursera.org/learn/ml-classification/lecture/1ZeTC/very-optional-expressing-the-log-likelihood) using indicator functions.

#### Calculating the Log-Likelihood

The log-likelihood can be viewed as as sum over all the training data. Mathematically,
$$
ğ‘™ğ‘™={\sum_{i=1}^\N}ğ‘¦_ğ‘–ğ›½^ğ‘‡ğ‘¥_ğ‘–âˆ’ğ‘™ğ‘œğ‘”(1+ğ‘’^{ğ›½^ğ‘‡ğ‘¥_ğ‘–})
$$
where ğ‘¦ is the target class, ğ‘¥_ğ‘– represents an individual data point, and ğ›½ is the weights vector.

I can easily turn that into a function and take advantage of matrix algebra.



In[14]:

```python
def log_likelihood(features, target, weights):
    scores = np.dot(features, weights)
    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )
    return ll
```



#### Calculating the Gradient

Now I need an equation for the gradient of the log-likelihood. By taking the derivative of the equation above and reformulating in matrix form, the gradient becomes:
$$
{\nabla}ğ‘™ğ‘™=ğ‘‹^ğ‘‡(ğ‘Œâˆ’ğ‘ƒğ‘Ÿğ‘’ğ‘‘ğ‘–ğ‘ğ‘¡ğ‘–ğ‘œğ‘›ğ‘ )
$$
Again, this is really easy to implement. It's so simple I don't even need to wrap it into a function. The gradient here looks very similar to the output layer gradient in a neural network (see my [post](https://beckernick.github.io/neural-network-scratch/) on neural networks if you're curious).

This shouldn't be too surprising, since a neural network is basically just a series of non-linear link functions applied after linear manipulations of the input data.

### Building the Logistic Regression Function

Finally, I'm ready to build the model function. I'll add in the option to calculate the model with an intercept, since it's a good option to have.



In[15]:

```python
def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):
    if add_intercept:
        intercept = np.ones((features.shape[0], 1))
        features = np.hstack((intercept, features))
        
    weights = np.zeros(features.shape[1])
    
    for step in range(num_steps):
        scores = np.dot(features, weights)
        predictions = sigmoid(scores)

        # Update weights with log likelihood gradient
        output_error_signal = target - predictions
        
        gradient = np.dot(features.T, output_error_signal)
        weights += learning_rate * gradient

        # Print log-likelihood every so often
        if step % 10000 == 0:
            print(log_likelihood(features, target, weights))
        
    return weights
```

Time to do the regression.

In[16]:

```python
weights = logistic_regression(features, label,
                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)
```

Out[16]:

```python
-29.794300659677482
-12.17665666438134
-10.368615637493031
-9.699007253564739
-9.355548233950788
```

In[17]:

```python
print(weights)
```

Out[17]:

```python
[-1.62788588 -3.1418227  -2.31358577  2.1596935 ]
```

In[18]:

```python
def predict(features, weights):
    global mean
    global std
    features = (features - mean)/std
    intercept = np.ones((features.shape[0], 1))
    features = np.hstack((intercept, features))
    scores = np.dot(features, weights)
    predictions = sigmoid(scores)
    
    return predictions
```

In[19]:

```python
student1 = np.array([[188, 85, 2]])
print(predict(student1, weights))
```

Out[19]:

```python
[0.00115921]
```

In[20]:

```python
student2 = np.array([[165, 50, 25]])
print(predict(student2, weights))
```

Out[20]:

```python
[0.76002054]
```

