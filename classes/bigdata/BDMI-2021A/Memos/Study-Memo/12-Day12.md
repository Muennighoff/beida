# 循环网络 RNN Recurrent Neural Network

## 循环网络的结构

属于反馈网络

特点：有时间维度权重共享，有反馈连接



### 基本循环网络 Vanilla RNN

<img src="https://i.imgur.com/zFeOlbF.png" alt="image-20211201215813977" style="zoom:50%;" />

- U：输入到隐藏
- V：隐藏到输出
- W：隐藏到隐藏

**都是全连接**



##### 通过时间反向传播算法 Back Propagation Through Times

涉及1次向前传播（不能并行化）和1次向后传播，运行时间O(T)，所以 RNN的训练代价很大

存在梯度消失/梯度爆炸的问题，解决方法：梯度阶段 Gradient Clipping



#### RNN的变种

- 长短时记忆网络 LSTM（解决收敛慢的问题）
- 门控循环单元网络 GRU（解决计算成本高的问题）





#### 长短记忆网络 LSTM

##### 门结构 Gate

输入 Input 和 控制 Control 是形状一致的张量

门结构可以反向传播训练

<img src="https://i.imgur.com/SHCY54h.png" alt="image-20211201223432693" style="zoom:50%;" />

#####  LSTM

增加了1个记忆单元和3个门（输入门、遗忘门和输出门）

<img src="https://i.imgur.com/16PnXN2.png" alt="image-20211201223857432" style="zoom:50%;" />

#### 门控循环单元网络 GRU

- 是一种常用的LSTM变体
- 比标准的LSTM模型要简单
- 合并了记忆cell状态和隐藏状态
- 合并了输入门和遗忘门， 成为单一的更新门，加另外一个重置门

<img src="![img](https://i.imgur.com/dPa6NbA.png)" alt="image-20211201224228207" style="zoom:50%;" />





#### 序列对序列模型 Sequence-to-Sequence Models

- 通常用于机器翻译
- 注意力机制
- Transformer 模型





# Keras RNN实现

`return_sequences = True`：输出形状为(batch_size, timesteps, units)

LSTM层

```python
model = keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))
model.add(layers.LSTM(128))
model.add(layers.Dense(10))
model.summary()
```

GRU层、Simple RNN层

```python
model = keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))
model.add(layers.GRU(256,return_sequence=True))
model.add(layers.SimpleRNN(128))
model.add(layers.Dense(10))
model.summary()
```

双向RNN

```python
model = keras.Sequential()
model.add(layers.Bidirectional(layers.LSTM(64,return_sequences=True),input_shape=5,10))
model.add(layers.Bidirectional(layers.LSTM(32))
model.add(layers.Dense(10))
model.summary()
```

