# 第十二堂学习小结

> **陈彦扬 2019080117 软件93**

## 循环网络

> RNN Recurrent Neural Network

属于反馈网络

##### 特点：

- 时间维度上权重共享
- 反馈连接

![](https://gitee.com/YYTan/image/raw/master/%E7%AC%AC12%E6%AC%A1%E5%B0%8F%E7%BB%93/%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C%E6%9D%83%E9%87%8D%E5%85%B1%E4%BA%AB.png)

> 在时间维度上，每一个时间步处理时采用相同权重，隐藏层的当前状态会反馈到下一时间步。

![](https://gitee.com/YYTan/image/raw/master/%E7%AC%AC12%E6%AC%A1%E5%B0%8F%E7%BB%93/%E5%9F%BA%E6%9C%AC%E5%BE%AA%E7%8E%AF%E7%BD%91%E7%BB%9C.png)

反向传播

> 通过时间反向传播算法（BPTT）进行反向传播，执行一次前向传播的同时会执行一次由右到左（时间前向）的反向传播。

梯度截断

> RNN训练时梯度有可能会出现过大或过小，为了加速训练采用梯度截断的方式，使得最大不会超过某值，最小不会小于某值。

##### 长短记忆网络 LSTM



## Keras

LSTM

```python
keras.layers.LSTM(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, 
                  	kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', 										bias_initializer='zeros', 
                	unit_forget_bias=True, kernel_regularizer=None, recurrent_regularizer=None, 								bias_regularizer=None, 
                  	activity_regularizer=None, kernel_constraint=None, recurrent_constraint=None, 								bias_constraint=None, 
                  	dropout=0.0, recurrent_dropout=0.0, implementation=1, return_sequences=False, 								return_state=False, 
                  	go_backwards=False, stateful=False, unroll=False)

model = keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))
model.add(layers.LSTM(128))
model.add(layers.Dense(10))
model.summary()
```

GRU

```python
keras.layers.GRU(units, activation='tanh', recurrent_activation='hard_sigmoid', use_bias=True, 
                 kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', 									bias_initializer='zeros', 
               		kernel_regularizer=None, recurrent_regularizer=None, bias_regularizer=None, 							activity_regularizer=None, 
                 kernel_constraint=None, recurrent_constraint=None, bias_constraint=None, dropout=0.0, 
                 recurrent_dropout=0.0, implementation=1, return_sequences=False, return_state=False, 							go_backwards=False, 
                 stateful=False, unroll=False)

model = keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))
model.add(layers.GRU(256,return_sequence=True))
model.add(layers.SimpleRNN(128))
model.add(layers.Dense(10))
model.summary()
```

