## 第八周课程总结
### 一、pandas
1.一维数据Series
s=pd.Series(np.random.randn(5),index=["a","b","c","d","e"])

2.二维数据DataFrame
d={'one':pd.Series([1.,2.,3.],index=['a','b','c']),
   'two':pd.Series([1.,2.,3.,4.],index=['a','b','c','d'])}
   
3.三维数据Panel

### 二、多层神经网络
1.反向传播算法
梯度：函数F(x)在某个点上增长变化率最大的方向，就是倒数的导数的方向
梯度下降法：找到一个函数的局部极小值，必须想函数向当前点对应的梯度的反方向，按照规定步长距离点，进行迭代搜索。
反向传播算法：
根据损失函数的性质以及链式求导法则反向逐层计算孙树函数对权重的梯度（各个偏导数）

2.权重更新
随机梯度下降方法：
步骤1.随机初始化每个神经元输入权重和偏差
步骤2.选取一个随机样本
步骤3.根据网络的输出结果，从最后一层开始，逐层计算每层权重的偏导数
步骤4.逐层调整每层的权重，产生新的权重值
返回到步骤2，继续随机选取下一个样本

3.过拟合解决方法
正则化：L1正则化、L2正则化、丢弃正则化
正则化率：用于指定正则化函数的相对重要性

4.用张量表示神经网络
张量的是指是N维数组
1维Tensor的形式是向量
2维Tensor的形式是矩阵
3维Tensor的形式是由矩阵组成的向量

### 三、逻辑斯蒂回归
自动化训练流程：
1.获取训练数据
2.搭建模型
3.定义损失函数
4.运行训练数据，从目标值计算损失
5.计算损失的梯度，并使用优化器来调整变量
6.结果评估