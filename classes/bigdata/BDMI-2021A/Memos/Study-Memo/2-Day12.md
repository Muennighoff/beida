# 第十二天课堂笔记 2021/12/01

## 循环网络

### vanilla RNN 基本结构

> Recurrent Neural Network：时间维度上参数共享

隐藏层中有一个**反馈连接（全连接）**，连接到下一个时刻。

- U: 输入层到隐藏层
- V: 隐藏层到输出层
- W: 隐藏层到隐藏层

均为全连接。

> 回顾多层全连接网络：FCN、MLP、Dense
> 将全连接网络增加一个时间维度，每一个时间步的FCN与之前的FCN之间有一个全连接结构

### LSTM 长短时记忆网络

门：sigmoid control 函数，1 通过，0 截止

增加输入门、遗忘门、输出门，mem cell(记忆单元)

### GRU

合并了记忆状态和隐藏状态，参数少，计算快

### seq2seq

可以用于翻译

## 单词嵌入

> 用数字表示文本

### 独热编码

效率低下，向量稀疏

### 单词嵌入向量

相似的单词有相似的编码
