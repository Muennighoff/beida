# BDMI Day12

`Student No.52`

## 循环神经网络（RNN）

循环神经网络（Recurrent Neural Network, RNN）是一种适宜于处理序列数据的神经网络，被广泛用于语言模型、文本生成、机器翻译等。

这里，我们使用 RNN 来进行尼采风格文本的自动生成。 [5](https://tf.wiki/zh_hans/basic/models.html#rnn-reference)

这个任务的本质其实预测一段英文文本的接续字母的概率分布。比如，我们有以下句子:

```
I am a studen
```

这个句子（序列）一共有 13 个字符（包含空格）。当我们阅读到这个由 13 个字符组成的序列后，根据我们的经验，我们可以预测出下一个字符很大概率是 “t”。我们希望建立这样一个模型，逐个输入一段长为 `seq_length` 的序列，输出这些序列接续的下一个字符的概率分布。我们从下一个字符的概率分布中采样作为预测值，然后滚雪球式地生成下两个字符，下三个字符等等，即可完成文本的生成任务。

首先，还是实现一个简单的 `DataLoader` 类来读取文本，并以字符为单位进行编码。设字符种类数为 `num_chars` ，则每种字符赋予一个 0 到 `num_chars - 1` 之间的唯一整数编号 i。

```python
class DataLoader():
    def __init__(self):
        path = tf.keras.utils.get_file('nietzsche.txt',
            origin='https://s3.amazonaws.com/text-datasets/nietzsche.txt')
        with open(path, encoding='utf-8') as f:
            self.raw_text = f.read().lower()
        self.chars = sorted(list(set(self.raw_text)))
        self.char_indices = dict((c, i) for i, c in enumerate(self.chars))
        self.indices_char = dict((i, c) for i, c in enumerate(self.chars))
        self.text = [self.char_indices[c] for c in self.raw_text]

    def get_batch(self, seq_length, batch_size):
        seq = []
        next_char = []
        for i in range(batch_size):
            index = np.random.randint(0, len(self.text) - seq_length)
            seq.append(self.text[index:index+seq_length])
            next_char.append(self.text[index+seq_length])
        return np.array(seq), np.array(next_char)       # [batch_size, seq_length], [num_batch]
```

接下来进行模型的实现。在 `__init__` 方法中我们实例化一个常用的 `LSTMCell` 单元，以及一个线性变换用的全连接层，我们首先对序列进行 “One Hot” 操作，即将序列中的每个字符的编码 i 均变换为一个 `num_char` 维向量，其第 i 位为 1，其余均为 0。变换后的序列张量形状为 `[seq_length, num_chars]` 。然后，我们初始化 RNN 单元的状态，存入变量 `state` 中。接下来，将序列从头到尾依次送入 RNN 单元，即在 t 时刻，将上一个时刻 t-1 的 RNN 单元状态 `state` 和序列的第 t 个元素 `inputs[t, :]` 送入 RNN 单元，得到当前时刻的输出 `output` 和 RNN 单元状态。取 RNN 单元最后一次的输出，通过全连接层变换到 `num_chars` 维，即作为模型的输出。

![../../_images/rnn.jpg](https://s2.loli.net/2021/12/22/TisUwWOnBop83Ha.jpg)

定义一些模型超参数：

```
    num_batches = 1000
    seq_length = 40
    batch_size = 50
    learning_rate = 1e-3
```

训练过程与前节基本一致，在此复述：

- 从 `DataLoader` 中随机取一批训练数据；
- 将这批数据送入模型，计算出模型的预测值；
- 将模型预测值与真实值进行比较，计算损失函数（loss）；
- 计算损失函数关于模型变量的导数；
- 使用优化器更新模型参数以最小化损失函数。

关于文本生成的过程有一点需要特别注意。之前，我们一直使用 `tf.argmax()` 函数，将对应概率最大的值作为预测值。然而对于文本生成而言，这样的预测方式过于绝对，会使得生成的文本失去丰富性。于是，我们使用 `np.random.choice()` 函数按照生成的概率分布取样。这样，即使是对应概率较小的字符，也有机会被取样到。同时，我们加入一个 `temperature` 参数控制分布的形状，参数值越大则分布越平缓（最大值和最小值的差值越小），生成文本的丰富度越高；参数值越小则分布越陡峭，生成文本的丰富度越低。

为了实现这一点，我们为前面所建立的 `RNN` 类加入下面的 `predict` 成员函数：

```python
def predict(self, inputs, temperature=1.):
        batch_size, _ = tf.shape(inputs)
        logits = self(inputs, from_logits=True)                         # 调用训练好的RNN模型，预测下一个字符的概率分布
        prob = tf.nn.softmax(logits / temperature).numpy()              # 使用带 temperature 参数的 softmax 函数获得归一化的概率分布值
        return np.array([np.random.choice(self.num_chars, p=prob[i, :]) # 使用 np.random.choice 函数，
                         for i in range(batch_size.numpy())])           # 在预测的概率分布 prob 上进行随机取样
```

## RNN变种

双向RNN单向RNN的问题在于t时刻进行分类的时候只能利用t时刻之前的信息，但是在t时刻进行分类的时候可能也需要利用未来时刻的信息。双向RNN（bi-directionalRNN）模型正是为了解决这个问题，双向RNN在任意时刻t都保持两个隐藏层，一个隐藏层用于从左往右的信息传播记作，另一个隐藏层用于从右往左的信息传播记作。

## LSTM

LSTM全称叫LongShort-TermMemorynetworks，它和传统RNN唯一的不同就在与其中的神经元（感知机）的构造不同。传统的RNN每个神经元和一般神经网络的感知机没啥区别，但在LSTM中，每个神经元是一个“记忆细胞”（元胞状态，CellState），将以前的信息连接到当前的任务中来。每个LSTM细胞里面都包含

- 记忆单元

  > 可以寄存时间序列的输入

  The LSTM does have the ability to **remove or add information** to the cell state, carefully regulated by structures called gates.

- 输入门、遗忘门、输出门

  - 输入门：控制是否输入
  - 遗忘门：控制是否存储
  - 输出门：控制是否输出

## GRU

GRU(GatedRecurrentUnit)是LSTM的变种，把LSTM中的遗忘门和输入门合并成为单一的“更新门(UpdateGate)”，同时也将元胞状态(CellState)和隐状态(HiddenState)合并，在计算当前时刻新信息的方法和LSTM有所不同。
