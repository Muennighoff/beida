# 课程总结8

## 1.pandas

​	import pandas as pd

### 1-1 series

 s= pd.Series(data, index= ) data可以为字典或者ndarray

可以通过加和数乘对于data的值做数乘，只是根据index对齐，未对齐的部分视为缺失值。

### 1-2 dataframe

s = pd.Dataframe(data,index,colomn),index标注行，colomn标注列

data可以是字典series,dataframe、数组等

也可以写成s = pd.Dataframe（{'a':data1,index1；'b':data2,index2}）

可以直接使用index 或者colomn的值来调用某个行或者列

可以通过行列的序列数和bool判断来筛选行列

同时dataframe还和csv和sql能够方便地相互连接。

## 2.多层神经网络

### 2-1 损失函数

度量输出值和标签的差异

对于回归问题通常使用MSE函数，即为求偏差值的方均根

对于分类问题通常使用交叉熵函数（标签纸乘以真实值的对数求和取负）（配合softmax使用）

```
def softmax(L):
    pass
    expL = np.exp(L)
    sumExpL = sum(expL)
    result = []
    for i in expL:
        result.append(i*1.0/sumExpL)
    return result
    
def cross_entropy(Y, P):
    Y = np.float_(Y)
    P = np.float_(P)
    return -np.sum(Y * np.log(P) + (1 - Y) * np.log(1 - P))
```

### 2-3 优化算法

​		梯度下降法：找到一个函数的局部极小值，向当前点对应的梯度的反方向，按照规定步长距离点，进行迭代搜索。 

​		反向传播算法： 损失函数的偏导数对于前一层的权重求偏导数，依次对前一层做这样的操作。

​		正则化：防止过拟合，包含L1正则化、L2正则化、丢弃正则化

​		随机梯度下降方法（SGD）最常用的权重调节方法，以最小化损失函数步骤：

​				1、随机初始化每个神经元输入权重和偏差；

​				2、选取一个随机样本；

​				3、根据网络的输出结果，从最后一层开始后，逐层计算每层权重的偏导数；

​				4、逐层调整每层的权重，产生新的权重值；返回步骤2，继续随机选取下一个样本

### 2-4.二元分类的例子

```python
import pandas as pd
import numpy as np


data = pd.read_excel('data.xlsx')
data = data.rename(columns={'Q1_性别': 'label', 
                            'Q2_身高（厘米）': 'height', 
                            'Q3_体重 （公斤）': 'weight', 
                            'Q4_头发长度（厘米）': 'hair'})
data['label'] = data['label'].apply(lambda x : {'男': 0, '女': 1}[x])
features = data[['height', 'weight', 'hair']].to_numpy()
mean = np.mean(features, axis=0)
std = np.std(features, axis=0)
features = (features - mean)/std
label = data['label'].to_numpy()


def sigmoid(scores):
    return 1 / (1 + np.exp(-scores))


def log_likelihood(features, target, weights):##对数似然
    scores = np.dot(features, weights)
    ll = np.sum( target*scores - np.log(1 + np.exp(scores)) )
    return ll


def logistic_regression(features, target, num_steps, learning_rate, add_intercept = False):##梯度下降递归权重更新
    if add_intercept:
        intercept = np.ones((features.shape[0], 1))
        features = np.hstack((intercept, features))
        
    weights = np.zeros(features.shape[1])
    
    for step in range(num_steps):
        scores = np.dot(features, weights)
        predictions = sigmoid(scores)

        # Update weights with log likelihood gradient
        output_error_signal = target - predictions
        
        gradient = np.dot(features.T, output_error_signal)
        weights += learning_rate * gradient

        # Print log-likelihood every so often
        if step % 10000 == 0:
            print(log_likelihood(features, target, weights))
        
    return weights
weights = logistic_regression(features, label,
                     num_steps = 50000, learning_rate = 5e-5, add_intercept=True)


def predict(features, weights):##训练完成后的预测函数
    global mean
    global std
    features = (features - mean)/std
    intercept = np.ones((features.shape[0], 1))
    features = np.hstack((intercept, features))
    scores = np.dot(features, weights)
    predictions = sigmoid(scores)
    
    return predictions
```