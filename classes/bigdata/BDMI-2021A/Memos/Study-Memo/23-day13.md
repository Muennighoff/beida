# 第13次课总结

## 1. 文本分类实例

使用tensorflow_dataset读取文本数据集，文本数据使用读取结果info自带编码器：

```python
import tensorflow_dataset as tfds
dataset, info = tfds.load('.../...',with_info=True,as_supervised=True)
encoder = info.features['text'].encoder
```

dataset分为’train'和‘test’，提取后打乱并使用AUTOTUNE进行prefetch

情绪标签二比特量化，输出Dense层为一层，模型搭建如下：

```python
model=tf.keras.Sequential([
    layers.Embedding(encoder.vocab_size,64),
    layers.Bidirectional(layers.LSTM(64)),%双向RNN
    layers.Dense(64,activation='relu'),
    layers.Dense(1),
])
```

可以堆叠多个LSTM，需要前一层return_sequences:

```python
layers.Bidirectional(layers.LSTM(64,return_sequences=True)),
layers.Bidirectional(layers.LSTM(32))
```

## 2.文本预测实例

从莎士比亚文本数据集中，创建文本字符到int型数字的转换，使用RNN进行单向循环预测训练，通过Embedding、GRU、Dense三层搭建网络。

预测时只需要给定输入，便可以对照字符-数字转换表循环输出文本并输入到下一个循环预测。

## 3.语音识别硬件实例

### 语音识别对象：

- 波形图
- 频率图
- 时频谱图

### 评价指标：

词错误率，即出现错误的词数与正确文本的词数的差异

### 语音识别的原理：

基于贝叶斯原理，分声学模型和语言模型，其中声学模型没有先验知识，可以随时训练。

### 传统识别模型：

特征提取（MFCC）--> 声学模型（GMM）解码 --> 模型预测识别

神经网络运用：以谷歌CLDNN为例（CNN+LSTM+DNN）

两层Conv+线性化+两层LSTM+两层DNN

### 端到端语音识别模型（end-2-end）

标准的基于深度学习的语音识别：特征提取-神经网络输入-输出-输入声学模型，获得不同字符的输出概率，结合语言模型获得输出字符的最大概然，作为语音文本输出。

### CTC

由于输出帧的切分长度固定，英文输出会有重复问题，例如：

INPUT : HELLO

OUPUT : HHELLLLLOO

可以使用CTC算法解决重复输出的问题。

### 百度的DeepSpeech2

五层网络，其中第四层为LSTM。

RNN采用双向获得反向信息，结果助于更好的预测。



