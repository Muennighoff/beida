## 学习小结-1201-day12
#### 1. 循环网络基本结构

- 网络结构
- 反馈连接
- 循环网络训练
  - 通过时间反向传播算法（BPTT）
  - 梯度截断
- 循环网络的变种
  - LSTM（长短时记忆网络）：解决基本循环网络收敛慢的问题
  - GRU（门控循环单元网络）：简化LSTM的结构，解决LSTM的计算成本高的问题
- 双向RNN

#### 2. LSTM

- 门结构（Gate）

  - 输入和控制是形状一致的张量
  - 控制经过Sigmoid函数后，变成一个范围在0-1之间的一个同形状张量，与输入相乘，得到一个同形输出

- LSTM增加了一个主记忆单元和其他三个辅助的门输入单元

  - 三个辅助门单元

    输入门：控制是否输入

    输出门：控制是否输出

    遗忘门：控制是否存储

  - 辅助记忆门单元

    可以寄存时间序列的输入

    在训练过程中会利用后向传播的方式进行

#### 3. GRU

- 是LSTM的简化版本

  GRU参数少，运算更简单也更快

  GRU合并了记忆cell状态和隐藏状态

  GRU合并了输入门和遗忘门，成为单一的更新门

#### 4. 循环网络应用

- 用于序列建模预测问题

  手写识别、股价预测、天气预测、语音识别、图片注释、机器翻译

#### 5. 循环神经网络

- `Keras RNN`简介

  - 易用性：内置常用RNN层，可以随时调用
  - 易于定制：可以用for循环自定义行为RNN单元层，并将其与通用`keras.layers.RNN`层一起使用

- 共有三种内置 RNN 单元，每种单元对应于匹配的 RNN 层。

  - `keras.layers.SimpleRNNCell` 对应于 `SimpleRNN` 层。

  - `keras.layers.GRUCell` 对应于 `GRU` 层。

  - `keras.layers.LSTMCell` 对应于 `LSTM` 层。

- LSTM层

  ```python
  model = keras.Sequential()
  model.add(layers.Embedding(input_dim=1000, output_dim=64))
  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)
  model.add(layers.GRU(256, return_sequences=True))
  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)
  model.add(layers.SimpleRNN(128))
  model.add(layers.Dense(10))
  model.summary()
  ```

- GRU层

  ```python
  model = keras.Sequential()
  model.add(layers.Embedding(input_dim=1000, output_dim=64))
  # The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)
  model.add(layers.GRU(256, return_sequences=True))
  # The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)
  model.add(layers.SimpleRNN(128))
  model.add(layers.Dense(10))
  model.summary()
  ```

- 双向RNN

  ```python
  model = keras.Sequential()
  model.add(
      layers.Bidirectional(layers.LSTM(64, return_sequences=True), input_shape=(5, 10)))
  model.add(layers.Bidirectional(layers.LSTM(32)))
  model.add(layers.Dense(10))
  model.summary()
  ```

#### 6. 用数字表示文本

- 编码：

  - 独热编码

    创建长度等于词汇量的零向量表示每个单词，在与该单词对应的索引中放置1

  - 用唯一的数字编码每个单词

    编码是任意的

  - 单词嵌入向量

    - 单词嵌入向量是一种使用高效、密集表示的方法，其中相似的单词具有相似的编码

    - 无需手动指定此编码

    每个单词都表示为浮点值的4维向量。还可以将嵌入向量视为“查找表”，学习完这些权重后，可以通过在表中查找对应的密集向量来编码每个单词

- 嵌入向量层

  - 嵌入向量层类似于一个从整数索引（代表特定单词）映射到密集向量（其嵌入向量）的查找表
  - 嵌入向量的维数（或宽度）是一个参数，根据具体问题确定
  - 创建嵌入向量层时，嵌入向量的权重会随机初始化，训练后学习到的单词嵌入向量将粗略编码单词之间的相似性
  - 如果将整数传递给嵌入向量层，结果会将每个整数替换为嵌入向量表中的向量
  - 对于文本或序列问题，嵌入向量层采用整数组成的2D张量，其形状为（samples, sequence_length），其中每个条目都是一个整数序列
  - 返回的张量比输入多一个轴，嵌入向量沿新的最后一个轴对齐

  
