# 循环网络一RNN 

・循环网络（Recurrent neural network，简称RNN) 

・循环网络的特点 :

> ・在时间维度上，每一个时间步处理时，采用相同的权重 （称为权重共享，weights sharing) 

・循环的反馈连接 

#### 基本循环网络（vanilla} 

・网络结构 

> ・训练目标：y 
>
> ・损失函数：L 
>
> ・网络输出层：o 
>
> ・网络隐藏层：h 
>
> ・网络输入层：x 

・反馈连接： 

> ・隐藏层的当前状态，会反馈到下一个时间步的状态 

・时间步上展开，得到对应的计算图 

![Recurrent Neural Networks | Sungtae&#39;s awesome homepage](https://www.cc.gatech.edu/~san37/img/dl/unfold.png)

#### 循环网络一权重共享（时间上） 

・在不同的时间步上，采用相同的U\V\W权重矩阵 

・U: 输入层到隐藏层，全连接的权重矩阵 

・V：隐藏层到输出层，全连接的权重矩阵 

・w：隐藏层到隐藏层，全连接的权重矩阵 
$$
a^{(t)} = b + Wh^{(t-1)} + Ux^{(t)},
$$

$$
h(t) = tanh(a^{(t)}),
$$

$$
o^{(t)} = c + Vh^{(t)},
$$

$$
y^{(t)} = softmax(o^{(t)}),
$$

・将一个输入序列映射到相同长度（变长）的输出序列。 

・数据流动路径：在时间上向前（计算输出和损失）和向后（计算梯度）。 

・U、V、W分别对应于：输入到隐藏、隐藏到输出和隐藏到隐藏的全连接的权重矩阵。 

・b和c是偏置向量。 



循环网络一损失函数 

・循环网络的训练损失： 

> ・将输入序列(x)映射到对应的输出序列(o) 

・损失L： 

> ・衡量每个输出o与相应的训练目标y的距离。 

・计算y'=softmax(o)，并将y'与目标y比较，得到损失L。 

> ・其中使用softmax输出时，o就是未归一化的对数分数（logit) 。

・与x序列配对的y得总损失就是所有时间部得损失之和。

## **神经网络基础**

神经网络可以当做是能够拟合任意函数的黑盒子，只要训练数据足够，给定特定的x，就能得到希望的y，结构图如下：![img](https://pic2.zhimg.com/80/v2-dbd25d81a8537985345a3e46077931ed_720w.jpg)

将神经网络模型训练好之后，在输入层给定一个x，通过网络之后就能够在输出层得到特定的y，那么既然有了这么强大的模型，为什么还需要RNN（循环神经网络）呢？

## **为什么需要RNN（循环神经网络）**

他们都只能单独的取处理一个个的输入，前一个输入和后一个输入是完全没有关系的。但是，某些任务需要能够更好的处理**序列**的信息，即前面的输入和后面的输入是有关系的。

> **比如，当我们在理解一句话意思时，孤立的理解这句话的每个词是不够的，我们需要处理这些词连接起来的整个序列；** **当我们处理视频的时候，我们也不能只单独的去分析每一帧，而要分析这些帧连接起来的整个序列。**

**以nlp的一个最简单词性标注任务来说，将我 吃 苹果 三个单词标注词性为 我/nn 吃/v 苹果/nn。**

那么这个任务的输入就是：

我 吃 苹果 （已经分词好的句子）

这个任务的输出是：

*我/nn 吃/v 苹果/nn(词性标注好的句子)*

对于这个任务来说，我们当然可以直接用普通的神经网络来做，给网络的训练数据格式了就是我-> 我/nn 这样的多个单独的单词->[词性](https://www.zhihu.com/search?q=词性&search_source=Entity&hybrid_search_source=Entity&hybrid_search_extra={"sourceType"%3A"article"%2C"sourceId"%3A30844905})标注好的单词。

**但是很明显，一个句子中，前一个单词其实对于当前单词的词性预测是有很大影响的，比如预测苹果的时候，由于前面的吃是一个动词，那么很显然苹果作为名词的概率就会远大于动词的概率，因为动词后面接名词很常见，而动词后面接动词很少见。**

所以为了解决一些这样类似的问题，能够更好的处理序列的信息，RNN就诞生了。

## **RNN结构**

首先看一个简单的循环神经网络如，它由输入层、一个隐藏层和一个输出层组成：

![img](https://pic4.zhimg.com/80/v2-3884f344d71e92d70ec3c44d2795141f_720w.jpg)

我们现在这样来理解，如果把上面有W的那个带箭头的圈去掉，它就变成了最普通的**全连接神经网络**。x是一个向量，它表示**输入层**的值（这里面没有画出来表示**神经元**节点的圆圈）；s是一个向量，它表示**隐藏层**的值（这里隐藏层面画了一个节点，你也可以想象这一层其实是多个节点，节点数与向量s的维度相同）；

U是输入层到隐藏层的**权重矩阵**，o也是一个向量，它表示**输出层**的值；V是隐藏层到输出层的**权重矩阵**。

那么，现在我们来看看W是什么。**循环神经网络**的**隐藏层**的值s不仅仅取决于当前这次的输入x，还取决于上一次**隐藏层**的值s。**权重矩阵** W就是**隐藏层**上一次的值作为这一次的输入的权重。

我们给出这个抽象图对应的具体图：

![img](https://pic1.zhimg.com/80/v2-206db7ba9d32a80ff56b6cc988a62440_720w.jpg)

**我们从上图就能够很清楚的看到，上一时刻的隐藏层是如何影响当前时刻的隐藏层的。**

如果我们把上面的图展开，**循环神经网络**也可以画成下面这个样子：

![img](https://pic2.zhimg.com/80/v2-b0175ebd3419f9a11a3d0d8b00e28675_720w.jpg)RNN时间线展开图

现在看上去就比较清楚了，这个网络在t时刻接收到输入 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bt%7D) 之后，隐藏层的值是 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%7D) ，输出值是 ![[公式]](https://www.zhihu.com/equation?tex=o_%7Bt%7D) 。关键一点是， ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt%7D) 的值不仅仅取决于 ![[公式]](https://www.zhihu.com/equation?tex=x_%7Bt%7D) ，还取决于 ![[公式]](https://www.zhihu.com/equation?tex=s_%7Bt-1%7D) 。我们可以用下面的公式来表示**循环神经网络**的计算方法：

#### **用公式表示RNN：**

![img](https://pic4.zhimg.com/80/v2-9524a28210c98ed130644eb3c3002087_720w.jpg)                               





#### RNN的总括图：

![img](https://pic3.zhimg.com/80/v2-9e50e23bd3dff0d91b0198d0e6b6429a_720w.jpg)