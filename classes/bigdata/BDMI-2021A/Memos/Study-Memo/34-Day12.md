# 第12周

## 1 RNN

### 1.基本RNN

**结构：** 在时间维度上，每一个时间步处理时采用相同权重；隐藏层的当前状态会反馈到下一时间步。相当于原有的神经网络在时间维度上进行展开，并在相邻时间步的神经元之间建立连接，将上一时间步的信息传递到下一时间步。

[图片](https://qn-st0.yuketang.cn/Fs9H4IcnfQwyQi_GXc3Eq9d0vDBh)

**损失函数：** 计算训练损失时，将输入序列映射到对应的输出序列并计算输出与目标的距离作为损失量。

**反向传播：** 通过时间反向传播算法（BPTT）进行反向传播，执行一次前向传播的同时会执行一次由右到左（时间前向）的反向传播。传播过程中必须保存前向传播的各个状态直至在反向传播中被再次使用，内存与时间复杂度均为O(T)。

**梯度截断：** RNN训练时梯度可能会过大或过小，为了加速训练采用梯度截断的方式，当梯度某分量小于（大于）某数值时即固定为该数值。

### 2.双向RNN

**结构：** 由一个时间上从序列起点开始移动的RNN和另一个时间上从序列末尾开始移动的RNN组成。可以做到理解“上下文”。

### 3.长短时记忆网络LSTM

**门结构：** 输入和控制是形状一致的张量，控制张量经过Sigmoid函数处理后与输入相乘得到输出，可以起到类似于门电路的控制作用。控制张量也可以通过反向传播训练。

**LSTM中的门：** 输入门控制是否输入，输出门控制是否输出，遗忘门控制是否存储。

**加上门的神经元：**

[图片](https://qn-st0.yuketang.cn/FkB6X5zfZ-Dn1hPwxSmd5-ISC0cH)

记忆单元是LSTM的核心，作用是对时间的积分器。为损失函数的导数传播增加了一条路径。

### 4.门控循环单元全程GRU

**结构：** LSTM的变体，合并了记忆cell状态和隐藏状态，合并了输入门和遗忘门，只有更新门（原有的输入门和遗忘门）和重置门（决定历史信息是否参与运算）。

[图片](https://qn-st0.yuketang.cn/FjizjSnl3u3WZ6rTijHXfNRWvXNr)

### 5.RNN应用

- 语音识别：语音信号->文本
- 图片注解
- 单词嵌入向量：用向量表示单词，同时使相似的单词具有相似的编码
- 文本分类、情感分析
- 文本生成：判断序列后下一字母出现的概率，不断循环生成个更长的字符串

### 6.Se2Seq网络

- 神经网络机器翻译NMT：编码器和解码器架构
- 两个多层LSTM：一个将输入序列映射为中间向量，另一个将中间向量解码为目标序列
- Transformer：位置编码，注意力机制，不包括卷积和循环架构
- BERT：双向模型，预训练模型

### 7.else

- 记忆网络： 增加记忆层记录前面所有时刻的信息
- 神经图灵机
- 可微神经计算机

## 2 Keras RNN

### 1.基本API

Keras内置RNN层、LSTM层和GRU层，并提供了相应的`dropout` `go_backwards`等方法

**LSTM**

    model = keras.Sequential()
    model.add(layers.Embedding(input_dim=100, output_dim=64))
    model.add(layers.LSTM(128))
    model.add(layers.Dense(10))

**GRU**

    model = keras.Sequential()
    model.add(layers.Embedding(input_dim=100, output_dim=64))
    model.add(layers.GRU(256, return_sequences=True))
    model.add(layers.Dense(10))

### 2.cell

RNN API提供了cell级接口`SimpleRNNCell` `LSTMCell` `GRUCell` 。与层不同，cell仅处理单个时间步。

**SimpleRNNCell**创建RNN层

    rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4, return_sequence=True,return_State=True))

**LSTMCell**

    rnn = tf.keras.layers.RNN(tf.keras.layers.LSTMCell(4, return_sequence=True,return_State=True))

### 3.跨批次状态

长序列处理时可能需要跨批次保存状态

    l = layers.LSTM(64, stateful=True)#设为保存
    l.reset_states()#清除状态
    s = l.states #状态重用

### 4.双向RNN

    layers.Bidirectional(layers.SimpleRNN(4))

## 3 单词嵌入向量

### 1.One-hot encodeing

向量长度等于词汇量，每个分量对应一个单词，对应该单词则为1，否则为0。表达效率低下。

改进：单词对应数字编号，每个句子对应一个等于单词数的向量，向量分量为单词编号。但是仍没有有效的联系相近单词。

单词嵌入向量：单词对应浮点值的四位向量

### 2.嵌入向量层

嵌入向量层：从整数索引映射到上述对应单词的密集向量

    l = layer.Embedding(1000, 5)

完整代码

[file](imdb.py)