## 学习小结-1208-day13
#### 1. 循环网络应用

- 文本分类

  - 建立输入流水线

    使用`tensorflow_datasets`下载数据集

    ```python
    import tensorflow_datasets as tfds
    import tensorflow as tf
    dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,
                              as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']
    ```

  - 文本编码器

    数据集info包括编码器（`tfds.features.text.SubwordTextEnroder`）

    此编码器将以可逆方式对任何字符串进行编码，并在必要时退回到字节编码

  - 准备训练数据

    ```python
    BUFFER_SIZE = 10000
    BATCH_SIZE = 64
    train_dataset = train_dataset.shuffle(BUFFER_SIZE)
    train_dataset = train_dataset.padded_batch(BATCH_SIZE)
    
    test_dataset = test_dataset.padded_batch(BATCH_SIZE)
    ```

  - 创建模型

    ```python
    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(encoder.vocab_size, 64),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)
    ])
    ```

  - 编译&训练模型

    ```python
    model.compile(loss=tf.keras.losses.BinaryCrossentropy(from_logits=True),
                  optimizer=tf.keras.optimizers.Adam(1e-4),
                  metrics=['accuracy'])
    history = model.fit(train_dataset, epochs=10,
                        validation_data=test_dataset, 
                        validation_steps=30)
    ```

  - 训练精度

    ```python
    test_loss, test_acc = model.evaluate(test_dataset)
    
    print('Test Loss: {}'.format(test_loss))
    print('Test Accuracy: {}'.format(test_acc))
    ```

- 文本生成

  - 读取数据

    ```python
    path_to_file = tf.keras.utils.get_file('shakespeare.txt', 'https://storage.googleapis.com/download.tensorflow.org/data/shakespeare.txt')
    # 读取并为 py2 compat 解码
    text = open(path_to_file, 'rb').read().decode(encoding='utf-8')
    # 文本长度是指文本中的字符个数
    print ('Length of text: {} characters'.format(len(text)))
    # 文本中的非重复字符
    vocab = sorted(set(text))
    print ('{} unique characters'.format(len(vocab)))
    ```

  - 处理文本

    ```python
    # 创建从非重复字符到索引的映射
    char2idx = {u:i for i, u in enumerate(vocab)}
    idx2char = np.array(vocab)
    
    text_as_int = np.array([char2idx[c] for c in text])
    ```

  - 创建模型

    ```python
    # 设定每个输入句子长度的最大值
    seq_length = 100
    examples_per_epoch = len(text)//seq_length
    # 创建训练样本 / 目标
    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
    for i in char_dataset.take(5):
      print(idx2char[i.numpy()])
    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)
    for item in sequences.take(5):
      print(repr(''.join(idx2char[item.numpy()])))
    def split_input_target(chunk):
        input_text = chunk[:-1]
        target_text = chunk[1:]
        return input_text, target_text
    dataset = sequences.map(split_input_target)
    # 批大小
    BATCH_SIZE = 64 
    BUFFER_SIZE = 10000
    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)
    dataset
    # 词集的长度
    vocab_size = len(vocab)
    # 嵌入的维度
    embedding_dim = 256
    # RNN 的单元数量
    rnn_units = 1024
    def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
      model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim,
                                  batch_input_shape=[batch_size, None]),
        tf.keras.layers.GRU(rnn_units,
                            return_sequences=True,
                            stateful=True,
                            recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)
      ])
      return model
    model = build_model(
      vocab_size = len(vocab),
      embedding_dim=embedding_dim,
      rnn_units=rnn_units,
      batch_size=BATCH_SIZE)
    ```

  - 训练模型

    ```python
    def loss(labels, logits):
      return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)
    
    example_batch_loss  = loss(target_example_batch, example_batch_predictions)
    print("Prediction shape: ", example_batch_predictions.shape, " # (batch_size, sequence_length, vocab_size)")
    print("scalar_loss:      ", example_batch_loss.numpy().mean())
    model.compile(optimizer='adam', loss=loss)
    EPOCHS=10
    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])
    ```

  - 生成文本

- 语音识别

  - ASR的挑战问题

    口音、语气、腔调、感情

    说话的方式

    词汇表的大小，同音词

    噪声，录音质量

    鸡尾酒会问题（cocktail party）

  - 语音识别的对象

    语音信号是声波

    语音信号的表示：波形图（waveform）、频率图（frequency）、时频谱图（spectrogram）

  - 评价指标

    词错误率WER（word error rate)

    错误率等于替换、插入、删除的总数除以标准答案长度

  - 语音识别-基于深度学习

    语音识别中引入深度学习技术，识别准确率取得了持续的重大突破，语音识别词错误率下降了30%或更多

  - 端到端的语音识别模型（end-to-end）

    分为特征提取、声学模型、语言模型

  - 循环网络处理语音

    - 用RNN处理语音识别的主要贡献点

      CTC（connectionist temporal classification）

      语音与文字对齐的问题，解决了语料标注，端到端系统做好准备

      基于循环网络的端到端语音识别系统

  - CTC解决的问题

    对齐问题：输入语音X和输出文字Y之间

  - CTC原理

    对于给定的输入语音X，CTC算法给出了所有可能的文字Y的分布，CTC算法推断出可能的输出或者给定的输出文字Y的概率
