# 第12次课总结

## 1.循环网络RNN

通过添加时间维度，使得网络后一步的输出和前一步的隐藏层输出有关，可以视为CNN旋转90°。

### 基本分类

1. 基本训练模型BPTT，按时间反向传播
2. 双向RNN，时间维度双向进行
3. 变种LSTM，长短时记忆网络，加速收敛
4. GRU，简化LSTM，减少运算单元占用

### 双向RNN

时间维度双向进行，最终使得每一步输出和前一步与后一步均有关，相当于两个RNN。

可广泛运用于手写识别、语音识别、自然语言处理。

### LSTM

增加了记忆单元和遗忘门，可以选择性遗忘加速收敛，减少长距离依赖问题。

通过类似于积分器的传播，逐级减少远处输入的影响。

### GRU

简化了LSTM模型，合并cell和隐藏层，合并输入门和遗忘门，因此结构更简单，运算速度也更快。

### 应用

语音识别、自然语言预测、图片注解、文本分类、密集向量产生、文本生成

### 序列输出预测

引入注意力机制(attention mechanism)，给不同的输入分配不同的注意力大小，便于预测对应的输出值，例如跨语种翻译的首单词预测，此后的翻译类似于RNN。

## Keras.RNN

Keras有内置RNN层，便于调用

* LSTM层：

```python
model = keras.Sequential([
    layers.Embedding(input_dim=1000,output_dim=64),
    layers.LSTM(128),
    layers.Dense(10),
])
```

* GRU层

```python
model = keras.Sequential([
    layers.Embedding(input_dim=1000,output_dim=64),
    layers.GRU(256,return_sequences=True),
    layers.SimpleRNN(128),
    layers.Dense(10),
])
```

也可以使用cell类制作RNN层。

* 双向RNN

```python
model = keras.Sequential([
    layers.Bidirectional(layers.LSTM(64,return_sequences=True),input_shape=(5,10)),
    layers.Bidirectional(layers.LSTM(32)),
    layers.Dense(10),
])
```

可以通过构建层子类的方式构建自定义输入输出的RNN。

## 用数字表示文本

1. one-hot，空间利用率低
2. 用唯一的数字表示每个单词，不同单词之间的相关性低，可学习型差
3. 单词嵌入向量，使用密集多维向量代表不同单词，可以解决以上两种的问题

### 单词嵌入向量

#### 嵌入向量层

将数字集合大小为M的输入数字映射到N个维度的向量中：

```python
embedding_layer = layers.Embedding(M,N)
```

可以将多维tensor输入进该层，输出的密集向量会添加到最后一个维度后，作为新一个维度。

### 模型搭建

以1bit量化为例：将输入评论编码(encoder)后输入到网络中：

```python
embedding_dim=16
model=keras.Sequential([
    layers.Embedding(encoder.vocab_size，embedding_dim),
    layers.GlobalAveragePooling1D(),
    layers.Dense(16,activation='relu'),
    layers.Dense(1)
])
```

### 权重保存

使用Embedding Projector，保存向量文件和元数据文件，以制表符分隔

```python
import io

encoder = info.features['text'].encoder

out_v=io.open('vecs.tsv','w',encoding='utf-8')
out_m=io.open('meta.tsv','w',encoding='utf-8')

for num, word in enumerate(encoder.subwords):
    vec=weights[num+1]
    out_m.write(word+'\n')
    out_v.write('\t'.join([str(x) for x in vec])+'\n')
    
out_v.close()
out_m.close()
```



