# 课堂总结12

## RNN

​		在时间维度上，每一时间步处理时，采取相同的权重。典型例子比如Vanilla RNN: U：输入层->隐藏层，V：隐藏层->输入层，W：隐藏层->隐藏层。

## 几个变种

### LSTM

增加了记忆单元和输入门、遗忘门、输出门

```
model = keras.Sequential()#建立网络
model.add(layers.Embedding(input_dim=1000, output_dim=64))
model.add(layers.LSTM(128))
model.add(layers.Dense(10))
```

### LSTM的简化GRU

```
model = keras.Sequential()
model.add(layers.Embedding(input_dim=1000, output_dim=64))
# The output of GRU will be a 3D tensor of shape (batch_size, timesteps, 256)
model.add(layers.GRU(256, return_sequences=True))
# The output of SimpleRNN will be a 2D tensor of shape (batch_size, 128)
model.add(layers.SimpleRNN(128))
model.add(layers.Dense(10))
model.summary()
```

### 双向RNN

```
model = keras.Sequential()
model.add(layers.Bidirectional(layers.LSTM(64,return_sequences=True),input_shape=5,10))
model.add(layers.Bidirectional(layers.LSTM(32))
model.add(layers.Dense(10))
model.summary()
```

### SimpleRNNCell 创建RNN层

单个时间步

```
rnn = tf.keras.layers.RNN(tf.keras.layers.SimpleRNNCell(4, return_sequence=True,return_State=True))
```