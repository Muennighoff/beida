### 第1 RNN

- Recurrent model through time

- Problems: 
  - Vanishing Gradients
  - Time-intensive (i.e. cannot parallelize)

### 第2 LSTM

- Like an RNN, but on steriods
  - Splits up the RNN recurring connection into two parts: "Long-term" & "Short-term" memory

- Improves the vanishing gradient problem
- Still time-intensive and not parallelizable


--> Transformers: Can parallelize


### 第3 Seq2Seq Models

- Can be used for translation or literally any language task
