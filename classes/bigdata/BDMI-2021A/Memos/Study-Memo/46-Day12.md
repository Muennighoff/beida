#	BDMI	Class12



##	Part1.	RNN



###	基本网络结构(Vanilla RNN)

* 在不同的时间步上，采用相同的U/V/W 权重矩阵（时间上权重共享）
  * U：输入层到隐藏层，全连接的权重矩阵
  * V：隐藏层到输出层，全连接的权重矩阵
  * W：**隐藏层到隐藏层**，全连接的权重矩阵

<img src="https://images2015.cnblogs.com/blog/1042406/201703/1042406-20170306142253375-175971779.png" alt="img" style="zoom: 67%;" />

<img src="https://img-blog.csdn.net/20160731210844164" alt="img" style="zoom: 25%;" />

> 上图只是标准的RNN网络结构，实际应用中还有很多结构



###	循环网络-损失函数

* 与x序列配对的y的总损失就是所有时间步的损失之和。例如，L(t)为给定x(1), ..., x(t)后y(t)的负对数似然

  <img src="D:\大三课程\笔记image\image-20211205143215499.png" alt="image-20211205143215499" style="zoom:67%;" />

* 训练：通过时间反向传播BPTT，训练代价很大





### 循环网络训练-存在的问题

* BPTT训练RNN时的梯度消失或梯度爆炸问题

  * 最初和最后的时间步的梯度的幅度过大或过小
  * Relu函数的问题
  * Sigmoid函数的饱和问题

* 梯度截断

  * 训练循环网络时，经常出现梯度太大或太小，为了加速训练，需要把梯度设置为一些固定数值；
  * 比如说，梯度的任何维度的数值应该小于1，如果某个维度的数值大于1，则固定设置为1
  
  

### 循环网络的变种

* LSTM：解决基本循环网络收敛慢的问题
* GRU：解决LSTM计算成本高的问题



### LSTM

* 参考网址：[Understanding LSTM Networks](http://colah.github.io/posts/2015-08-Understanding-LSTMs/)

* 门结构

  <img src="D:\大三课程\笔记image\image-20211205143628559.png" alt="image-20211205143628559" style="zoom:50%;" />

* 网络结构

  ![img](https://img-blog.csdn.net/20160731211238288)
  
  
  
  * 记忆单元
  
    > 可以寄存时间序列的输入
  
    The LSTM does have the ability to **remove or add information** to the cell state, carefully regulated by structures called gates.
  
  * 输入门、遗忘门、输出门
  
    * 输入门：控制是否输入
    * 遗忘门：控制是否存储
    * 输出门：控制是否输出
  
  * Step-by-Step LSTM Walk Through
  
    * The first step in our LSTM is to **decide what information we’re going to throw away from the cell state**
  
      <img src="D:\大三课程\笔记image\image-20211205154026589.png" alt="image-20211205154026589" style="zoom:50%;" />
  
      > ​		Let’s go back to our example of a language model trying to predict the next word based on all the previous ones. In such a problem, the cell state might include the gender of the present subject, so that the correct pronouns can be used. When we see a new subject, we want to forget the gender of the old subject.
      >
      > ​		当有新主语加入时，我们需要遗忘掉之前的主语，这样才能保证后面代词能正确应用
  
    
  
    * The next step is to **decide what new information we’re going to store in the cell state.** This has two parts.
  
      <img src="D:\大三课程\笔记image\image-20211205153919866.png" alt="image-20211205153919866" style="zoom:50%;" />
  
      <img src="D:\大三课程\笔记image\image-20211205154310427.png" alt="image-20211205154310427" style="zoom:50%;" />
  
      > ​		In the example of our language model, we’d want to add the gender of the new subject to the cell state, to replace the old one we’re forgetting.
      >
      > ​		添加新的主语到细胞状态
  
    * Finally, we need to **decide what we’re going to output**. This output will be based on our cell state, but will be a filtered version.  First, we run a sigmoid layer which **decides what parts of the cell state we’re going to output.**
  
      <img src="D:\大三课程\笔记image\image-20211205154556032.png" alt="image-20211205154556032" style="zoom:50%;" />
  
      > ​		For the language model example, since it just saw a subject, it might want to output information relevant to a verb, in case that’s what is coming next. For example, it might output whether the subject is singular or plural, so that we know what form a verb should be conjugated into if that’s what follows next.
      >
      > ​		根据当前细胞状态，输出动词所需要的形式
  
  ​	 
  
  

###	GRU

* It combines the forget and input gates into a single “update gate.” 
* It also merges the cell state and hidden state, and makes some other changes
* The resulting model is simpler than standard LSTM models, and has been growing increasingly popular.

<img src="D:\大三课程\笔记image\image-20211205155053774.png" alt="image-20211205155053774" style="zoom:50%;" />





##	Part2.	循环网络应用

* 用于序列建模预测问题：
  * 手写识别
  * 股价预测、天气预测
  * 语音识别、图片注释、机器翻译
* 语音识别ASR
* 图片注解image captioning
* 单词嵌入向量
* 文本分类
* 文本生成



##	Part3.	序列对序列模型



### 神经网络机器翻译NMT

* 编码器将输入语句转化为一个中间向量
* 解码器将中间向量转化为翻译的结果
* 评价指标BLEU



## Part4.	记忆网络









