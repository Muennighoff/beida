# Day7课程小结Memo

## 课程回顾
## 课程感想
## 课后作业
>>>>读：《机器学习实战》（Hands-on Machine Learning with Scikit-Learn, Keras,TensorFLow2)第1-4章，第10章

## Numpy/Scipy介绍
>>>>Numpy矩阵运算、数组形状变形、Debugging
>>>>http://numpy.org/
>>>>http://www.scipy.org/

### 创建数组和数据类型定义
>>>>创建数组、算术运算、函数运算、矩阵乘积、数组变形（transpose)

### Numpy实践
#### Numpy 使用
>>>> np.matmul 矩阵相乘（Matrix multiply)
>>>> np.zeros 创建零矩阵(Create a matrix filled with zeros(np.ones))
>>>> np.arange 定义范围（开始，停止，步长）（Start,stop,step size(np.linspace))
>>>> np.identity创建一个单位矩阵（Create an identity matrix)
>>>> np.vstack 垂直叠加2阵列（Vertically stack 2 arrays(np.hstack))

#### Numpy调试
>>>> Numoy方法 Description
>>>> array.shape 得到numpy数组的形状（Get shape of numpy array）
>>>> array.dtype 检查数组的数据类型（Check data type of array(for precision for weird behavior))
>>>> type(stuff) 获取变量的类型（Get type of a variable)
>>>> import pdb;pdb.set_trace() 设置断点（Set a breakpoint)
>>>> print(f'My name is {name}'')

## 深度学习1：人工神经元
### 人工神经元（带权重的函数）
>>>>单个人工神经元（Artificial Neuron):
1.一组输入的线性加权叠加
2.经过一个非线性变换进行输出

#### 激活函数（activation function)
#### 练习代码numpy_activation_function.ipynb
>>>>激活函数有sigmoid函数，tanh函数，ReLU函数等

##### tanh函数：
>>>>import numpy as np
>>>>exp = np.exp
>>>>max = np.max
>>>>def tanh(x):
    return(exp(x) - exp(-x))/(exp(x)+exp(-x))
>>>>def relu(x):
    return max(x,0)
>>>>print(tanh(5),relu(5))

##### ReLU单元：
>>>>def ReLU(x):
    return np.maximum(0,x)
>>>>ReLU(-1)
>>>>ReLU(5)
>>>>weight=np.array([-0.21,0.3,0.7])
>>>>ReLU(np.dot([1,0,1],weight))

### 人工神经元2-逻辑斯提回归单元
>>>>所有输入线性加权叠加，经过一个非线性函数（激活函数）输出
>>>>逻辑斯提回归单元（Logistic Regression Unit）是最简单人工神经元结构之一。
>>>>逻辑斯提回归单元的激活函数采用sigmoid函数或罗季斯提函数
>>>>自己实现一个relu单元和逻辑斯提单元：参数的设置可以有多种方式

### 单个人工神经元能力-模拟布尔运算
>>>>布尔运算是现代计算机的基础能力
>>>>>>>>1.与运算（AND）、或运算(OR)、非运算(NOT)
>>>>>>>>2.与非（NAND）、异或（XOR)
>>>>import numpy as np
>>>>def sigmoid(x):
    return 1/(1+np.exp(-x))
>>>>arr = np.array([[0,0],[0,1],[1,0],[1,1]])
>>>>w = np.array([20,20])
>>>>b_and=-30
>>>>b_or=-10
>>>>print("and:")
>>>>print(sigmoid(np.dot(arr,w)+b_and).reshape(2,2))
>>>>print('or:')
>>>>print(sigmoid(np.dot(arr,w)+b_or).reshape(2,2))

### 多个人工神经元能力-解决XOR问题
#### XOR运算
##### 实现1 X1 XOR X2={X1 OR X2}AND{NAND{X1,X2}}
##### 实现2 X1 XOR X2={X1 OR X2}AND{(NOT X1)OR(NOT X2)}
*XOR运算可以由AND运算；NOT运算；OR运算组合完成；

*### 拓展思考-理论如何落实到实践
>>1、布尔运算构建了计算的体系，实际中用布尔门电路实现
>>>>什么是计算？
>>>>布尔门电路，可以由晶体管电路实现
>>2、理论上讲，用人工神经元作为基本的 计算单元，能力不逊于计算机。
>>>>问题1：实际中有用人工神经元电路的方式来实现么？
>>>>问题2：你认为这种方法的最大的挑战是什么呢？

## activation Funcation:numpy实现
## Pandas准备
## pandas练习

## 深度学习2：网络训练
### 名词术语（中英文对照）
*机器学习、监督学习、深度学习；分类、二元分类；回归、逻辑斯提回归（分类）；网络训练、网络推断；批量训练、小批量训练；时代、迭代；多维数组、张量；Softmax、logit分对数；损失函数、度量函数、均方误差、交叉熵；导数、梯度、梯度下降法；算子操作、链式求导法则；自动微分、反向传播；权重更新
*随机梯度下降方法（stocastic gradient descent,SGD);批量训练（batch training）；小批量训练（mini-batch）；迭代（iteration）；时代（epoch）；前馈网络（Feedforward Network);多层全连接网络（FCN)；多层感知机（MLP）；多个密集层网络（Dense）


## 深度学习-多层神经网络（机器学习监督学习的方法）
### 机器学习
Machine Learning，简称ML，根据输入数据构建（训练）预测模型。
>>>>利用学到的模型，根据从数据中提取的分布中，对新数据（以前从未见过的数据，与训练模型时使用的同一分布）进行实用的预测。
>>>>机器学习的主要三种范式为：
>>>>·监督学习-1：决策树、决策森林、支撑向量机、贝叶斯分类器、核方法逻辑回归；
>>>>·监督学习-2：神经网络、全连接网络、卷积网络、循环网络；
>>>>·非监督学习：降维方法（PCA，自动编码器，主题模型）、聚类；
>>>>·强化学习：策略梯度、深度Q网络、Actor-Critic算法；

### 数据集
>>>>·训练集（training set）：数据集的子集，用于训练模型。
>>>>·验证集（validation set）：数据集的一个子集，从训练集分离而来，用于验证学习效果。
>>>>·测试集（test set）：数据集的子集，用于在模型经由验证集的初步验证之后测试模型。

### 机器学习的应用
>>>>·机器学习主要用于解决两类任务：
>>>>分类任务、探测任务

### 监督式机器学习（supervised machine learning）
>>>>根据输入数据及其对应的标签来训练模型。
>>>>模型（model）是机器学习系统从训练数据学到的内容的表示形式。
>>>>在监督式学习中，有标签样本（labeled example）包含特征和标签的样本。在监督式学习中，标签指样本的“答案”或“结果”部分。
>>>>有标签数据集中的每个样本都包含一个或多个特征以及一个标签。
>>>>比如:在垃圾邮件检测数据集中，特征可能包括主题行、发件人以及电子邮件本身，而标签可能是“垃圾邮件”或“非垃圾邮件”。

### 监督学习的两阶段
·训练（Train）和推断（Inference）
·监督学习特点
>>>>训练数据集由样本（sample）组成，每个样本上都有对应的标签（label），用来指导学习过程。
>>>>根据训练数据集（data set）中的样本（sample）进行学习，然后推断新的实例。

### 迁移学习（transfer learning)
>>>>将信息从一个机器学习任务迁移到另一个机器学习任务。
>>>>迁移学习涉及将从较简单任务的解决方案迁移到较复杂的任务或者将从数据较多的任务迁移到数据较少的任务。
>>>>迁移学习是迈向通用人工智能的一小步。
>>>>大多数机器学习系统都只能完成一项任务。在通用人工智能中，单个程序可以完成多项任务。

## 深度学习2：网络训练示例-二元分类示例
## 深度学习：网络训练猜测性别（Logistic回归）
*用python代码描述一下Softmax函数（提示可以用Numpy）
>>>>def softmax(x):
    exp_x=np.exp(x)
    return exp_x/np.sum(exp_x)
>>>>softmax([i for i in range(1,10)])

## TensorFlow初步
### TensorFlow\TensorFlow-basic
### Tensor操作实验
### TensorFlow PlayGround介绍


