# 自动微分 Autodiff

$数值微分\rightarrow符号微分\rightarrow自动微分$

### 数值微分

$$
\frac{\partial f(x)}{\partial x_i}\approx \lim_{h\rightarrow 0} \frac{f(x+he_i)-f(x)}{h}
$$

缺点：

- 舍入误差
- 计算效率低



### 符号微分

$$
\frac{d(f+g)}{dx}=\frac{df}{dx}+\frac{dg}{dx},\frac{d(fg)}{dx}=\frac{df}{dx}g+\frac{dg}{dx}f,\frac{d(h(x))}{dx}=\frac{df(g(x))}{dx}\cdot\frac{d(g)}{dx}
$$

缺点：

- 符号表达式随着函数变得很复杂
- 需要保存大量中间变量，容易出错





### 自动微分

#### 前向模式

- 从$x_1,x_2$向$f(x_1,x_2)$计算

- 缺点：只能计算输出对**某个输入**的微分

![image-20211130225745774](https://i.imgur.com/N0owr8F.png)



#### 反向模式

- 从$f(x_1,x_2)$向$x_1,x_2$反向计算
- 可以计算输出对所有输入的微分值

![image-20211130225702640](https://i.imgur.com/JRDaXCo.png)



# 卷积网络 Convolution Network

人工神经网络的层间连接关系：前馈网络、反馈网络、记忆网络



### 全连接 Fully-Connected/密集连接 Dense

每层的神经元与下一层的神经元全部连接

<img src="https://i.imgur.com/bKmHstH.png" style="zoom:50%;" />

### 卷积网络

- 局部连接
- 卷积核 (Kernel / Filter)：内部的参数由学习算法得到
- 卷积运算：无填充 (no padding) / 有填充 (padding)
- 结构：
  - 卷积层
  - 非线性激活单元
  - 优化层
  - 随机丢弃层
- 优点：计算快，参数少，可以并行化

<img src="https://i.imgur.com/I0sxwqD.gif" alt="img" style="zoom:50%;" />



# Keras

- 模型 Model：Sequential, Graph
- 层 Layers：Dense, Activation, Dropout, Flatten, Reshape, Convolutional Layers, Pooling Layers, Normalization Layers...
- 激活函数 Activation：softmax, elu, softplus, softsign, relu, sigmoid, tanh...
- 优化器 Optimizers：SGD, RMSprop, Adam, Nadam



### Keras 使用流程

1. 定义网络
2. 编译网络
3. 训练网络
4. 评估网络
5. 数据预测
6. 保存/载入



### Keras 顺序模型

适用于：普通堆栈的图层，每层只有**一个**输入张量和**一个**输出张量

不适用：

- 模型有多个输入或多个输出
- 任何层都有多个输入或多个输出
- 需要做图层共享
- 需要非线性拓扑 （例如剩余连接residual 、多分支模型)

##### 创建顺序模型

方法1

```python
model = keras.Sequential(
    [
        layers.Dense(2, activation="relu"),
		layers.Dense(3, activation="relu"),
		layers.Dense(4),
    ])
```

方法2

```python
model = keras.Sequential()
model.add(layers.Dense(2,activation='relu'))
model.add(layers.Dense(3,activation='relu'))
model.add(layers.Dense(4,name='layer3'))

model.pop('layer3')
```



##### 顺序模型的迁移学习

为了只进行最顶层，而冻结其它层

方法1：设置其它层为不可训练

```python
for layer in model.layers[:-1]:
    layer.trainable = False
```

方法2：先堆叠预先训练的模型，然后添加新初始化的层

```python
base_model = keras.applications.Xception(
	weights='imagenet',
	include_top = False,
	pooling ='avg')
base_model.trainable = False
model = keras.Sequential([
    base_model,
    layers.Dense(1000),
])
```

##### Layers类

权重和部分计算的组合

权重：trainable=True/False

```python
x = tf.ones((2,2))
linear_layer = Linear(4,2) #输入2维，输出4维
y = linear_layer(x)
add_weight()
```



### 保存&加载

保存以下信息：

- 模型的架构/配制
- 模型的权重值
- 模型的编译信息
- 优化器及其状态

```python
#保存,会创建"my_model"的文件夹
model.save("my_model")
tf.keras.models.save_model("my_model")
#加载
tf.keras.models.load_model("my_model")
```

#### 保存模型的配制（层、顺序模型、函数式API模型）

##### 层的架构

```python
layer_config = layer.get_config()
new_layer = keras.layers.Dense.from_config(layer_config)
```

##### 顺序模型的架构

```python
config = model.get_config()
new_model = keras.Sequential.from_config(config)
```

##### 函数式API模型的架构

```python
config = model.get_config()
new_model = keras.Model.from_config(config)
```



##### 内存中迁移权重

get_weights()返回numpy数组

set_weights()设置权重

```python
layer_2.set_weights(layer_1.get_weights())
```

##### 磁盘中保存和加载权重

```python
sequentiaLmodel.save_weights("ckpt")
load_status = sequential_model.load_weights("ckpt")
```

