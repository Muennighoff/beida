# Week 9 Summary
** 重要 **
# 课程大纲
```
TensorFlow2 
	基础部分介绍
		张量、变量
		计算图、即刻执行
		计算图
		模型
		训练循环
		自动微分（原理）
	学习Keras
		总体介绍
```

## 神经网络一般流程
```
数据集准备
网络模型搭建
模型训练/模型测试
模型应用-推断
```

## TensorFlow2 简介
```
张量（Tensor）的概念
	N维数组
	1：向量，2：矩阵，3：矩阵组成的向量
	4：3d形成1(向量)，5：。。。

np.random.rand(2,3,4) --可以生成3维张量
[[[ 0.95052148 -1.05799236  0.82612343 -0.23362751]
  [-0.85824765 -0.45275333 -2.52677003  0.56326931]
  [-0.47394829 -1.51459052 -0.17017117 -0.90459823]]

 [[ 0.34412792  0.05538345 -1.92158088 -0.1274031 ]
  [ 0.29309072 -1.38690722  1.08329462 -0.0558741 ]
  [ 0.58888643  1.38041541  0.43055181  0.90731318]]]
  
TensorFlow2术语
形状shape
秩rank
轴axis或维度dimension
大小size
变量variable

print("Type of every element:", rank_4_tensor.dtype) 
print("Number of dimensions:", rank_4_tensor.ndim)
print("Shape of tensor:", rank_4_tensor.shape)
print("Elements along axis 0 of tensor:", rank_4_tensor.shape[0])
print("Elements along the last axis of tensor:", rank_4_tensor.shape[-1])
print("Total number of elements (3*2*4*5): ", tf.size(rank_4_tensor).numpy())
```

## TensorFlow2 张量
```
具有同一类型的N维数组
所有的张量是不可变的，永远无法更新内容，只能够创建新的张量

0维张量的创建
tf.constant(
	value,
	dtype=None
	shape=None
	name='Const'
	verify_shape=False)
	
一般0维是： tf.constant(4)

1维张量的创建
tf.constant([2.0, 3.0, 4.0], shape=(3,), dtype=float32)

2维张量
tf.constant([[1, 2],
			[3, 4]
			[5, 6]], dtype=tf.float16)
			
3维张量
tf.constant([[[],[]],[[],[]],[[],[]]])

4维：
多个3维张量的组合，有4个轴

```

## Tensor 运算
```
将张量转换为numpy数组： 
np.array(张量名) 将张量赋值给数组
张量名numpy() 将张量的numpy形式表达出来

tf.add 对应元素相加(需要大小一样)
tf.multiply 对应元素相乘(需要大小一样)
浮点加减整数是ok的，但是整数加减浮点会报错
tf.matmul 矩阵乘法(需要前一个的行数 == 后一个的列数)

tf.reduce_max 寻找元素最大值
tf.argmax 求最大值的索引
tf.nn.softmax 对元素进行归一化处理

DTypes：
Tensor.dtype 检查类型
	float64
	float16
	uint8
	int32

注：cast 和 reshape 都不会改变原来的张量，需要创建到新的张量中

tf.cast(x, dtype, name=None)
x:待转换的张量
dtype:目标数据类型
name:可选，定义操作的名称

tf.reshape(tensor, shape, name=None)
tensor 张量
shape 目标形状 如 (3,1)
name 同上

原 3个 2x5 形状
tf.reshape(tensor, [3*2, 5])
变成 6x5
tf.reshape(x, [3, -1], '/n')
变成 3x10
tf.reshape(x, [-1])
变成 1xn
```

## Tensor 广播
```
使用 tf.broadcast_to 可以了解广播的运算方式。

从numpy中的等效功能借用的概念
组合运算的时候 会对‘小张量’进行‘扩展’来适应‘大张量’
x = constant [1, 2, 3]
y = xx (2)
z = xx [2, 2, 2]

tf.multiply(x, 2)
x * y
x * z

三个方法得到的结果都是一样的
不需要进行额外操作来扩展
```

## Tensor 索引
```
和numpy基本相似
从0开始索引
负数表示倒叙
冒号： 用于切片 start:stop:step
含左不含右

1维
x = tf.constant([])
1st = x[0].numpy()
[:] 全部
[::2] 各一个
[::-1] 倒序

多维
[1, 1]
[1, :] 第二行
[:, 1] 第二列
[0, -1]
简单来说就是传递多个索引

不规则张量 (不能使用constant)
0 1 2 3
4 5
6 7 8 
9
如果用 x.shape 会得到 (4, None)
所以要使用 tf.ragged.RaggedTensor

tf.string
tf.constant('Grey wolf')
整个算0长度
'xxx'
'yyy'
'zzz'
以上如果用shape会得到 (3, )
dtype=string(一个字符为1长度)
dtype=uint8 转换为数值形式的码

tf.strings.split()
每个字符都是1长度

稀疏张量
1 0 0 0
0 0 2 0
0 0 0 0
里面只有一些是有用的
所以出现新类型
tf.sparse.SparseTensor(indices=[[0, 0], [1,2]],
values=[1, 2],
dense_shape=[3, 4])
得到
[[0 0],
[1 2]]

var_x = tf.Variable(tf.constant([[1], [2], [3]]))
print(var_x.shape)  得到(3, 1)
print(var_x.shape.as_list()) [3, 1]
```

## TensorFlow 变量
```
tf.Variable 进行创建和跟着
它约等于张量
	可以存储模型参数
	可以自己修改参数（刷新）

准备：
查看变量存储位置
	print(tf.debugging.set_log_device_placement)
根据安装位置会有所不同

它需要初始值，
	一般是 tf.Variable(tf.constant(xxx))
之后才能够创建变量
数据类型不限 bool 虚数
x.shape 查询shape
x.dtype
x.numpy 导出

tf.convert_to_tensor (viewed as a tensor)
tf.argmax (index of highest value)

变量variable无法改变形状，它会新建一个张量

变量特性：
	可以通过assign函数更改 (x.assign([]))
	但是不能够改变形状
	
注意格式会使得数组不同
[1, 2]
[[1, 2]]
以上两个是不一样的，前者是 (2,),后者是(2,1)

x.assign_add([2, 3]).numpy()
x.assign_sub([7, 9]).numpy()

a = tf.Variable(my_tensor, name='Mark')
b = tf.Variable(my_tensor + 1, name='Mark')
以上操作可以但是最好不要，命名是为了方便定位，不过现在不写也是可以的
print(a == b)
会出现相同大小的布尔值(全部是False)

创建变量的时候 tf.Variable(1, trainable = False)
可以关闭梯度
```

## TensorFlow 自动微分
```
梯度带
会把tf.GradientTape的所有操作记录在磁带上(tape)，基于每次操作的导数，用反向微分法来计算被 记录 的函数的导数

x = tf.ones((2, 2))

with tf.GradientTape() as t:
  t.watch(x)
  y = tf.reduce_sum(x) # = x[0][0] + ... + x[1][1] = 4
  z = tf.multiply(y, y) # = 4*4 = 16

# Derivative of z with respect to the original input tensor x
dz_dx = t.gradient(z, x)
for i in [0, 1]:
  for j in [0, 1]:
    assert dz_dx[i][j].numpy() == 8.0
    

```
```
x = tf.constant(3.0)
with tf.GradientTape(persistent=True) as t:
  t.watch(x)
  y = x * x
  z = y * y
dz_dx = t.gradient(z, x)  # 108.0 (4*x^3 at x = 3)
dy_dx = t.gradient(y, x)  # 6.0
print(dz_dx)
print(dy_dx)
del t  # Drop the reference to the tape
# 使用del t 来主动释放资源
```
```
def f(x, y):
  output = 1.0
  for i in range(y):
    if i > 1 and i < 5:
      output = tf.multiply(output, x)
  return output

def grad(x, y):
  with tf.GradientTape() as t:
    t.watch(x)
    out = f(x, y)
  return t.gradient(out, x)

x = tf.convert_to_tensor(2.0)
print(grad(x, 6))
print(grad(x, 5))
print(grad(x, 4))
assert grad(x, 6).numpy() == 12.0
assert grad(x, 5).numpy() == 12.0
assert grad(x, 4).numpy() == 4.0
```
```
如果再测试的时候将variable变成了 False trainable，最后会报错

自动微分
高阶导数
x = tf.Variable(1.0)  # Create a Tensorflow variable initialized to 1.0

with tf.GradientTape() as t:
  with tf.GradientTape() as t2:
    y = x * x * x
  # Compute the gradient inside the 't' context manager
  # which means the gradient computation is differentiable as well.
  dy_dx = t2.gradient(y, x)
d2y_dx2 = t.gradient(dy_dx, x)
print(dy_dx)
print(d2y_dx2)
assert dy_dx.numpy() == 3.0
assert d2y_dx2.numpy() == 6.0

求两次导，最后分别得到3 和 6
y = x ** 3
```
