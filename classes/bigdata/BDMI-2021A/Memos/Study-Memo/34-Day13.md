# 第13周

## 1 文本分类

[文件](text_classification.ipynb)

### 1.建立输入管道

    dataset, info = tfds.load('imdb_reviews/subwords8k', with_info=True,
                          as_supervised=True)
    train_dataset, test_dataset = dataset['train'], dataset['test']

### 2.准备数据

编码：

    encoder = info.features['text'].encoder
    for index in encoded_string:
        print('{} ----&gt; {}'.format(index, encoder.decode([index])))

准备数据：

    train_dataset = train_dataset.shuffle(BUFFER_SIZE)
    train_dataset = train_dataset.padded_batch(BATCH_SIZE)
    test_dataset = test_dataset.padded_batch(BATCH_SIZE)

### 3.创建模型

模型：

    model = tf.keras.Sequential([
        tf.keras.layers.Embedding(encoder.vocab_size, 64),
        tf.keras.layers.Bidirectional(tf.keras.layers.LSTM(64)),
        tf.keras.layers.Dense(64, activation='relu'),
        tf.keras.layers.Dense(1)
    ])

训练：

    model.compile(loss=tf.keras.losses.BinaryCrossentropy  (from_logits=True),optimizer=tf.keras.optimizers.Adam(1e-4),metrics=['accuracy'])
    history = model.fit(train_dataset,epochs=10,
                    validation_data=test_dataset, 
                    validation_steps=30)

### 4.精度评测：

    test_loss, test_acc = model.evaluate(test_dataset)

该样例中，堆叠更多LSTM层时会有更好的效果。

## 2 文本生成

[文件](text_generation.ipynb)

### 1.读取数据

    file = tf.keras.utils.get_file('shakespeare.txt', url) #下载
    text = open(file, 'rb').read().decode(encoding='utf-8')
    
### 2.文本处理

向量化：建立字符到整数的映射，然后将字符串映射到向量

    char2idx = {u:i for i, u in enumerate(vocab)}
    idx2char = np.array(vocab)
    text_as_int = np.array([char2idx[c] for c in text])

    for char,_ in zip(char2idx, range(20)):
        print('  {:4s}: {:3d},'.format(repr(char), char2idx[char]))

### 3.创建训练集

转化为所需长度的序列

    char_dataset = tf.data.Dataset.from_tensor_slices(text_as_int)
    sequences = char_dataset.batch(seq_length+1, drop_remainder=True)

复制、顺移

    def split_input_target(chunk):
        input_text = chunk[:-1]
        target_text = chunk[1:]
        return input_text, target_text
    dataset = sequences.map(split_input_target)

建立批次

    dataset = dataset.shuffle(BUFFER_SIZE).batch(BATCH_SIZE, drop_remainder=True)

### 4.创建模型

类定义

    def build_model(vocab_size, embedding_dim, rnn_units, batch_size):
        model = tf.keras.Sequential([
        tf.keras.layers.Embedding(vocab_size, embedding_dim,
                        batch_input_shape=[batch_size,None]),
        tf.keras.layers.GRU(rnn_units,
                        return_sequences=True,
                        stateful=True,
                        recurrent_initializer='glorot_uniform'),
        tf.keras.layers.Dense(vocab_size)])
    return model

实例化

    model = build_model(
        vocab_size = len(vocab),
        embedding_dim=embedding_dim,
        rnn_units=rnn_units,
        batch_size=BATCH_SIZE)

### 4.训练模型

优化器和损失函数

    def loss(labels, logits):
        return tf.keras.losses.sparse_categorical_crossentropy(labels, logits, from_logits=True)

    model.compile(optimizer='adam', loss=loss)

检查点

    checkpoint_dir = './training_checkpoints'
    checkpoint_prefix = os.path.join(checkpoint_dir, "ckpt_{epoch}")
    checkpoint_callback=tf.keras.callbacks.ModelCheckpoint(
        filepath=checkpoint_prefix,
        save_weights_only=True)

训练

    history = model.fit(dataset, epochs=EPOCHS, callbacks=[checkpoint_callback])

### 5.文本生成

预测循环

    def generate_text(model, start_string):
        num_generate = 1000
        input_eval = [char2idx[s] for s in start_string]
        input_eval = tf.expand_dims(input_eval, 0)
        text_generated = []
        temperature = 1.0
        model.reset_states()
        for i in range(num_generate):
            predictions = model(input_eval)
            predictions = tf.squeeze(predictions, 0)
            predictions = predictions / temperature
            predicted_id = tf.random.categorical(predictions, num_samples=1)[-1,0].numpy()

            # 把预测字符和前面的隐藏状态一起传递给模型作为下一个输入
            input_eval = tf.expand_dims([predicted_id], 0)
            text_generated.append(idx2char[predicted_id])

        return (start_string + ''.join(text_generated))

## 3 语音识别

语音识别的核心：语音->自然语言->自然语言处理->反馈。语音识别的本质其实是函数w = F(h, v)。

可以用深度神经网络的方法进行端到端的处理，使用大量的语料进行训练，将语音信号转换为文本输出。

eg: LG3320模块（基于非特定人语音识别技术的语音识别芯片，通过关键词列表进行运作），[资料](https://cloud.tsinghua.edu.cn/d/5f0b4fd5d326423e8602/), [声控定制化网站](http://smartpi.cn/#/)

## 4 自动语音识别

### 1.语音识别简介

**技术问题：** 

- 口音、与其、强调
- 鸡尾酒会问题
- 噪声
- ……
  
**识别对象：** 波形图、频率图、时频谱图

**评判指标：** 词错误率（将标准答案与结果对齐，做无虑等于插入、替换、删除的总数除以标准答案长度）

### 2.技术发展

**传统方法：** 傅里叶变换映射到向量->向量到音素序列->字母序列->词汇序列，中间主要应用贝叶斯公式进行统计推理

**深度学习的加入：** 深度神经网络与经典方法的模块混合，大幅降低了识别错误率

**纯深度学习：** 用神经网络逼近函数F，用大量数据进行训练。主要包括特征提取、声学模型、语言模型三个模块。

### 3.CTC

连接主义的时间分类。CTC解决了语音和文字之间的对齐问题：对于输入语音，CTC算法通过神经网络推断出可能的文字分布。

CTC采用循环网络（RNN）的方式来估计概率，并通过束搜索的方式优化结果。

