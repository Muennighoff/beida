# W12 课堂总结

## 循环网络的原理

## RNN
- 全连接的network，左旋
- word embedding
  - 向量化处理word
- RNN application
  - Couplet Writing
- 卷积网络
  - 在时间维度上，权重共享（卷积核的平移，参数少，可以并行化）
- 使用反馈网络
- Vanilla RNN
  - 存在时间度
- 时间步上的权重共享
- 通过时间反向传播算法
- 梯度截断：
  - RNN存在梯度爆炸/梯度消失的问题
  - 把梯度设置为一些固定值
- 双向RNN


## 长短时记忆网络 LSTM
- 门Gate
  - Structure：Input，Control
  - 可以反向传播训练
- 增加一个记忆单元，三个辅助门unit（Input Gate, Output Gate，Forget Gate）
- 训练比RNN简单


## GRU
- 合并了隐藏状态和记忆门，变成更新门

## 序列对序列模型
- Machine Translation
- Attention Mechanism
  - 通过过去的神经元生成权重，对input weighted，变成attention vector

## Instance
Keras 中有三种内置 RNN 层：

1. `keras.layers.SimpleRNN`，一个全连接 RNN，其中前一个时间步骤的输出会被馈送至下一个时间步骤。

2. `keras.layers.GRU`，最初由 [Cho 等人于 2014 年](https://arxiv.org/abs/1406.1078)提出。

3. `keras.layers.LSTM`，最初由 [Hochreiter 和 Schmidhuber 于 1997 年](https://www.bioinf.jku.at/publications/older/2604.pdf)提出。

### word_embeddings
- One-hot encodings
- 用一个唯一的数字编码每个单词
- 单词嵌入向量
```python
embedding_layer = layers.Embedding(1000, 5) #嵌入向量层
```
