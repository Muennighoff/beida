## 学习小结-1124-day11
---

#### 1. 自动微分

- 求导运算

  - 数值微分

    根据导数定义计算

    具有舍入误差和计算效率低的缺点

  - 符号微分

    从表达式出发，将表达式本身看做符号的运算，直接得到求导结果的表达式

    随着函数变复杂，符号微分表达式复杂程度指数级增加

    需要保存大量中间变量，容易出错

- 自动微分（Automatic differentiation）

  - 前向模式

    - 前向模式的二元数方法

    利用计算图从前往后逐步求出最终结果

    可以得到输出对某个输入的微分

  - 反向模式

    - 反向模式的反向计算图方法

    可以计算输出对所有输入的微分

- 反向传播与自动微分

  - 反向传播是在神经网络上执行梯度下降法的主要算法
  - 反向传播算法会先按前向传播方式计算（并缓存）每个节点的输出值，然后再按反向传播遍历图的方式，计算损失函数值相对于每个参数的偏导数
  - 可以认为反向传播是AD模式的一种，本质上是一致的



#### 2. 卷积网络（Convolution Networks）

- 深度网络的网络结构

     - 多个神经元可以进行线性分类，而多个神经元的组合就可以完成复杂的分类工作

     - 多层神经网络，又称为深度神经网络

     - 全连接（Fully-connected）

          又称为密集连接（Dense）

          输入特征与中间（每个）神经元进行边连接

          隐藏层和下一个隐藏层的神经元全连接

- 卷积网络的基本结构

     - 网络的拓扑结构：不同层神经元之间的连接关系

          前馈网络、反馈网络和记忆网络

     - 卷积网络属于前馈网络的典型结构

          与全连接网络不同，卷积网络是局部连接

     - 卷积核

          卷积核（kernel/Filter）是一个多维数组，通过学习算法得到

          卷积核数组一般选32、64等

          卷积是对输入的多维数组的数据，应用卷积核的过程

          卷积是一种张量运算

     - 下采样层

          每一个卷积层后通常紧跟着一个下采样层

          下采样的作用是缩小输出张量的大小

          有两种方法：

          - 最大池法（max-pooling），取其中最大值

          - 平均池化（avg-pooling），取其中的平均值

     - 归一化层

       有时候在卷积运算后，添加进去归一化层，或称为正则化层

       例如LCN（local contrast normalization），其目标是减去平均值，除以标准差，具有亮度不变性，对于图像识别用处很大，LCN操作在最大池化层之后

     - 优点

          计算快、参数的个数少，可以并行化

- 深度卷积网络的应用

     - 图像分类

       - 图像表示

         矩阵表示，每个像素是数字

       - 手写字体MINIST数据集

         灰度图像，二值图像，黑白

         0代表黑色，1代表白色

     - 卷积网络应用

       - 图像处理
       - 人脸辨识与识别
       - 对象检测
       - 自动驾驶

#### 3、Keras

- 简介

  `Keras`是由Francois Chollet开发的一套高层API框架。可以用来快速搭建深度神经网络

  - 优点

    设计采用极简主义原则，是一套高度模块化的人工神经网络架构库

    具有方便易用的特点

    保留了足够的扩展性，同时具有灵活性，可以自行实现各种层

    用户众多，适合快速学习，快速搭建深度学习网络

- `Keras`核心概念

  - 顺序模型（Sequential model）

    又称为序列模型

    网络结构是以一个栈的形式给出，一层接一层

  - 层

    `Module:tf.keras.layers`

    其中包括

  - 激活函数

    `Module:tf.keras.activations`

    包括`softmax(x), elu(x, alpha=1.0), softplus(x), relu(x, alpha=0.0, max_value=None)`

  - 优化器

    `Keras`优化器（optimizers）包括`Opimizer`基类，`SGD`

    `SGD`是最简单的优化器之一，带有动量参数（momentum）

- `Keras`使用流程

  - 定义网络（Define）
  - 编译网络（Compile）
  - 训练网络（Fit）
  - 评估网络（Evaluate）
  - 数据预测（Predict）
  - 保存/载入网络（Save/Load）

#### 4、顺序模型

- 适用范围

  - 模型适用于一个普通堆栈的涂层，其中每个图层只有一个输入张量和一个输出张量

    ```python
    # Define Sequential model with 3 layers
    model = keras.Sequential(
        [
            layers.Dense(2, activation="relu", name="layer1"),
            layers.Dense(3, activation="relu", name="layer2"),
            layers.Dense(4, name="layer3"),
        ]
    )
    # Call model on a test input
    x = tf.ones((3, 3))
    y = model(x)
    ```

    不适用情况：

    - 模型有多个输入
    - 任何层都有多个输入或多个输出
    - 需要做图层共享
    - 需要非线性拓扑

- 创建顺序模型

  - 有两种方法

    ```python
    #method 1
    model = keras.Sequential(
        [
            layers.Dense(2, activation="relu"),
            layers.Dense(3, activation="relu"),
            layers.Dense(4),
        ]
    )
    #method 2:usd add()
    model = keras.Sequential()
    model.add(layers.Dense(2, activation="relu"))
    model.add(layers.Dense(3, activation="relu"))
    model.add(layers.Dense(4))
    ```

  - 预先指定输入形状

    ```python
    model = keras.Sequential()
    model.add(keras.Input(shape=(4,)))
    model.add(layers.Dense(2, activation="relu"))
    #or
    model = keras.Sequential()
    model.add(layers.Dense(2, activation="relu", input_shape=(4,)))
    ```

  - 基于顺序模型的迁移学习

    迁移学习包括冻结模型中的底层，而知训练顶层

    假设有一个顺序模型，目标希望冻结除最后一个之外的所有层

    方法一：迭代模型层然后设置可训练层=除最后一层外，每层均为假

    ```python
    model = keras.Sequential([
        keras.Input(shape=(784))
        layers.Dense(32, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(32, activation='relu'),
        layers.Dense(10),
    ])
    model.load_weights(...)
    # Freeze all layers except the last one.
    for layer in model.layers[:-1]:
      layer.trainable = False
    # Recompile and train (this will only update the weights of the last layer).
    model.compile(...)
    model.fit(...)
    ```

    方法二：使用顺序模型来堆叠预先训练的模型和一些新初始化的分类层

    ```python
    # Load a convolutional base with pre-trained weights
    base_model = keras.applications.Xception(
        weights='imagenet',
        include_top=False,
        pooling='avg')
    # Freeze the base model
    base_model.trainable = False
    # Use a Sequential model to add a trainable classifier on top
    model = keras.Sequential([
        base_model,
        layers.Dense(1000),
    ])
    # Compile & train
    model.compile(...)
    model.fit(...)
    ```

#### 5、Keras层和模型

- 层

  - `Keras`的一个中心抽象是`Layer`类

    `layer`层封装了状态（层的“权重”）和从输入到输出的转换（“调用”，即层的前向传递）

- 模型

  - 使用`Layer`类类定义内部计算块，使用`Model`类来定义外部模型
  - `Model`类具有与`Layer`相同的API，但有区别
    - 它会公开内置训练、平局和预测循环（`model.fit()、model.evaluate()、model.predict()`）
    - 它会通过`model.layers`属性公开其内部层的列表
    - 它会公开保存和序列化API

#### 6、Keras模型的保存和加载

- 保存和加载整个模型

  `model.save()`或`tf.keras.models.save_model()`

  `tf.keras.models.load_model()`

  其中包括模型的架构/配置、模型的权重值、模型的编译信息、优化其及其状态

- 仅保存网络架构

  `get_config()`

  `from_config()`

  模型的配置（或架构）指定模型包含的层，以及这些层的连接方式。如果有模型的配置，则可以使用带权重信息的新初始化状态创建模型，而无需编译信息

- 仅保存和加载模型的权重

  `tf.keras.layers.Layer.get_weights()`

  `tf.keras.layers.Layer.set_weights()`

  
