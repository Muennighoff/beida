# 学习小结 20211124

#### 尹哲良

## 循环网络的原理

### 单词嵌入

将语言变成向量的形式

应用：诗歌创作、代码生成等

## 循环网络基本结构

### 循环网络的特点

权重共享：不同时间处理时使用相同的权重

反馈连接：某时刻的输出会作为下一时刻的模型输入

### 基本循环网络

按时间展开

U、V、W权重矩阵：输入层、隐藏层、输出层之间全连接

输入向量映射到相同长度的输出序列

注意：是在隐藏层连接，并非输出层作为下一阶段输入层

损失函数：负对数似然，与普通神经网络类似

### 局限性

+ 不能并行运算，因为前向传播有先后顺序
+ 梯度消失和梯度爆炸（解决方案：梯度截断，避免过大或过小）

### 双向RNN

算法逻辑：相反的时间维度

应用：手写识别、自然语言处理

## 长短时记忆网络/门控循环单元网络

### 长短时记忆网络（LSTM）

门结构：输入+控制->输出

输入们、遗忘门、输出门

防止梯度爆炸：长期记忆通过sigmoid或tanh变换

### 门控循环单元网络（GRU）

相比LSTM的变化：合并记忆单元和隐藏状态，合并输入门和遗忘门

实质：记忆状态和隐藏状态合并为一个状态

## 循环网络的应用

语音识别

图片注解

单词嵌入向量：为单词编码

文本分类：情感分析

文本生成

神经网络机器翻译：输入 -> 中间向量 -> 输出，需要encoder和decoder

## 循环网络的扩展

记忆网络：隐藏层生成记忆信息

神经图灵机