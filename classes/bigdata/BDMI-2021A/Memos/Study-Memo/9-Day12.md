# 第12次课笔记

# 1 TensorFlow2-Keras 循环网络

循环网络的一些应用：单词嵌入、文本分类、文本生成；音乐生成、图像注解…

## 1.1 基本循环网络

### 1.1 网络结构

设当前时刻为t时刻。从下网上看下面的结构。

**训练目标：y（标签）**
$$
y(t)
$$

**损失函数：L（用y和o来计算损失函数）**
$$
L(t) = f_{loss}(y(t),o(t))
$$

**网络输出层：o**
通过全连接V接收h(t)。
$$
o(t) = c + \hat V a(t)
$$

**网络隐藏层：h**
通过全连接U接收x(t)，通过全连接V和o(t)连接；
通过全连接W，接收h(t-1)；且连接h(t+1)。
$$
a(t) = b + \hat W h(t-1) + \hat U x(t) \\
h(t) = f(a(t))
$$

**网络输入层：x**
通过全连接U，连接h(t)
$$
x(t)
$$

### 1.1.2 几个注意点：

权重共享：不同的时间点，采用相同的U、V、W权重矩阵；

反馈连接：隐藏层h的当前状态h(t)，会反馈到下一个时间步h(t+1)；

计算图：可以按照时间步进行展开得到计算图。

### 1.1.3 循环网络的损失函数

## 1.2 循环网络的训练-通过时间反向传播（BPTT）

## 1.3 双向循环网络

## 1.4 长短时记忆网络（LSTM）

为了解决梯度消失问题而引入的网络。

引入了：1、记忆单元；2、输入门，遗忘门，输出门。
辅助记忆单元：可以寄存时间序列的输入。
输入门：控制是否输入；遗忘门：控制是否储存；输出门：控制是否输出。

什么是门结构？

**总的来说：**
LSTM相比于传统的循环网络，增加了一条可以传递矢量的线，用来传递记忆单元C(t)。这就是LSTM的核心。
而传统循环网络的隐藏层h(t)的传播过程，额外受到了C(t-1)、三个门的控制。

## 1.5 门控循环单元全程（GRU：Gated Recurrent Unit）

是LSTM的简化版本。合并了记忆状态C(t)和隐藏状态h(t)，合并了输入门和遗忘门，称为单一的更新门。

## 1.6 记忆网络

记录前面所有时刻的状态。

图灵机模型。

## 2 RNN代码示例


## 3 单词嵌入向量

## 3.1 一些编码方法

独热编码：创建长度等于词汇量的向量。当词汇量很大时，效率极其低下。

整数编码：用唯一的数字编码每个单词，将句子编码成密集向量。单词之间的关系没有得到体现。

## 3.2 单词嵌入向量

每个单词表示为四位浮点数。嵌入向量层：采用整数组成的2D张量。

