# TensorFlow2 基础总体

pip install tensorflow

tensorflow test:

```python
import tensorflow as tf
print(tf.__version__)
a=tf.constant(1.0)
b=tf.constant(3.0)
c=a+b
print(c)
print(c.numpy())
```

Out(Example):

```python
2.6.0
tf.Tensor(4.0, shape=(), dtype=float32)
4.0
```



## Tensor 创建

##### 零维张量的创建

In / Out:

```python
rank_0_tensor=tf.constant(4)
print(rank_0_tensor)
```

```python
tf.Tensor(4, shape=(), dtype=int32)
```



##### 一维张量的创建

In / Out:

```python
rank_1_tensor=tf.constant([2.0,3.0,4.0])
print(rank_1_tensor)
```

```python
tf.Tensor([2. 3. 4.], shape=(3,), dtype=float32)
```



##### 二维张量的创建

In / Out:

```python
rank_2_tensor=tf.constant([[1,2],
                          [3,4],
                          [5,6]],dtype=tf.float16)
print(rank_2_tensor)
```

```python
tf.Tensor(
[[1. 2.]
 [3. 4.]
 [5. 6.]], shape=(3, 2), dtype=float16)
```



##### 三维张量的创建

In / Out:

```python
rank_3_tensor = tf.constant([
  [[0, 1, 2, 3, 4],
   [5, 6, 7, 8, 9]],
  [[10, 11, 12, 13, 14],
   [15, 16, 17, 18, 19]],
  [[20, 21, 22, 23, 24],
   [25, 26, 27, 28, 29]],])
                    
print(rank_3_tensor)
```

```python
tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
```



#### More Examples:

In:

```python
import tensorflow as tf
a= tf.constant([[1,1,1,1],[2,2,2,2],[3,3,3,3],[4,4,4,4]])
b= tf.ones((4,5,6))
print(a)
print(b)
```

Out:

```python
tf.Tensor(
[[1 1 1 1]
 [2 2 2 2]
 [3 3 3 3]
 [4 4 4 4]], shape=(4, 4), dtype=int32)
tf.Tensor(
[[[1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]]

 [[1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]]

 [[1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]]

 [[1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]
  [1. 1. 1. 1. 1. 1.]]], shape=(4, 5, 6), dtype=float32)
```



## 函数

### tf.constant

In：

```python
a = tf.constant([[1, 2],
                 [3, 4]])
b = tf.constant([[1, 1],
                 [1, 1]]) # Could have also said `tf.ones([2,2])`

print(tf.add(a, b), "\n")
print(tf.multiply(a, b), "\n")
print(tf.matmul(a, b), "\n")
```

Out：

```python
tf.Tensor(
[[2 3]
 [4 5]], shape=(2, 2), dtype=int32) 

tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32) 

tf.Tensor(
[[3 3]
 [7 7]], shape=(2, 2), dtype=int32) 
```



#### 运算：

In:

```python
print(a + b, "\n") # element-wise addition
print(a * b, "\n") # element-wise multiplication
print(a @ b, "\n") # matrix multiplication
```

Out:

```python
tf.Tensor(
[[2 3]
 [4 5]], shape=(2, 2), dtype=int32) 

tf.Tensor(
[[1 2]
 [3 4]], shape=(2, 2), dtype=int32) 

tf.Tensor(
[[3 3]
 [7 7]], shape=(2, 2), dtype=int32) 
```



In:

```python
c = tf.constant([[4.0, 5.0], [10.0, 1.0]])

# Find the largest value
print(tf.reduce_max(c))
# Find the index of the largest value
print(tf.argmax(c))
# Compute the softmax
print(tf.nn.softmax(c))
```

Out:

```python
tf.Tensor(10.0, shape=(), dtype=float32)
tf.Tensor([1 0], shape=(2,), dtype=int64)
tf.Tensor(
[[2.6894143e-01 7.3105854e-01]
 [9.9987662e-01 1.2339458e-04]], shape=(2, 2), dtype=float32)
```



### 运算函数

tf.add(a,b)

tf.multiply(a,b)

tf.matmul(a,b)

tf.reduce_max(c)

tf.argmax(c)

tf.nn.softmax(c)



## Tensor 索引

#### 单轴索引

TensorFlow 遵循标准 Python 索引规则（类似于[在 Python 中为列表或字符串编制索引](https://docs.python.org/3/tutorial/introduction.html#strings)）以及 NumPy 索引的基本规则。

- 索引从 `0` 开始编制
- 负索引表示按倒序编制索引
- 冒号 `:` 用于切片 `start:stop:step`

##### Examples:

In/Out[1]:

```python
rank_1_tensor = tf.constant([0, 1, 1, 2, 3, 5, 8, 13, 21, 34])
print(rank_1_tensor.numpy())
```

```python
[ 0  1  1  2  3  5  8 13 21 34]
```

使用标量编制索引会移除轴：

In/Out[2]:

```python
print("First:", rank_1_tensor[0].numpy())
print("Second:", rank_1_tensor[1].numpy())
print("Last:", rank_1_tensor[-1].numpy())
```

```python
First: 0
Second: 1
Last: 34
```

使用 `:` 切片编制索引会保留轴：

In/Out[3]:

```python
print("Everything:", rank_1_tensor[:].numpy())
print("Before 4:", rank_1_tensor[:4].numpy())
print("From 4 to the end:", rank_1_tensor[4:].numpy())
print("From 2, before 7:", rank_1_tensor[2:7].numpy())
print("Every other item:", rank_1_tensor[::2].numpy())
print("Reversed:", rank_1_tensor[::-1].numpy())
```

```python
Everything: [ 0  1  1  2  3  5  8 13 21 34]
Before 4: [0 1 1 2]
From 4 to the end: [ 3  5  8 13 21 34]
From 2, before 7: [1 2 3 5 8]
Every other item: [ 0  1  3  8 21]
Reversed: [34 21 13  8  5  3  2  1  1  0]
```

#### 多轴索引

更高秩的张量通过传递多个索引来编制索引。

对于高秩张量的每个单独的轴，遵循与单轴情形完全相同的规则。

In/Out[4]:

```python
print(rank_2_tensor.numpy())
```

```python
[[1. 2.]
 [3. 4.]
 [5. 6.]]
```

为每个索引传递一个整数，结果是一个标量。

In/Out[5]:

```python
# Pull out a single value from a 2-rank tensor
print(rank_2_tensor[1, 1].numpy())
```

```python
4.0
```

您可以使用整数与切片的任意组合编制索引：

In/Out[6]:

```python
# Get row and column tensors
print("Second row:", rank_2_tensor[1, :].numpy())
print("Second column:", rank_2_tensor[:, 1].numpy())
print("Last row:", rank_2_tensor[-1, :].numpy())
print("First item in last column:", rank_2_tensor[0, -1].numpy())
print("Skip the first row:")
print(rank_2_tensor[1:, :].numpy(), "\n")
```

```python
Second row: [3. 4.]
Second column: [2. 4. 6.]
Last row: [5. 6.]
First item in last column: 2.0
Skip the first row:
[[3. 4.]
 [5. 6.]] 
```

下面是一个 3 轴张量的示例：

In/Out[7]:

```python
print(rank_3_tensor[:, :, 4])
```

```python
tf.Tensor(
[[ 4  9]
 [14 19]
 [24 29]], shape=(3, 2), dtype=int32)
```



### 操作形状

In/Out[1]:

```python
# Shape returns a `TensorShape` object that shows the size on each dimension
var_x = tf.Variable(tf.constant([[1], [2], [3]]))
print(var_x.shape)
```

```python
(3, 1)
```

In/Out[2]:

```python
# You can convert this object into a Python list, too
print(var_x.shape.as_list())
```

```python
[3, 1]
```

In[3]:

```python
# We can reshape a tensor to a new shape.
# Note that we're passing in a list
reshaped = tf.reshape(var_x, [1, 3])
```

In/Out[4]:

```python
print(var_x.shape)
print(reshaped.shape)
```

```python
(3, 1)
(1, 3)
```

数据在内存中的布局保持不变，同时使用请求的形状创建一个指向同一数据的新张量。TensorFlow 采用 C 样式的“行优先”内存访问顺序，即最右侧的索引值递增对应于内存中的单步位移。

In/Out[5]:

```python
print(rank_3_tensor)
```

```python
tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]]

 [[10 11 12 13 14]
  [15 16 17 18 19]]

 [[20 21 22 23 24]
  [25 26 27 28 29]]], shape=(3, 2, 5), dtype=int32)
```

如果您展平张量，则可以看到它在内存中的排列顺序。

In/Out[6]:

```
# A `-1` passed in the `shape` argument says "Whatever fits".
print(tf.reshape(rank_3_tensor, [-1]))
```

```python
tf.Tensor(
[ 0  1  2  3  4  5  6  7  8  9 10 11 12 13 14 15 16 17 18 19 20 21 22 23
 24 25 26 27 28 29], shape=(30,), dtype=int32)
```

一般来说，`tf.reshape` 唯一合理的用途是用于合并或拆分相邻轴（或添加/移除 `1`）。

对于 3x2x5 张量，重构为 (3x2)x5 或 3x(2x5) 都合理，因为切片不会混淆：

In/Out[7]:

```python
print(tf.reshape(rank_3_tensor, [3*2, 5]), "\n")
print(tf.reshape(rank_3_tensor, [3, -1]))
```

```python
tf.Tensor(
[[ 0  1  2  3  4]
 [ 5  6  7  8  9]
 [10 11 12 13 14]
 [15 16 17 18 19]
 [20 21 22 23 24]
 [25 26 27 28 29]], shape=(6, 5), dtype=int32) 

tf.Tensor(
[[ 0  1  2  3  4  5  6  7  8  9]
 [10 11 12 13 14 15 16 17 18 19]
 [20 21 22 23 24 25 26 27 28 29]], shape=(3, 10), dtype=int32)
```

重构可以处理总元素个数相同的任何新形状，但是如果不遵从轴的顺序，则不会发挥任何作用。

利用 `tf.reshape` 无法实现轴的交换，要交换轴，您需要使用 `tf.transpose`。

In/Out[8]:

```python
# Bad examples: don't do this

# You can't reorder axes with reshape.
print(tf.reshape(rank_3_tensor, [2, 3, 5]), "\n") 

# This is a mess
print(tf.reshape(rank_3_tensor, [5, 6]), "\n")

# This doesn't work at all
try:
  tf.reshape(rank_3_tensor, [7, -1])
except Exception as e:
  print(f"{type(e).__name__}: {e}")
```

```python
tf.Tensor(
[[[ 0  1  2  3  4]
  [ 5  6  7  8  9]
  [10 11 12 13 14]]

 [[15 16 17 18 19]
  [20 21 22 23 24]
  [25 26 27 28 29]]], shape=(2, 3, 5), dtype=int32) 

tf.Tensor(
[[ 0  1  2  3  4  5]
 [ 6  7  8  9 10 11]
 [12 13 14 15 16 17]
 [18 19 20 21 22 23]
 [24 25 26 27 28 29]], shape=(5, 6), dtype=int32) 

InvalidArgumentError: Input to reshape is a tensor with 30 values, but the requested shape requires a multiple of 7 [Op:Reshape]
```



### `DTypes` 详解

使用 `Tensor.dtype` 属性可以检查 `tf.Tensor` 的数据类型。

从 Python 对象创建 `tf.Tensor` 时，您可以选择指定数据类型。

如果不指定，TensorFlow 会选择一个可以表示您的数据的数据类型。TensorFlow 将 Python 整数转换为 `tf.int32`，将 Python 浮点数转换为 `tf.float32`。另外，当转换为数组时，TensorFlow 会采用与 NumPy 相同的规则。

数据类型可以相互转换。

In/Out:

```python
the_f64_tensor = tf.constant([2.2, 3.3, 4.4], dtype=tf.float64)
the_f16_tensor = tf.cast(the_f64_tensor, dtype=tf.float16)
# Now, let's cast to an uint8 and lose the decimal precision
the_u8_tensor = tf.cast(the_f16_tensor, dtype=tf.uint8)
print(the_u8_tensor)
```

```python
tf.Tensor([2 3 4], shape=(3,), dtype=uint8)
```



### 广播

广播是从 [NumPy 中的等效功能](https://numpy.org/doc/stable/user/basics.html)借用的一个概念。简而言之，在一定条件下，对一组张量执行组合运算时，为了适应大张量，会对小张量进行“扩展”。

最简单和最常见的例子是尝试将张量与标量相乘或相加。在这种情况下会对标量进行广播，使其变成与其他参数相同的形状。

In/Out[1]:

```python
x = tf.constant([1, 2, 3])

y = tf.constant(2)
z = tf.constant([2, 2, 2])
# All of these are the same computation
print(tf.multiply(x, 2))
print(x * y)
print(x * z)
```

```python
tf.Tensor([2 4 6], shape=(3,), dtype=int32)
tf.Tensor([2 4 6], shape=(3,), dtype=int32)
tf.Tensor([2 4 6], shape=(3,), dtype=int32)
```

同样，可以扩展长度为 1 的轴，使其匹配其他参数。在同一个计算中可以同时扩展两个参数。

在本例中，一个 3x1 的矩阵与一个 1x4 进行元素级乘法运算，从而产生一个 3x4 的矩阵。注意前导 1 是可选的：y 的形状是 `[4]`。

In/Out[2]:

```python
# These are the same computations
x = tf.reshape(x,[3,1])
y = tf.range(1, 5)
print(x, "\n")
print(y, "\n")
print(tf.multiply(x, y))
```

```python
tf.Tensor(
[[1]
 [2]
 [3]], shape=(3, 1), dtype=int32) 

tf.Tensor([1 2 3 4], shape=(4,), dtype=int32) 

tf.Tensor(
[[ 1  2  3  4]
 [ 2  4  6  8]
 [ 3  6  9 12]], shape=(3, 4), dtype=int32)
```

下面是不使用广播的同一运算：

In/Out[3]:

```python
x_stretch = tf.constant([[1, 1, 1, 1],
                         [2, 2, 2, 2],
                         [3, 3, 3, 3]])

y_stretch = tf.constant([[1, 2, 3, 4],
                         [1, 2, 3, 4],
                         [1, 2, 3, 4]])

print(x_stretch * y_stretch)  # Again, operator overloading
```

```python
tf.Tensor(
[[ 1  2  3  4]
 [ 2  4  6  8]
 [ 3  6  9 12]], shape=(3, 4), dtype=int32)
```

在大多数情况下，广播的时间和空间效率更高，因为广播运算不会在内存中具体化扩展的张量。

使用 `tf.broadcast_to` 可以了解广播的运算方式。

```python
print(tf.broadcast_to(tf.constant([1, 2, 3]), [3, 3]))
```

```python
tf.Tensor(
[[1 2 3]
 [1 2 3]
 [1 2 3]], shape=(3, 3), dtype=int32)
```

### 不规则张量

如果张量的某个轴上的元素个数可变，则称为“不规则”张量。对于不规则数据，请使用 `tf.ragged.RaggedTensor`。

例如，下面的例子无法用规则张量表示：

In[1]:

```python
ragged_list = [
    [0, 1, 2, 3],
    [4, 5],
    [6, 7, 8],
    [9]]
```

In/Out[2]:

```python
try:
  tensor = tf.constant(ragged_list)
except Exception as e:
  print(f"{type(e).__name__}: {e}")
```

```python
ValueError: Can't convert non-rectangular Python sequence to Tensor.
```

应使用 `tf.ragged.constant` 来创建 `tf.RaggedTensor`：

In/Out[3]:

```python
ragged_tensor = tf.ragged.constant(ragged_list)
print(ragged_tensor)
```

```python
<tf.RaggedTensor [[0, 1, 2, 3], [4, 5], [6, 7, 8], [9]]>
```

`tf.RaggedTensor` 的形状将包含一些具有未知长度的轴：

In/Out[4]:

```python
print(ragged_tensor.shape)
```

```python
(4, None)
```



### 字符串张量

`tf.string` 是一种 `dtype`，也就是说，在张量中，您可以用字符串（可变长度字节数组）来表示数据。

字符串是原子类型，无法像 Python 字符串一样编制索引。字符串的长度并不是张量的一个轴。有关操作字符串的函数，请参阅 `tf.strings`。

下面是一个标量字符串张量：

In/Out[1]:

```python
# Tensors can be strings, too here is a scalar string.
scalar_string_tensor = tf.constant("Gray wolf")
print(scalar_string_tensor)
```

```python
tf.Tensor(b'Gray wolf', shape=(), dtype=string)
```

In/Out[2]:

```python
# If we have three string tensors of different lengths, this is OK.
tensor_of_strings = tf.constant(["Gray wolf",
                                 "Quick brown fox",
                                 "Lazy dog"])
# Note that the shape is (3,). The string length is not included.
print(tensor_of_strings)
```

```python
tf.Tensor([b'Gray wolf' b'Quick brown fox' b'Lazy dog'], shape=(3,), dtype=string)
```

在 `tf.strings` 中可以找到用于操作字符串的一些基本函数，包括 `tf.strings.split`。

In/Out[3]:

```python
# We can use split to split a string into a set of tensors
print(tf.strings.split(scalar_string_tensor, sep=" "))
```

```python
tf.Tensor([b'Gray' b'wolf'], shape=(2,), dtype=string)
```

In/Out[4]:

```python
# ...but it turns into a `RaggedTensor` if we split up a tensor of strings,
# as each string might be split into a different number of parts.
print(tf.strings.split(tensor_of_strings))
```

```python
<tf.RaggedTensor [[b'Gray', b'wolf'], [b'Quick', b'brown', b'fox'], [b'Lazy', b'dog']]>
```

以及 `tf.string.to_number`：

In/Out[5]:

```python
text = tf.constant("1 10 100")
print(tf.strings.to_number(tf.strings.split(text, " ")))
```

```python

tf.Tensor([  1.  10. 100.], shape=(3,), dtype=float32)
```

虽然不能使用 `tf.cast` 将字符串张量转换为数值，但是可以先将其转换为字节，然后转换为数值。

In/Out[6]:

```python
byte_strings = tf.strings.bytes_split(tf.constant("Duck"))
byte_ints = tf.io.decode_raw(tf.constant("Duck"), tf.uint8)
print("Byte strings:", byte_strings)
print("Bytes:", byte_ints)
```

```python
Byte strings: tf.Tensor([b'D' b'u' b'c' b'k'], shape=(4,), dtype=string)
Bytes: tf.Tensor([ 68 117  99 107], shape=(4,), dtype=uint8)
```



### 稀疏张量

在某些情况下，数据很稀疏，比如说在一个非常宽的嵌入空间中。为了高效存储稀疏数据，TensorFlow 支持 `tf.sparse.SparseTensor` 和相关运算。

In/Out[1]:

```python
# Sparse tensors store values by index in a memory-efficient manner
sparse_tensor = tf.sparse.SparseTensor(indices=[[0, 0], [1, 2]],
                                       values=[1, 2],
                                       dense_shape=[3, 4])
print(sparse_tensor, "\n")

# We can convert sparse tensors to dense
print(tf.sparse.to_dense(sparse_tensor))
```

```python
SparseTensor(indices=tf.Tensor(
[[0 0]
 [1 2]], shape=(2, 2), dtype=int64), values=tf.Tensor([1 2], shape=(2,), dtype=int32), dense_shape=tf.Tensor([3 4], shape=(2,), dtype=int64)) 

tf.Tensor(
[[1 0 0 0]
 [0 0 2 0]
 [0 0 0 0]], shape=(3, 4), dtype=int32)
```



## Tensor Flow 变量

test:

```python
import tensorflow as tf

print(tf.debugging.set_log_device_placement)
```

#### 创建变量

要创建变量，请提供一个初始值。`tf.Variable` 与初始值的 `dtype` 相同。

In[1]:

```python
my_tensor = tf.constant([[1.0, 2.0], [3.0, 4.0]])
my_variable = tf.Variable(my_tensor)

# Variables can be all kinds of types, just like tensors
bool_variable = tf.Variable([False, False, False, True])
complex_variable = tf.Variable([5 + 4j, 6 + 1j])
```

变量与张量的定义方式和操作行为都十分相似，实际上，它们都是 `tf.Tensor` 支持的一种数据结构。与张量类似，变量也有 `dtype` 和形状，并且可以导出至 NumPy。

In/Out[2]:

```python
print("Shape: ",my_variable.shape)
print("DType: ",my_variable.dtype)
print("As NumPy: ", my_variable.numpy)
```

```python
Shape:  (2, 2)
DType:  <dtype: 'float32'>
As NumPy:  <bound method BaseResourceVariable.numpy of <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
array([[1., 2.],
       [3., 4.]], dtype=float32)>>
```

大部分张量运算在变量上也可以按预期运行，不过变量无法重构形状。

In/Out[3]:

```
print("A variable:",my_variable)
print("\nViewed as a tensor:", tf.convert_to_tensor(my_variable))
print("\nIndex of highest value:", tf.argmax(my_variable))

# This creates a new tensor; it does not reshape the variable.
print("\nCopying and reshaping: ", tf.reshape(my_variable, ([1,4])))
```

```python
A variable: <tf.Variable 'Variable:0' shape=(2, 2) dtype=float32, numpy=
array([[1., 2.],
       [3., 4.]], dtype=float32)>

Viewed as a tensor: tf.Tensor(
[[1. 2.]
 [3. 4.]], shape=(2, 2), dtype=float32)

Index of highest value: tf.Tensor([1 1], shape=(2,), dtype=int64)

Copying and reshaping:  tf.Tensor([[1. 2. 3. 4.]], shape=(1, 4), dtype=float32)
```

如上所述，变量由张量提供支持。您可以使用 `tf.Variable.assign` 重新分配张量。调用 `assign`（通常）不会分配新张量，而会重用现有张量的内存。

In/Out[4]:

```python
a = tf.Variable([2.0, 3.0])
# This will keep the same dtype, float32
a.assign([1, 2]) 
# Not allowed as it resizes the variable: 
try:
  a.assign([1.0, 2.0, 3.0])
except Exception as e:
  print(f"{type(e).__name__}: {e}")
```

```python
ValueError: Shapes (2,) and (3,) are incompatible
```

如果在运算中像使用张量一样使用变量，那么通常会对支持张量执行运算。

从现有变量创建新变量会复制支持张量。两个变量不能共享同一内存空间。

In/Out[5]:

```python
a = tf.Variable([2.0, 3.0])
# Create b based on the value of a
b = tf.Variable(a)
a.assign([5, 6])

# a and b are different
print(a.numpy())
print(b.numpy())

# There are other versions of assign
print(a.assign_add([2,3]).numpy())  # [7. 9.]
print(a.assign_sub([7,9]).numpy())  # [0. 0.]
```

```python
[5. 6.]
[2. 3.]
[7. 9.]
[0. 0.]
```

#### 生命周期、命名和监视

在基于 Python 的 TensorFlow 中，`tf.Variable` 实例与其他 Python 对象的生命周期相同。如果没有对变量的引用，则会自动将其解除分配。

为了便于跟踪和调试，您还可以为变量命名。两个变量可以使用相同的名称。

In/Out[6]:

```python
# Create a and b; they have the same value but are backed by different tensors.
a = tf.Variable(my_tensor, name="Mark")
# A new variable with the same name, but different value
# Note that the scalar add is broadcast
b = tf.Variable(my_tensor + 1, name="Mark")

# These are elementwise-unequal, despite having the same name
print(a == b)
```

```python
tf.Tensor(
[[False False]
 [False False]], shape=(2, 2), dtype=bool)
```

保存和加载模型时会保留变量名。默认情况下，模型中的变量会自动获得唯一变量名，所以除非您希望自行命名，否则不必多此一举。

虽然变量对微分很重要，但某些变量不需要进行微分。在创建时，通过将 `trainable` 设置为 False 可以关闭梯度。例如，训练计步器就是一个不需要梯度的变量。

In/Out[7]:

```python
step_counter = tf.Variable(1, trainable=False)
```



#### 放置变量和张量

为了提高性能，TensorFlow 会尝试将张量和变量放在与其 `dtype` 兼容的最快设备上。这意味着如果有 GPU，那么大部分变量都会放置在 GPU 上。

不过，我们可以重写变量的位置。在以下代码段中，即使存在可用的 GPU，我们也可以将一个浮点张量和一个变量放置在 CPU 上。通过打开设备分配日志记录（参阅[设置](http://localhost:8888/notebooks/Downloads/2-variable.ipynb#scrollTo=xZoJJ4vdvTrD)），可以查看变量的所在位置。

注：虽然可以手动放置变量，但使用[分布策略](http://localhost:8888/notebooks/Downloads/distributed_training)是一种可优化计算的更便捷且可扩展的方式。

如果在有 GPU 和没有 GPU 的不同后端上运行此笔记本，则会看到不同的记录。*请注意，必须在会话开始时打开设备布局记录。*

In/Out[1]:

```python
with tf.device('CPU:0'):

  # Create some tensors
  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.constant([[1.0, 2.0], [3.0, 4.0], [5.0, 6.0]])
  c = tf.matmul(a, b)

print(c)
```

```python
tf.Tensor(
[[22. 28.]
 [49. 64.]], shape=(2, 2), dtype=float32)
```

您可以将变量或张量的位置设置在一个设备上，然后在另一个设备上执行计算。但这样会产生延迟，因为需要在两个设备之间复制数据。

不过，如果您有多个 GPU 工作进程，但希望变量只有一个副本，则可以这样做。

In/Out[2]:

```python
with tf.device('CPU:0'):
  a = tf.Variable([[1.0, 2.0, 3.0], [4.0, 5.0, 6.0]])
  b = tf.Variable([[1.0, 2.0, 3.0]])

with tf.device('GPU:0'):
  # Element-wise multiply
  k = a * b

print(k)
```

```python
tf.Tensor(
[[ 1.  4.  9.]
 [ 4. 10. 18.]], shape=(2, 3), dtype=float32)
```



## TensorFlow 自动微分

In/Out[1]:

```python
import tensorflow as tf
x=tf.constant(1.0)
with tf.GradientTape() as t:
    t.watch(x)
    y=x*x+tf.exp(x)
t.gradient(y,x).numpy()
```

```python
4.7182817
```

In/Out[2]:

```python
# dy = 2x * dx
dy_dx = tape.gradient(y, x)
dy_dx.numpy()
```

```python
6.0
```

In/Out[3]:

```python
import tensorflow as tf
x=tf.Variable(1.0)
with tf.GradientTape() as t:
    with tf.GradientTape() as t2:
        t.watch(x)
        y=x*x*x+tf.exp(x)
    dy_dx = t2.gradient(y,x)
d2y_dx2 = t.gradient(dy_dx,x)
print(dy_dx)
print(d2y_dx2)
```

```python
tf.Tensor(5.7182817, shape=(), dtype=float32)
tf.Tensor(8.718282, shape=(), dtype=float32)
```

