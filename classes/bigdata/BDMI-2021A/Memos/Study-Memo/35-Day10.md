# Python Class 类

Example In/Out:

```python
import numpy as np
import tensorflow as tf
class Dense(tf.Module):
    def __init__(self,name=None):
        super().__init__(name=name)
        self.w = tf.Variable(tf.reshape(tf.constant([-3.14,-2.31,2.16]),[3,1]),name="w")
    def __call__(self,x):
        y=tf.matmul(x,self.w)
        return tf.nn.sigmoid(y)

d=Dense()
np.array(d(tf.reshape(tf.constant([0.0288,-0.3256,0.5925]),[1,3])))
```

```python
array([[0.8745173]], dtype=float32)
```



# 模块、层和模型 

##### ・模块（定义、保存、恢复） 

​	・对张量进行计算的函数（前向运算）

​	・ 变量在训练过程中被更新 

##### ・在TensorFlow中定义模型（Model）和层（Layers) 

​	・层：可重用的带参数的结构 

​	・所有模型和层都是tf.ModuIe的派生类 

##### .tensorboard: 

​	・对TensorFlow模型和张量进行可视化的工具 

### 模块

In[1]:

```python
import tensorflow as tf
from datetime import datetime
%load_ext tensorboard
```

In[2]:

```python
class SimpleModule(tf.Module): 
    def __init__(self, name=None): 
        super().__init__(name=name) 
        self.a_variable = tf.Variable(5.0, name="train_me") 
        self.non_trainable_variable = tf.Variable(5.0, trainable=False, name="do_not_train_me") 
    def __call__(self, x): 
        return self.a_variable * x + self.non_trainable_variable 
simple_module= SimpleModule(name="simple") 
simple_module(tf.constant(5.0)) 
```

Out[2]:

```python
<tf.Tensor: shape=(), dtype=float32, numpy=30.0>
```

### 层

In[3]:

```python
class Dense(tf.Module): 
    def __init__ (self, in_features, out_features, name=None): 
        super().__init__(name=name) 
        self.w = tf.Variable( tf.random.normal([in_features, out_features]), name='w') 
        self.b = tf.Variable(tf.zeros([out_features]), name='b') 
    def __call__(self, x): 
        y = tf.matmul(x, self.w) + self.b 
        return tf.nn.relu(y) 
```

### 模型

In[4]:

```python
class SequentialModule(tf.Module): 
    def __init__(self, name=None): 
        super().__init__(name=name) 
        self.dense_1 = Dense(in_features=3, out_features=3) 
        self.dense_2 = Dense(in_features=3, out_features=2) 
    def __call__(self, x): 
        x = self.dense_1(x) 
        return self.dense_2(x) 
# You have made a model! 
my_model = SequentialModule(name="the_model") 
# Call it, with random results 
print("Model results:", my_model(tf.constant([[2.0, 2.0, 2.0]]))) 
```

Out[4]:

```python
Model results: tf.Tensor([[0. 0.]], shape=(1, 2), dtype=float32)
```



### 模型的存储和恢复

#### 存储函数

In[5]:

```python
class MySequentialModule(tf.Module): 
    def __init__(self, name=None):  
        super().__init__(name=name) 
        self.dense_1 = Dense(in_features=3, out_features=3) 
        self.dense_2 = Dense(in_features=3, out_features=2) 
    @tf.function 
    def __call__(self, x): 
        x = self.dense_l(x) 
        return self.dense_2(x) 
# You have made a model with a graph! 
my_model = MySequentialModule(name="the_model") 
```



# Graph（计算图）

### Graph（计算图）简介 

・Graph（图）是包含一系列tensorflow 操作（tf.Operation对象）的数据结构，这些操作代表计算单元。 

・Graph包含（tf.Tensor对象），Tensor （张量），代表操作之间流动的数据 单位 

・它们是在tf.Graph上下文中定义的。

 ![TensorFlow for Beginners With Examples and Python Implementation](data:image/png;base64,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)



・计算图的优势 

​	・计算图拥有很大的灵活性 
​	・计算图执行效率高 
​	・计算图容易优化 

・Grappler进行图的优化和加速 

### Graph（图）的建立

・利用tf.function建立图并进行追踪 

・tf.function功能化的函数是Python可调用的函数，其功能与Python 函数相同等效 

```python
#Define a Python function 
def function_to_get_faster(x, y, b):
	x=tf.matmul(x. y) 
    x=x+b 
	return x 

#Create a 'Function' object that contains a graph 
a_function_that_uses_a_graph=tf.function(function_to_get_faster) 

#Make some tensors 
xl=tf.constant([[1 0. 2.0]]) 
yl=tf.constant([[2.0], [3.0]]) 
b1=tf.constant(4.0) 

#it just works! 
a_function_that_uses_a_graph(xl, yl. b1)numpy() 
```

#### tensorboard可视化

In[6]:

```python
# Set up logging. 
stamp = datetime.now().strftime("%Y%m%d-%H%M%S") 
logdir = "logs/func/%s" % stamp 
writer = tf.summary.create_file_writer(logdir) 

# Create a new model to get a fresh trace 
# Otherwise the summary will not see the graph. 
new_model = MySequentialModule() 

# Bracket the function call with 
# tf.summary.trace_on() and tf.summary.trace_export(). 
tf.summary.trace_on(graph=True) 
tf.profiler.experimental.start(logdir) 
# Call only one tf.function when tracing. 
z = print(new_model(tf.constant([[2.0, 2.0, 2.0]]))) 
with writer.as_default(): 
    tf.summary.trace_export( name="my_func_trace", step=0, profiler_outdir=logdir) 
```

create_ file_writer指定输出的路径 
trace_on开始记录计算图 
trace_export停止记录并把之前的记录导出 

##### 打开tensorboard可视化界面:

```python
%tensorboard --logdir logs/func
```



# 训练流程

1．获取训练数据。 

2．定义模型。 

3．定义损失函数。 

4．运行训练数据，从目标值计算损失。

5．计算损失的梯度，并使用优化器来调整变量以适应数据。

6．结果评估。 



#### 1．获取训练数据。

f(x)=x*W+b

W：权重

b：偏置

```python
# The actual line 
TRUE_W = 3.0 
TRUE_B = 2.0
NUM EXAMPLES = 1000 

# A vector of random x 
values x = tf.random.normal(shape=[NUM_EXAMPLES]) 

# Generate some noise
noise = tf.random.normal(shape=[NUM_EXAMPLES]) 

# Calculate y 
y = x * TRUE_W + TRUE_B + noise 
```



#### 2.定义模型

．用变量表示权重和偏置 
．给出初始值 
・使用模块封装变量和计算 
・验证模型的有效性

```python
class MyModel(tf.Module): 
    def __init__(self, **kwargs)： 
    	super().__init__(**kwargs) 
		# Initialize the weights to '5.0' and the bias to '0.0' 
		# In practice, these should be randomly initialized 
		self.w = tf.Variable(5.0) 
		self.b = tf.Variable(0.0) 
        
	def __call__(self, x): 
    	return self.w * x + self.b 

model = MyModel() 
# List the variables tf.modules's built-in variable aggregation. 
print("Variables:", model.variables) 

# Verify the model works assert 
model(3.0).numpy() == 15.0 
```



#### 3．定义损失函数 

损失函数度量给定输入模型的输出与目标输出的匹配程度。 

```python
# This computes a single loss value for an entire batch 
def loss(target_y, predicted_y): 
    return tf.reduce_mean(tf.square(target_y - predicted_y)) 
```

·可视化损失值 
红色：模型的预测值 

蓝色：训练数据 

```python
plt.scatter(x, y, c="b") 
plt.scatter(x, model(x), c="r") 
plt.show() 
print("Current loss: %1.6f" % loss(model(x), y).numpy()) 
```



#### 4．运行训练数据，从目标值计算损失 

・训练循环由重复执行的任务组成，依次为： 
1．通过发送一批输入到模型中以生成输出 

2．通过生成的输出与目标输出的比较来计算损失 

3．使用GradientTap计算损失loss对权重w的梯度 

4．用梯度优化变量w,b



使用梯度下降来训练这个模型。

```python
# Given a callable model, inputs, outputs, and a learning rate... 
def train(model, x, y, learning_rate): 
	with tf.GradientTape() as t: 
        # Trainable variables are automatically tracked by GradientTape 
        current_loss = loss(y, model(x)) 
	
    # Use GradientTape to calculate the gradients with respect to W and b 
    dw, db = t.gradient(current_loss, [model.w, model.b]) 
	
    # Subtract the gradient scaled by the learning rate 
    model.w.assign_sub(learning_rate * dw) 
    model.b.assign_sub(learning_rate * db) 
```



#### 5．计算损失的梯度，并使用优化器来调整变量以适应数据。

```python
model = MyModel() 

# Collect the history of W-values and b-values to plot later 
Ws, bs = [], [] 
epochs = range(10) 

# Define a training loop 
def training_loop(model, x, y): 
	for epoch in epochs: 
        # Update the model with the single giant batch 
        train(model, x, y, learning_rate=0.1) 
		
        # Track this before I update 
        Ws.append(model.w.numpy()) 
        bs.append(model.b.numpy()) 
        current_loss = loss(y, model(x)) 
		
        print("Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f" % (epoch, Ws[-1], bs[-1], current_loss)) 
```

```python
print("Starting: W=%1.2f b=%1.2f, loss=%2.5f" % (model.w, model.b, loss(y, model(x)))) 
# Do the training 
training_loop(model, x, y) 
# Plot it 
plt.plot(epochs, Ws, "r", epochs, bs, "b") 
plt.plot([TRUE_W] * len(epochs), "r--", [TRUE...81 * len(epochs), "b--") 
plt.legend(["W", "b", "True W", "True V]) 
plt.show() 
```



#### 6.结果评估

```python
# Visualize how the trained model performs 
plt.scatter(x, y, c="6") 
plt.scatter(x, model(x), c="r") 
plt.show() 

print("Current loss: %1.6f" % loss(model(x), y).numpy()) 
```

#### 使用keras模型

```python
class MyModelKeras(tf.keras.Model): 
    def __init__(self, **kwargs): 
        super().__init__(**kwargs) 
        # Initialize the weights to '5.0' and the bias to '0.0' 
        # In practice, these should be randomly initialized 
        self.w = tf.Variable(5.0) 
        self.b = tf.Variable(0.0) 

    def __call__(self, x, **kwargs): 
        return self.w * x + self.b 

keras_model = MyModelKeras() 

# Reuse the training loop with a Keras model 
training_loop(keras_model, x, y) 

# You can also save a checkpoint using Keras's built-in sup' 
keras_model.save_weights("my_checkpoint") 
```

```python
keras_model = MyModelKeras() 

# compile sets the training paramaeters 
keras_model.compile( 
    # By default, fit() uses tf.function(). You can 
    # turn that off for debugging, but it is on now. 	
    run_eagerly=False, 
	# Using a built-in optimizer, configuring as an object   
    optimizer=tf.keras.optimizers.SGD(learning_rate=0.1), 
	# Keras comes with built-in MSE error 
    # However, you could use the loss function 
    # defined above 
    loss=tf.keras.losses.mean_sguared_error, 
)
```



# tf.data

#### tf.data的用途 

・TensorFiow数据接口tf.data API 接口用于处理大量数据，包括 ：

​	・从不同的数据格式中读取数据

​	・进行复杂的变换 

・tf.data.Dataset用于表示序列的元素，每个元素是一个基本的训练样本，用一对张量表示图像和对应的标签 

・用tf.data读取数据 

​	・列表list, numpy数组，文本text，表格，图片，TFRecord 

​	・减小内存开销 

​	・打乱数据（s h u If I e) 

​	・数据预处理（preprocessing) 

​	・对类别不平衡数据的处理 

・用tf.data加载图片示例 

​	・加载数据集 ，确定标签 

​	・格式化图片 

​	・构建数据集 

​	・（图片，标签）对数据集 

Example：

In:

```python
import pandas as pd
data_file = tf.keras.utils.get_file("data.csv", "https://gitee.com/zhenchen3419/BDMI-2021A/raw/master/Computing/logistic_regression/data.csv")
df = pd.read_csv(data_file)
df.head()
```

Out: 

```python
	Q1_性别	Q2_身高（厘米）	Q3_体重 （公斤）	Q4_头发长度（厘米）
0		男		190				70					7
1		女		160				45					20
2		男		179				61					5
3		女		173				60					50
4		男		175				70					15
```

