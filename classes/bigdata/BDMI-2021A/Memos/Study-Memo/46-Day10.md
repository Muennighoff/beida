# BDMI	Class10



##	1	Python类与继承

###	1.1	基类和派生类

* 派生类可以调用父类的构造函数
* 还可以覆写父类的方法

### 1.2	Super方法

* 用子类对象调用父类已被覆盖的方法



##	2	Tensorflow模块、层和模型

* [网址](https://tensorflow.google.cn/guide/intro_to_modules)

###	2.1	定义模型和层

* 由模块组成的两层线性层模型

  ```python
  #首先是一个密集(线性)层
  class Dense(tf.Module):
    def __init__(self, in_features, out_features, name=None):
      super().__init__(name=name)
      self.w = tf.Variable(
        tf.random.normal([in_features, out_features]), name='w')
      self.b = tf.Variable(tf.zeros([out_features]), name='b')
    def __call__(self, x):
      y = tf.matmul(x, self.w) + self.b
      return tf.nn.relu(y)
  ```

  ```python
  class SequentialModule(tf.Module):
    def __init__(self, name=None):
      super().__init__(name=name)
  
      self.dense_1 = Dense(in_features=3, out_features=3)
      self.dense_2 = Dense(in_features=3, out_features=2)
  
    def __call__(self, x):
      x = self.dense_1(x)
      return self.dense_2(x)
  
  # You have made a model!
  my_model = SequentialModule(name="the_model")
  
  # Call it, with random results
  print("Model results:", my_model(tf.constant([[2.0, 2.0, 2.0]])))
  ```

* 查看模型内的子模块

  ```python
  print("Submodules:", my_model.submodules)
  ```

* 查看模型内的变量

  ```python
  for var in my_model.variables:
    print(var, "\n")
  ```



###	2.2	动态决定张量维度

```python
class FlexibleDenseModule(tf.Module):
  # Note: No need for `in+features`
  def __init__(self, out_features, name=None):
    super().__init__(name=name)
    self.is_built = False
    self.out_features = out_features

  def __call__(self, x):
    # Create variables on first call.
    if not self.is_built:
      self.w = tf.Variable(
        tf.random.normal([x.shape[-1], self.out_features]), name='w')
      self.b = tf.Variable(tf.zeros([self.out_features]), name='b')
      self.is_built = True

    y = tf.matmul(x, self.w) + self.b
    return tf.nn.relu(y)
```

```python
# Used in a module
class MySequentialModule(tf.Module):
  def __init__(self, name=None):
    super().__init__(name=name)

    self.dense_1 = FlexibleDenseModule(out_features=3)
    self.dense_2 = FlexibleDenseModule(out_features=2)

  def __call__(self, x):
    x = self.dense_1(x)
    return self.dense_2(x)

my_model = MySequentialModule(name="the_model")
print("Model results:", my_model(tf.constant([[2.0, 2.0, 2.0]])))
```

> 这种灵活性是 TensorFlow 层通常仅需要指定其输出的形状（例如在 [`tf.keras.layers.Dense`](https://tensorflow.google.cn/api_docs/python/tf/keras/layers/Dense) 中），而无需指定输入和输出大小的原因。



###	2.3	模型存储和恢复

* 保存检查点(权重)

  ```python
  chkp_path = "my_checkpoint"
  checkpoint = tf.train.Checkpoint(model=my_model)
  checkpoint.write(chkp_path)
  checkpoint.write(chkp_path)
  ```

  > * 检查点由两种文件组成---数据本身以及元数据的索引文件
  > * 索引文件跟踪实际保存的内容和检查点的编号，而检查点数据包含变量值及其特性查找路径

* 查看检查点内部

  ```python
  tf.train.list_variables(chkp_path)
  ```

* 从检查点恢复模型

  ```python
  new_model = MySequentialModule()
  new_checkpoint = tf.train.Checkpoint(model=new_model)
  new_checkpoint.restore("my_checkpoint")
  
  # Should be the same result as above
  new_model(tf.constant([[2.0, 2.0, 2.0]]))
  ```

* 存储整个模型

  ```python
  tf.saved_model.save(my_model, "the_saved_model")
  ```

* 恢复整个模型

  ```python
  new_model = tf.saved_model.load("the_saved_model")
  ```

  > * 恢复的模型不是原来的类

  

##	3	Tensorflow计算图

* [网址](https://tensorflow.google.cn/guide/intro_to_graphs)

###	3.1	建立

* 利用`tf.function`建立图并进行追踪

```python
# Define a Python function.
def a_regular_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# `a_function_that_uses_a_graph` is a TensorFlow `Function`.
a_function_that_uses_a_graph = tf.function(a_regular_function)

# Make some tensors.
x1 = tf.constant([[1.0, 2.0]])
y1 = tf.constant([[2.0], [3.0]])
b1 = tf.constant(4.0)

orig_value = a_regular_function(x1, y1, b1).numpy()
# Call a `Function` like a Python function.
tf_function_value = a_function_that_uses_a_graph(x1, y1, b1).numpy()
assert(orig_value == tf_function_value)
```

* `tf.function`可以递归地跟踪它调用的任何Python函数

```python
def inner_function(x, y, b):
  x = tf.matmul(x, y)
  x = x + b
  return x

# Use the decorator to make `outer_function` a `Function`.
@tf.function
def outer_function(x):
  y = tf.constant([[2.0], [3.0]])
  b = tf.constant(4.0)

  return inner_function(x, y, b)

# Note that the callable will create a graph that
# includes `inner_function` as well as `outer_function`.
outer_function(tf.constant([[1.0, 2.0]])).numpy()
```



###	3.2	恢复eager模式

* Eager模式更易于debug

* 方法:

  * 直接调用模型和图层
  * 使用Keras编译/拟合时,在编译时使用`model.compile(run_eagerly=True)`
  * 通过设置全局执行模式`tf.config.run_functions_eagerly(True)`

  

##	4	Tensorflow训练流程

* 获取训练数据

  ```python
  TRUE_W = 3.0
  TRUE_B = 2.0
  
  NUM_EXAMPLES = 1000
  
  x = tf.random.normal(shape=[NUM_EXAMPLES])
  noise = tf.random.normal(shape=[NUM_EXAMPLES])
  y = x*TRUE_W + TRUE_B + noise
  ```

* 定义模型

  * 用**变量**定义权重和偏置
  * 给出初始值
  * 使用模块封装变量和计算
  * 验证模型的有效性`assert`

  ```python
  class MyModel(tf.Module):
      def __init__(self, **kwargs):
          super().__init__(**kwargs)
          self.w = tf.Variable(5.0)
          self.b = tf.Variable(0.0)
  
      def __call__(self, x):
          return self.w * x + self.b
  
  model = MyModel()
  
  print("Variables:",model.variables)
  
  assert model(3.0).numpy() == 15.0
  ```

* 定义损失函数

  * 损失函数度量给定输入模型的输出与目标输出的匹配程度

  ```python
  def loss(target_y, pred_y):
      return tf.reduce_mean(tf.square(target_y - pred_y))
  
  print('Current loss : %1.6f'% loss(model(x),y).numpy())
  ```

* 运行训练数据,从目标值计算损失

  ```python
  #使用梯度下降法进行训练
  def train(model, x, y, alpha):
      with tf.GradientTape() as t:
          current_loss = loss(y,model(x))
  
      #Use GradientTape to calculate the gradients with respect to W and b
      dw,db = t.gradient(current_loss, [model.w, model.b])
  
      model.w.assign_sub(alpha*dw)
      model.b.assign_sub(alpha*db)
  ```

* 计算损失的梯度并用优化器来调整变量

  ```python
  model = MyModel()
  
  epochs = 10
  
  #Define a training loop
  def training_loop(model,x,y):
      for epoch in range(epochs):
          train(model,x,y,alpha=0.1)
          current_loss = loss(y,model(x))
          print("Epoch %2d: W=%1.2f b=%1.2f, loss=%2.5f" %(epoch,model.w,model.b,current_loss))
  
  #Do the training
  training_loop(model,x,y)
  ```

* 结果评估

  ```python
  #Visualize how the trained model performs
  import matplotlib.pyplot as plt
  plt.scatter(x,y,c='b')
  plt.scatter(x,model(x),c='r')
  plt.show()
  ```

  

##	5	Tensorflow数据流水线

 ###	5.1	tf.data读取数据

* `tf.data.Dataset`用于表示序列的元素,每个元素是一个基本的训练样本,用一对张量表示图像和对应的标签

#### 5.1.1	读取list

```python
dataset = tf.data.Dataset.from_tensor_slices([8, 3, 0, 8, 2, 1])
```

```python
for elem in dataset:
  print(elem.numpy())
```

####	5.1.2	读取numpy数据

```python
dataset = tf.data.Dataset.from_tensor_slices((images, labels))
```

#### 5.1.3	读取TFRecord数据

```python
dataset = tf.data.TFRecordDataset(filenames = [fsns_test_file])
```

####	5.1.4	读取文本数据

```python
dataset = tf.data.TextLineDataset(file_paths)
```

```python
for line in dataset.take(5):
  print(line.numpy())
```

#### 5.1.5	利用高级API读取图片

```python
img_gen = tf.keras.preprocessing.image.ImageDataGenerator(rescale=1./255, rotation_range=20)
```

```python
images, labels = next(img_gen.flow_from_directory(flowers))
```

####	5.1.6	读取表格数据

```python
titanic_batches = tf.data.experimental.make_csv_dataset(
    titanic_file, batch_size=4,
    label_name="survived")
```



### 5.2	打乱数据

```python
dataset.shuffle(buffer_size=100)
```



### 5.3	数据预处理

* `dataset.map(预处理函数)`

* 对类别不平衡数据的处理

  ```python
  balanced_ds = tf.data.experimental.sample_from_datasets(
  [negative_ds, positive_ds],[0.5,0.5]).batch(10)
  ```

  

  
