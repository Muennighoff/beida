

HW1: 6/10 =(
Feedback:

Problem 1: Once getting the eigenvalues﻿ and eigenvectors, we can have decomposition A=Q^{-1}\Lambda Q, where Q is the matrix with eigenvectors as its columns and \Lambda is a diagonal matrix with eigenvalues on its dignonal. For A which is symetric, we can even have decomposition A=Q^T \Lambda Q (check slides﻿﻿﻿). Then A^m=Q^T \Lambda^m Q. 

Problem 3: With the classification condition, use matrix operation to simplify the condition. Since the covariance matrix is the same for two classifiers, the second order term x^T \Sigma^{-1}x in both classifiers ﻿﻿can be canceled in comparison, so the classifier is equivalent to linear classifier (please think carefully and fill in the details by yourself﻿﻿﻿﻿﻿﻿﻿﻿﻿﻿)


HW2: 9/10
